{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline with mse loss with wide cnn modified + adam\n",
    "#### Convert histone mark signals and use deep CNN for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Need to activate genomelake environment before this code. Simply type 'genomelake' in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3,4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1,2\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import random\n",
    "# custom file path package\n",
    "from data import Data_Directories\n",
    "# custom utility package\n",
    "from utils.compute_util import *\n",
    "# package for genomic data\n",
    "from pybedtools import Interval, BedTool\n",
    "from genomelake.extractors import ArrayExtractor, BigwigExtractor\n",
    "# package for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats.stats import pearsonr,spearmanr\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 5001\n",
    "process_all = False\n",
    "sample_num = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day6', 'day3', 'day0']\n",
      "['100', '140']\n",
      "['H3K27me3', 'H3K4me1', 'H3K27ac']\n"
     ]
    }
   ],
   "source": [
    "# retrieve data\n",
    "data = Data_Directories()\n",
    "print data.intervals.keys()\n",
    "print data.input_atac['day0'].keys()\n",
    "print data.output_histone['day0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Intervals Extracted for day0: 267226\n"
     ]
    }
   ],
   "source": [
    "# get intervals for day0 data\n",
    "day0_intervals = list(BedTool(data.intervals['day0']))\n",
    "print '# of Intervals Extracted for day0: {}'.format(len(day0_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, 140bp\n"
     ]
    }
   ],
   "source": [
    "# create an ArrayExtractor for ATAC-seq for day0 with 140 base pairs\n",
    "bw_140bp_day0 = ArrayExtractor(data.input_atac['day0']['140'])\n",
    "print 'Finished extracting bigwig for day0, 140bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, 140bp\n"
     ]
    }
   ],
   "source": [
    "# create a BigWigExtractor for histone makr 'H3K27ac' for day0\n",
    "bw_histone_mark_day0 = BigwigExtractor(data.output_histone['day0']['H3K27ac'])\n",
    "print 'Finished extracting bigwig for day0, 140bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished normalizing day0 intervals!\n"
     ]
    }
   ],
   "source": [
    "# normalize day0 intervals\n",
    "normalized_day0_intervals = [normalize_interval(interval, window_size) for interval in day0_intervals if normalize_interval(interval, window_size)]\n",
    "print 'Finished normalizing day0 intervals!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of original intervals\n",
      "[(123412027, [123411855, 123412989]), (123411941, [123411855, 123412989]), (131908564, [131908487, 131910071])]\n",
      "Examples of normalized intervals with window size of 5001\n",
      "[[123409527, 123414528], [123409441, 123414442], [131906064, 131911065]]\n"
     ]
    }
   ],
   "source": [
    "assert (len(day0_intervals)==len(normalized_day0_intervals))\n",
    "print \"Examples of original intervals\"\n",
    "print [(int(_interval.start)+int(_interval[-1]), [int(_interval.start), int(_interval.end)])\n",
    "       for _interval in day0_intervals[:3]]\n",
    "print \"Examples of normalized intervals with window size of {}\".format(window_size)\n",
    "print [([int(_interval.start), int(_interval.end)])\n",
    "       for _interval in  normalized_day0_intervals[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 5001, 5)\n"
     ]
    }
   ],
   "source": [
    "atac_seq_day0 = bw_140bp_day0(normalized_day0_intervals)\n",
    "print atac_seq_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning day0: 267226\n",
      "After pruning day0: 267226\n"
     ]
    }
   ],
   "source": [
    "#TODO: put this into utils if possible\n",
    "def prune_invalid_intervals(intervals, bigwig_file):\n",
    "    for _interval in intervals[:]:\n",
    "        try:\n",
    "            bigwig_file([_interval])\n",
    "        except:\n",
    "            intervals.remove(_interval)\n",
    "            pass\n",
    "        \n",
    "print \"Before pruning day0: {}\".format(len(normalized_day0_intervals))\n",
    "prune_invalid_intervals(normalized_day0_intervals, bw_140bp_day0)\n",
    "print \"After pruning day0: {}\".format(len(normalized_day0_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of ATAC-seq signal: (1, 5001, 5)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of ATAC-seq signal: {}\".format(bw_140bp_day0(normalized_day0_intervals[:1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of histone mark signal: (1, 5001)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of histone mark signal: {}\".format(bw_histone_mark_day0(normalized_day0_intervals[:1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 5001)\n"
     ]
    }
   ],
   "source": [
    "# replace nan values with zeros and convert it to p-values\n",
    "histone_mark_day0 = np.nan_to_num(bw_histone_mark_day0(normalized_day0_intervals))\n",
    "print histone_mark_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 5001, 1)\n"
     ]
    }
   ],
   "source": [
    "histone_mark_day0 = np.expand_dims(histone_mark_day0, axis=2)\n",
    "print histone_mark_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example histone mark signal\n",
      "\tRaw value: [ 0.97687  0.97687  0.97687  0.97687  0.97687]\n"
     ]
    }
   ],
   "source": [
    "print \"Example histone mark signal\"\n",
    "print \"\\tRaw value: {}\".format(bw_histone_mark_day0(normalized_day0_intervals[:1])[0][:5].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, Dropout, BatchNormalization, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "# parameters for first conv layer\n",
    "hidden_filters_1 = 32\n",
    "hidden_kernel_size_1 = window_size\n",
    "# parameters for second conv layer\n",
    "output_filters = 1\n",
    "output_kernel_size = 32\n",
    "# parameters for training\n",
    "batch_size = 128\n",
    "num_epochs = 300\n",
    "evaluation_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Baseline CNN (based on KERAS functional API)\n",
    "'''\n",
    "inputs = Input(shape=(window_size, 5, ))\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1)(inputs)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "x = Flatten(x)\n",
    "\n",
    "outputs = Conv1D(\n",
    "    filters=output_filters,\n",
    "    kernel_size=output_kernel_size,\n",
    "    padding='same',\n",
    "    activation='linear',\n",
    "    strides=1\n",
    "    )(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5001, 5)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 5001, 32)          800192    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5001, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5001, 1)           1025      \n",
      "=================================================================\n",
      "Total params: 801,217\n",
      "Trainable params: 801,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print out model summary\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearson(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(np.multiply(xm,ym))\n",
    "    r_den = K.sqrt(np.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for writing the scores into bigwig file\n",
    "from itertools import izip\n",
    "from itertools import groupby\n",
    "import subprocess\n",
    "\n",
    "def interval_key(interval):\n",
    "    return (interval.chrom, interval.start, interval.stop)\n",
    "\n",
    "def merged_scores(scores, intervals, merge_type):\n",
    "    # A generator that returns merged intervals/scores\n",
    "    # Scores should have shape: #examples x #categories x #interval_size\n",
    "    # Second dimension can be omitted for a 1D signal\n",
    "    signal_dims = scores.ndim - 1\n",
    "    assert signal_dims in {1, 2}\n",
    "\n",
    "    # Only support max for now\n",
    "    assert merge_type == 'max'\n",
    "    score_first_dim = 1 if signal_dims == 1 else scores.shape[1]\n",
    "\n",
    "    dtype = scores.dtype\n",
    "\n",
    "    sort_idx, sorted_intervals = \\\n",
    "        zip(*sorted(enumerate(intervals),\n",
    "                    key=lambda item: interval_key(item[1])))\n",
    "    sorted_intervals = BedTool(sorted_intervals)\n",
    "\n",
    "    # Require at least 1bp overlap\n",
    "    # Explicitly convert to list otherwise it will keep opening a file when\n",
    "    # retrieving an index resulting in an error (too many open files)\n",
    "    interval_clust = list(sorted_intervals.cluster(d=-1))\n",
    "    for _, group in groupby(izip(sort_idx, interval_clust),\n",
    "                            key=lambda item: item[1].fields[-1]):\n",
    "        idx_interval_pairs = list(group)\n",
    "        group_idx, group_intervals = zip(*idx_interval_pairs)\n",
    "\n",
    "        if len(idx_interval_pairs) == 1:\n",
    "            yield group_intervals[0], scores[group_idx[0], ...]\n",
    "        else:\n",
    "            group_chrom = group_intervals[0].chrom\n",
    "            group_start = min(interval.start for interval in group_intervals)\n",
    "            group_stop = max(interval.stop for interval in group_intervals)\n",
    "\n",
    "            # This part needs to change to support more merge_types (e.g. mean)\n",
    "            group_score = np.full((score_first_dim, group_stop - group_start),\n",
    "                                  -np.inf, dtype)\n",
    "            for idx, interval in idx_interval_pairs:\n",
    "                slice_start = interval.start - group_start\n",
    "                slice_stop = slice_start + (interval.stop - interval.start)\n",
    "                group_score[..., slice_start:slice_stop] = \\\n",
    "                    np.maximum(group_score[..., slice_start:slice_stop],\n",
    "                               scores[idx, ...])\n",
    "            if signal_dims == 1:\n",
    "                group_score = group_score.squeeze(axis=0)\n",
    "            yield Interval(group_chrom, group_start, group_stop), group_score\n",
    "            \n",
    "def interval_score_pairs(intervals, scores, merge_type):\n",
    "    return (izip(intervals, scores) if merge_type is None\n",
    "            else merged_scores(scores, intervals, merge_type))\n",
    "\n",
    "def _write_1D_deeplift_track(scores, intervals, file_prefix, merge_type='max',\n",
    "                             CHROM_SIZES='/mnt/data/annotations/by_release/hg19.GRCh37/hg19.chrom.sizes'):\n",
    "    assert scores.ndim == 2\n",
    "\n",
    "    bedgraph = file_prefix + '.bedGraph'\n",
    "    bigwig = file_prefix + '.bw'\n",
    "\n",
    "    print 'Writing 1D track of shape: {}'.format(scores.shape)\n",
    "    print 'Writing to file: {}'.format(bigwig)\n",
    "\n",
    "    with open(bedgraph, 'w') as fp:\n",
    "        for interval, score in interval_score_pairs(intervals, scores,\n",
    "                                                    merge_type):\n",
    "            chrom = interval.chrom\n",
    "            start = interval.start\n",
    "            for score_idx, val in enumerate(score):\n",
    "                fp.write('%s\\t%d\\t%d\\t%g\\n' % (chrom,\n",
    "                                               start + score_idx,\n",
    "                                               start + score_idx + 1,\n",
    "                                               val))\n",
    "    print 'Wrote bedgraph.'\n",
    "\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            ['wigToBigWig', bedgraph, CHROM_SIZES, bigwig],\n",
    "            stderr=subprocess.STDOUT)\n",
    "        print 'wigToBigWig output: {}'.format(output)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print 'wigToBigWig terminated with exit code {}'.format(\n",
    "            e.returncode)\n",
    "        print 'output was:\\n' + e.output\n",
    "\n",
    "    print 'Wrote bigwig.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = os.path.join(\"models\", \"cnn_5000\")\n",
    "log_dir = os.path.join(\"logs\", \"cnn_5000\")\n",
    "#srv_dir = os.path.join(\"/users\", \"jesikmin\", \"mse_lowlr_window4000\")\n",
    "srv_dir = os.path.join(\"/srv\", \"www\", \"kundaje\", \"jesikmin\", \"cnn_5000\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(srv_dir):\n",
    "    os.makedirs(srv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling a model with adam optimizer\n"
     ]
    }
   ],
   "source": [
    "# setting an adam optimizer with 1.0 clip norm\n",
    "adam = optimizers.Adam(lr=1e-3, clipnorm=1.)\n",
    "\n",
    "print \"Compiling a model with adam optimizer\"\n",
    "model.compile(loss=losses.mean_squared_error,\n",
    "              optimizer=adam,\n",
    "              metrics=[pearson, metrics.mse, metrics.mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CallBack: tensorboard with grad histogram\n",
    "tensorboard = TensorBoard(log_dir=log_dir,\n",
    "                          histogram_freq=1,\n",
    "                          batch_size=batch_size,\n",
    "                          write_grads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CallBack: reduce learning rate when validation loss meets plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.96,\n",
    "                              patience=5,\n",
    "                              min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CallBack: evaluate model for every evaulation epoch\n",
    "class EvaluateModel(Callback):\n",
    "    def __init__(self, X_test, y_test):\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        self.epochs = 0\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.epochs += 1\n",
    "        # evaluate the model\n",
    "        if self.epochs % evaluation_freq == 0:\n",
    "            scores = model.evaluate(X_test, y_test)\n",
    "            print zip(model.metrics_names, scores)\n",
    "            test_prediction = model.predict(X_test)\n",
    "            test_spearman_sum = 0.0\n",
    "            for idx, _pred in enumerate(test_prediction):\n",
    "                rho, _ = spearmanr(histone_mark_day0[10000+idx], _pred)\n",
    "                test_spearman_sum += rho\n",
    "            print \"Test spearman: {}\".format(test_spearman_sum * 1.0 / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CallBack: store bigwig file for the best model\n",
    "class SaveBigwig(Callback):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        self.best_val_loss = float('Inf')\n",
    "        self.best_epoch = -1\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        self.epochs = 0\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.epochs += 1\n",
    "        cur_val_loss = logs['val_loss']\n",
    "        if cur_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = cur_val_loss\n",
    "            self.best_epoch = self.epochs\n",
    "            prediction = model.predict(X_train).reshape(7500, window_size)\n",
    "            _write_1D_deeplift_track(prediction,\n",
    "                                     normalized_day0_intervals[:7500], os.path.join(srv_dir, 'train'))\n",
    "            prediction = model.predict(X_val).reshape(2500, window_size)\n",
    "            _write_1D_deeplift_track(prediction,\n",
    "                                     normalized_day0_intervals[7500:10000], os.path.join(srv_dir, 'val'))\n",
    "            prediction = model.predict(X_test).reshape(1000, window_size)\n",
    "            _write_1D_deeplift_track(prediction,\n",
    "                                     normalized_day0_intervals[10000:11000], os.path.join(srv_dir, 'test'))\n",
    "            f = open(os.path.join(srv_dir, 'meta.txt'), 'wb')\n",
    "            f.write(str(self.epochs) + \"  \" + str(logs))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CallBack: Save model checkpoint based on validation loss\n",
    "checkpointer = ModelCheckpoint(os.path.join(model_dir, \"best_model.h5\"),\n",
    "                               monitor='val_loss',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True,\n",
    "                               mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model...\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 247.5970 - pearson: 0.1557 - mean_squared_error: 247.5970 - mean_absolute_error: 5.7415\n",
      "Epoch 00001: val_loss improved from inf to 97.93046, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 468s 62ms/step - loss: 246.8408 - pearson: 0.1555 - mean_squared_error: 246.8408 - mean_absolute_error: 5.7391 - val_loss: 97.9305 - val_pearson: 0.1700 - val_mean_squared_error: 97.9305 - val_mean_absolute_error: 2.6889\n",
      "Epoch 2/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 229.9974 - pearson: 0.1930 - mean_squared_error: 229.9974 - mean_absolute_error: 5.2446\n",
      "Epoch 00002: val_loss improved from 97.93046 to 97.07216, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 462s 62ms/step - loss: 228.3654 - pearson: 0.1925 - mean_squared_error: 228.3654 - mean_absolute_error: 5.2339 - val_loss: 97.0722 - val_pearson: 0.1693 - val_mean_squared_error: 97.0722 - val_mean_absolute_error: 3.4245\n",
      "Epoch 3/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 226.6016 - pearson: 0.2032 - mean_squared_error: 226.6016 - mean_absolute_error: 5.4751\n",
      "Epoch 00003: val_loss improved from 97.07216 to 95.81487, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 460s 61ms/step - loss: 225.2678 - pearson: 0.2036 - mean_squared_error: 225.2678 - mean_absolute_error: 5.4638 - val_loss: 95.8149 - val_pearson: 0.1754 - val_mean_squared_error: 95.8149 - val_mean_absolute_error: 2.9871\n",
      "Epoch 4/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 223.4304 - pearson: 0.2095 - mean_squared_error: 223.4304 - mean_absolute_error: 5.3774\n",
      "Epoch 00004: val_loss did not improve\n",
      "7500/7500 [==============================] - 77s 10ms/step - loss: 223.5913 - pearson: 0.2100 - mean_squared_error: 223.5913 - mean_absolute_error: 5.3779 - val_loss: 96.6979 - val_pearson: 0.1813 - val_mean_squared_error: 96.6979 - val_mean_absolute_error: 2.5764\n",
      "Epoch 5/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 223.0364 - pearson: 0.2166 - mean_squared_error: 223.0364 - mean_absolute_error: 5.3755\n",
      "Epoch 00005: val_loss did not improve\n",
      "7500/7500 [==============================] - 81s 11ms/step - loss: 223.1119 - pearson: 0.2163 - mean_squared_error: 223.1119 - mean_absolute_error: 5.3874 - val_loss: 97.2987 - val_pearson: 0.1762 - val_mean_squared_error: 97.2987 - val_mean_absolute_error: 3.4993\n",
      "Epoch 6/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 220.9357 - pearson: 0.2173 - mean_squared_error: 220.9357 - mean_absolute_error: 5.3116\n",
      "Epoch 00006: val_loss did not improve\n",
      "7500/7500 [==============================] - 80s 11ms/step - loss: 220.3733 - pearson: 0.2170 - mean_squared_error: 220.3733 - mean_absolute_error: 5.3142 - val_loss: 95.9424 - val_pearson: 0.1820 - val_mean_squared_error: 95.9424 - val_mean_absolute_error: 3.3183\n",
      "Epoch 7/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 219.9079 - pearson: 0.2298 - mean_squared_error: 219.9079 - mean_absolute_error: 5.4018\n",
      "Epoch 00007: val_loss did not improve\n",
      "7500/7500 [==============================] - 79s 11ms/step - loss: 219.9707 - pearson: 0.2291 - mean_squared_error: 219.9707 - mean_absolute_error: 5.3972 - val_loss: 96.0950 - val_pearson: 0.1775 - val_mean_squared_error: 96.0950 - val_mean_absolute_error: 2.6330\n",
      "Epoch 8/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 217.8077 - pearson: 0.2238 - mean_squared_error: 217.8077 - mean_absolute_error: 5.3742\n",
      "Epoch 00008: val_loss did not improve\n",
      "7500/7500 [==============================] - 79s 10ms/step - loss: 217.7328 - pearson: 0.2233 - mean_squared_error: 217.7328 - mean_absolute_error: 5.3702 - val_loss: 99.2020 - val_pearson: 0.1722 - val_mean_squared_error: 99.2020 - val_mean_absolute_error: 2.4858\n",
      "Epoch 9/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 215.5886 - pearson: 0.2283 - mean_squared_error: 215.5886 - mean_absolute_error: 5.1947\n",
      "Epoch 00009: val_loss improved from 95.81487 to 95.72471, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 467s 62ms/step - loss: 217.4542 - pearson: 0.2287 - mean_squared_error: 217.4542 - mean_absolute_error: 5.2151 - val_loss: 95.7247 - val_pearson: 0.1854 - val_mean_squared_error: 95.7247 - val_mean_absolute_error: 3.3063\n",
      "Epoch 10/300\n",
      "1000/1000 [==============================] - 4s 4ms/steposs: 218.2386 - pearson: 0.234\n",
      "[('loss', 116.62446072387695), ('pearson', 0.27800441884994509), ('mean_squared_error', 116.62446206665039), ('mean_absolute_error', 3.7150614681243899)]\n",
      "Test spearman: 0.307071124091\n",
      "\n",
      "Epoch 00010: val_loss improved from 95.72471 to 94.23947, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 473s 63ms/step - loss: 217.1629 - pearson: 0.2348 - mean_squared_error: 217.1629 - mean_absolute_error: 5.6061 - val_loss: 94.2395 - val_pearson: 0.1864 - val_mean_squared_error: 94.2395 - val_mean_absolute_error: 2.9421\n",
      "Epoch 11/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 217.0867 - pearson: 0.2374 - mean_squared_error: 217.0867 - mean_absolute_error: 5.3328\n",
      "Epoch 00011: val_loss did not improve\n",
      "7500/7500 [==============================] - 77s 10ms/step - loss: 216.1992 - pearson: 0.2379 - mean_squared_error: 216.1992 - mean_absolute_error: 5.3335 - val_loss: 95.3402 - val_pearson: 0.1883 - val_mean_squared_error: 95.3402 - val_mean_absolute_error: 3.2602\n",
      "Epoch 12/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 214.1217 - pearson: 0.2372 - mean_squared_error: 214.1217 - mean_absolute_error: 5.3596\n",
      "Epoch 00012: val_loss did not improve\n",
      "7500/7500 [==============================] - 80s 11ms/step - loss: 215.3176 - pearson: 0.2378 - mean_squared_error: 215.3176 - mean_absolute_error: 5.3718 - val_loss: 96.0933 - val_pearson: 0.1888 - val_mean_squared_error: 96.0933 - val_mean_absolute_error: 3.2602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 214.9570 - pearson: 0.2368 - mean_squared_error: 214.9570 - mean_absolute_error: 5.3482\n",
      "Epoch 00013: val_loss did not improve\n",
      "7500/7500 [==============================] - 79s 11ms/step - loss: 214.6071 - pearson: 0.2373 - mean_squared_error: 214.6071 - mean_absolute_error: 5.3495 - val_loss: 95.0014 - val_pearson: 0.1856 - val_mean_squared_error: 95.0014 - val_mean_absolute_error: 2.7689\n",
      "Epoch 14/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 212.8789 - pearson: 0.2380 - mean_squared_error: 212.8789 - mean_absolute_error: 5.2708\n",
      "Epoch 00014: val_loss did not improve\n",
      "7500/7500 [==============================] - 80s 11ms/step - loss: 213.7158 - pearson: 0.2386 - mean_squared_error: 213.7158 - mean_absolute_error: 5.2826 - val_loss: 94.4489 - val_pearson: 0.1904 - val_mean_squared_error: 94.4489 - val_mean_absolute_error: 3.2181\n",
      "Epoch 15/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 212.5969 - pearson: 0.2423 - mean_squared_error: 212.5969 - mean_absolute_error: 5.2844\n",
      "Epoch 00015: val_loss improved from 94.23947 to 93.97077, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 465s 62ms/step - loss: 212.1016 - pearson: 0.2432 - mean_squared_error: 212.1016 - mean_absolute_error: 5.2840 - val_loss: 93.9708 - val_pearson: 0.1915 - val_mean_squared_error: 93.9708 - val_mean_absolute_error: 3.1753\n",
      "Epoch 16/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 211.8332 - pearson: 0.2396 - mean_squared_error: 211.8332 - mean_absolute_error: 5.3262\n",
      "Epoch 00016: val_loss did not improve\n",
      "7500/7500 [==============================] - 77s 10ms/step - loss: 211.9141 - pearson: 0.2409 - mean_squared_error: 211.9141 - mean_absolute_error: 5.3394 - val_loss: 94.7873 - val_pearson: 0.1898 - val_mean_squared_error: 94.7873 - val_mean_absolute_error: 3.1489\n",
      "Epoch 17/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 213.0064 - pearson: 0.2412 - mean_squared_error: 213.0064 - mean_absolute_error: 5.2593\n",
      "Epoch 00017: val_loss did not improve\n",
      "7500/7500 [==============================] - 80s 11ms/step - loss: 213.1155 - pearson: 0.2412 - mean_squared_error: 213.1155 - mean_absolute_error: 5.2608 - val_loss: 96.0024 - val_pearson: 0.1890 - val_mean_squared_error: 96.0024 - val_mean_absolute_error: 3.2330\n",
      "Epoch 18/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 211.4757 - pearson: 0.2467 - mean_squared_error: 211.4757 - mean_absolute_error: 5.3015\n",
      "Epoch 00018: val_loss improved from 93.97077 to 93.69147, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 469s 63ms/step - loss: 211.6214 - pearson: 0.2461 - mean_squared_error: 211.6214 - mean_absolute_error: 5.3062 - val_loss: 93.6915 - val_pearson: 0.1956 - val_mean_squared_error: 93.6915 - val_mean_absolute_error: 2.9442\n",
      "Epoch 19/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 209.5663 - pearson: 0.2533 - mean_squared_error: 209.5663 - mean_absolute_error: 5.3254\n",
      "Epoch 00019: val_loss did not improve\n",
      "7500/7500 [==============================] - 77s 10ms/step - loss: 209.7582 - pearson: 0.2524 - mean_squared_error: 209.7582 - mean_absolute_error: 5.3332 - val_loss: 93.7807 - val_pearson: 0.1934 - val_mean_squared_error: 93.7807 - val_mean_absolute_error: 2.7837\n",
      "Epoch 20/300\n",
      "1000/1000 [==============================] - 4s 4ms/steposs: 209.7366 - pearson: 0.252\n",
      "[('loss', 119.87353535461426), ('pearson', 0.25890634232759474), ('mean_squared_error', 119.87353773498535), ('mean_absolute_error', 3.4370099506378176)]\n",
      "Test spearman: 0.296584773413\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "7500/7500 [==============================] - 89s 12ms/step - loss: 209.9225 - pearson: 0.2512 - mean_squared_error: 209.9225 - mean_absolute_error: 5.3199 - val_loss: 95.2394 - val_pearson: 0.1914 - val_mean_squared_error: 95.2394 - val_mean_absolute_error: 2.7033\n",
      "Epoch 21/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 211.2237 - pearson: 0.2544 - mean_squared_error: 211.2237 - mean_absolute_error: 5.2542\n",
      "Epoch 00021: val_loss improved from 93.69147 to 92.86161, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 472s 63ms/step - loss: 210.4165 - pearson: 0.2556 - mean_squared_error: 210.4165 - mean_absolute_error: 5.2456 - val_loss: 92.8616 - val_pearson: 0.2009 - val_mean_squared_error: 92.8616 - val_mean_absolute_error: 2.8054\n",
      "Epoch 22/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 211.7190 - pearson: 0.2537 - mean_squared_error: 211.7190 - mean_absolute_error: 5.3943\n",
      "Epoch 00022: val_loss improved from 92.86161 to 92.85496, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 471s 63ms/step - loss: 210.4645 - pearson: 0.2558 - mean_squared_error: 210.4645 - mean_absolute_error: 5.3831 - val_loss: 92.8550 - val_pearson: 0.1968 - val_mean_squared_error: 92.8550 - val_mean_absolute_error: 2.9354\n",
      "Epoch 23/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 208.7497 - pearson: 0.2551 - mean_squared_error: 208.7497 - mean_absolute_error: 5.2230\n",
      "Epoch 00023: val_loss did not improve\n",
      "7500/7500 [==============================] - 77s 10ms/step - loss: 208.5978 - pearson: 0.2547 - mean_squared_error: 208.5978 - mean_absolute_error: 5.2239 - val_loss: 92.9043 - val_pearson: 0.1973 - val_mean_squared_error: 92.9043 - val_mean_absolute_error: 2.9436\n",
      "Epoch 24/300\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 207.0498 - pearson: 0.2568 - mean_squared_error: 207.0498 - mean_absolute_error: 5.2652\n",
      "Epoch 00024: val_loss improved from 92.85496 to 92.39684, saving model to models/cnn_5000/best_model.h5\n",
      "Writing 1D track of shape: (7500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (1000, 5001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/cnn_5000/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "7500/7500 [==============================] - 471s 63ms/step - loss: 208.4441 - pearson: 0.2561 - mean_squared_error: 208.4441 - mean_absolute_error: 5.2779 - val_loss: 92.3968 - val_pearson: 0.2020 - val_mean_squared_error: 92.3968 - val_mean_absolute_error: 3.1106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/300\n",
      "3712/7500 [=============>................] - ETA: 25s - loss: 205.4143 - pearson: 0.2514 - mean_squared_error: 205.4143 - mean_absolute_error: 5.4007"
     ]
    }
   ],
   "source": [
    "print \"Fitting the model...\"\n",
    "X_train, y_train = atac_seq_day0[:7500], histone_mark_day0[:7500]\n",
    "X_val, y_val = atac_seq_day0[7500:10000], histone_mark_day0[7500:10000]\n",
    "X_test, y_test = atac_seq_day0[10000:11000], histone_mark_day0[10000:11000]\n",
    "\n",
    "model.fit(atac_seq_day0[:10000],\n",
    "          histone_mark_day0[:10000],\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epochs,\n",
    "          validation_split=0.25,   \n",
    "          #shuffle=True,\n",
    "          callbacks=[tensorboard,\n",
    "                     reduce_lr,\n",
    "                     EvaluateModel(X_test, y_test),\n",
    "                     checkpointer,\n",
    "                     SaveBigwig(X_train, y_train, X_val, y_val, X_test, y_test)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
