{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline with mse loss with wide cnn modified + adam\n",
    "#### Convert histone mark signals and use deep CNN for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Need to activate genomelake environment before this code. Simply type 'genomelake' in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=6,7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=6,7\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import random\n",
    "# custom file path package\n",
    "from data import Data_Directories\n",
    "# custom utility package\n",
    "from utils.compute_util import *\n",
    "# package for genomic data\n",
    "from pybedtools import Interval, BedTool\n",
    "from genomelake.extractors import ArrayExtractor, BigwigExtractor\n",
    "# package for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats.stats import pearsonr,spearmanr\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 2001\n",
    "process_all = False\n",
    "sample_num = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day6', 'day3', 'day0']\n",
      "['100', '140']\n",
      "['H3K27me3', 'H3K4me1', 'H3K27ac']\n"
     ]
    }
   ],
   "source": [
    "# retrieve data\n",
    "data = Data_Directories()\n",
    "print data.intervals.keys()\n",
    "print data.input_atac['day0'].keys()\n",
    "print data.output_histone['day0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Intervals Extracted for day0: 267226\n"
     ]
    }
   ],
   "source": [
    "# get intervals for day0 data\n",
    "day0_intervals = list(BedTool(data.intervals['day0']))\n",
    "print '# of Intervals Extracted for day0: {}'.format(len(day0_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, 140bp\n"
     ]
    }
   ],
   "source": [
    "# create an ArrayExtractor for ATAC-seq for day0 with 140 base pairs\n",
    "bw_140bp_day0 = ArrayExtractor(data.input_atac['day0']['140'])\n",
    "print 'Finished extracting bigwig for day0, 140bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, 140bp\n"
     ]
    }
   ],
   "source": [
    "# create a BigWigExtractor for histone makr 'H3K27ac' for day0\n",
    "bw_histone_mark_day0 = BigwigExtractor(data.output_histone['day0']['H3K27ac'])\n",
    "print 'Finished extracting bigwig for day0, 140bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished normalizing day0 intervals!\n"
     ]
    }
   ],
   "source": [
    "# normalize day0 intervals\n",
    "normalized_day0_intervals = [normalize_interval(interval, window_size) for interval in day0_intervals if normalize_interval(interval, window_size)]\n",
    "print 'Finished normalizing day0 intervals!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of original intervals\n",
      "[(123412027, [123411855, 123412989]), (123411941, [123411855, 123412989]), (131908564, [131908487, 131910071])]\n",
      "Examples of normalized intervals with window size of 2001\n",
      "[[123411027, 123413028], [123410941, 123412942], [131907564, 131909565]]\n"
     ]
    }
   ],
   "source": [
    "assert (len(day0_intervals)==len(normalized_day0_intervals))\n",
    "print \"Examples of original intervals\"\n",
    "print [(int(_interval.start)+int(_interval[-1]), [int(_interval.start), int(_interval.end)])\n",
    "       for _interval in day0_intervals[:3]]\n",
    "print \"Examples of normalized intervals with window size of {}\".format(window_size)\n",
    "print [([int(_interval.start), int(_interval.end)])\n",
    "       for _interval in  normalized_day0_intervals[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 2001, 5)\n"
     ]
    }
   ],
   "source": [
    "atac_seq_day0 = bw_140bp_day0(normalized_day0_intervals)\n",
    "print atac_seq_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning day0: 267226\n",
      "After pruning day0: 267226\n"
     ]
    }
   ],
   "source": [
    "#TODO: put this into utils if possible\n",
    "def prune_invalid_intervals(intervals, bigwig_file):\n",
    "    for _interval in intervals[:]:\n",
    "        try:\n",
    "            bigwig_file([_interval])\n",
    "        except:\n",
    "            intervals.remove(_interval)\n",
    "            pass\n",
    "        \n",
    "print \"Before pruning day0: {}\".format(len(normalized_day0_intervals))\n",
    "prune_invalid_intervals(normalized_day0_intervals, bw_140bp_day0)\n",
    "print \"After pruning day0: {}\".format(len(normalized_day0_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of ATAC-seq signal: (1, 2001, 5)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of ATAC-seq signal: {}\".format(bw_140bp_day0(normalized_day0_intervals[:1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of histone mark signal: (1, 2001)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of histone mark signal: {}\".format(bw_histone_mark_day0(normalized_day0_intervals[:1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 2001)\n"
     ]
    }
   ],
   "source": [
    "# replace nan values with zeros and convert it to p-values\n",
    "histone_mark_day0 = np.nan_to_num(bw_histone_mark_day0(normalized_day0_intervals))\n",
    "print histone_mark_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 2001, 1)\n"
     ]
    }
   ],
   "source": [
    "histone_mark_day0 = np.expand_dims(histone_mark_day0, axis=2)\n",
    "print histone_mark_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example histone mark signal\n",
      "\tRaw value: [ 0.01014  0.01014  0.01014  0.02435  0.02435]\n"
     ]
    }
   ],
   "source": [
    "print \"Example histone mark signal\"\n",
    "print \"\\tRaw value: {}\".format(bw_histone_mark_day0(normalized_day0_intervals[:1])[0][:5].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, Dropout, BatchNormalization, Activation, ZeroPadding1D, Reshape, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "# parameters for first conv layer\n",
    "hidden_filters_1 = 32\n",
    "hidden_kernel_size_1 = 2001\n",
    "# parameters for second conv layer\n",
    "output_filters = 1\n",
    "output_kernel_size = 16\n",
    "# parameters for training\n",
    "batch_size = 128\n",
    "num_epochs = 300\n",
    "evaluation_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.window_size = window_size\n",
    "        self.channels = 5\n",
    "        self.img_shape = (self.window_size, self.channels,)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', \n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy',\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(100,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity \n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy',\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = (100,)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size, X_train, y_train):\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearson(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(np.multiply(xm,ym))\n",
    "    r_den = K.sqrt(np.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for writing the scores into bigwig file\n",
    "from itertools import izip\n",
    "from itertools import groupby\n",
    "import subprocess\n",
    "\n",
    "def interval_key(interval):\n",
    "    return (interval.chrom, interval.start, interval.stop)\n",
    "\n",
    "def merged_scores(scores, intervals, merge_type):\n",
    "    # A generator that returns merged intervals/scores\n",
    "    # Scores should have shape: #examples x #categories x #interval_size\n",
    "    # Second dimension can be omitted for a 1D signal\n",
    "    signal_dims = scores.ndim - 1\n",
    "    assert signal_dims in {1, 2}\n",
    "\n",
    "    # Only support max for now\n",
    "    assert merge_type == 'max'\n",
    "    score_first_dim = 1 if signal_dims == 1 else scores.shape[1]\n",
    "\n",
    "    dtype = scores.dtype\n",
    "\n",
    "    sort_idx, sorted_intervals = \\\n",
    "        zip(*sorted(enumerate(intervals),\n",
    "                    key=lambda item: interval_key(item[1])))\n",
    "    sorted_intervals = BedTool(sorted_intervals)\n",
    "\n",
    "    # Require at least 1bp overlap\n",
    "    # Explicitly convert to list otherwise it will keep opening a file when\n",
    "    # retrieving an index resulting in an error (too many open files)\n",
    "    interval_clust = list(sorted_intervals.cluster(d=-1))\n",
    "    for _, group in groupby(izip(sort_idx, interval_clust),\n",
    "                            key=lambda item: item[1].fields[-1]):\n",
    "        idx_interval_pairs = list(group)\n",
    "        group_idx, group_intervals = zip(*idx_interval_pairs)\n",
    "\n",
    "        if len(idx_interval_pairs) == 1:\n",
    "            yield group_intervals[0], scores[group_idx[0], ...]\n",
    "        else:\n",
    "            group_chrom = group_intervals[0].chrom\n",
    "            group_start = min(interval.start for interval in group_intervals)\n",
    "            group_stop = max(interval.stop for interval in group_intervals)\n",
    "\n",
    "            # This part needs to change to support more merge_types (e.g. mean)\n",
    "            group_score = np.full((score_first_dim, group_stop - group_start),\n",
    "                                  -np.inf, dtype)\n",
    "            for idx, interval in idx_interval_pairs:\n",
    "                slice_start = interval.start - group_start\n",
    "                slice_stop = slice_start + (interval.stop - interval.start)\n",
    "                group_score[..., slice_start:slice_stop] = \\\n",
    "                    np.maximum(group_score[..., slice_start:slice_stop],\n",
    "                               scores[idx, ...])\n",
    "            if signal_dims == 1:\n",
    "                group_score = group_score.squeeze(axis=0)\n",
    "            yield Interval(group_chrom, group_start, group_stop), group_score\n",
    "            \n",
    "def interval_score_pairs(intervals, scores, merge_type):\n",
    "    return (izip(intervals, scores) if merge_type is None\n",
    "            else merged_scores(scores, intervals, merge_type))\n",
    "\n",
    "def _write_1D_deeplift_track(scores, intervals, file_prefix, merge_type='max',\n",
    "                             CHROM_SIZES='/mnt/data/annotations/by_release/hg19.GRCh37/hg19.chrom.sizes'):\n",
    "    assert scores.ndim == 2\n",
    "\n",
    "    bedgraph = file_prefix + '.bedGraph'\n",
    "    bigwig = file_prefix + '.bw'\n",
    "\n",
    "    print 'Writing 1D track of shape: {}'.format(scores.shape)\n",
    "    print 'Writing to file: {}'.format(bigwig)\n",
    "\n",
    "    with open(bedgraph, 'w') as fp:\n",
    "        for interval, score in interval_score_pairs(intervals, scores,\n",
    "                                                    merge_type):\n",
    "            chrom = interval.chrom\n",
    "            start = interval.start\n",
    "            for score_idx, val in enumerate(score):\n",
    "                fp.write('%s\\t%d\\t%d\\t%g\\n' % (chrom,\n",
    "                                               start + score_idx,\n",
    "                                               start + score_idx + 1,\n",
    "                                               val))\n",
    "    print 'Wrote bedgraph.'\n",
    "\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            ['wigToBigWig', bedgraph, CHROM_SIZES, bigwig],\n",
    "            stderr=subprocess.STDOUT)\n",
    "        print 'wigToBigWig output: {}'.format(output)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print 'wigToBigWig terminated with exit code {}'.format(\n",
    "            e.returncode)\n",
    "        print 'output was:\\n' + e.output\n",
    "\n",
    "    print 'Wrote bigwig.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = os.path.join(\"models\", \"gan\")\n",
    "log_dir = os.path.join(\"logs\", \"gan\")\n",
    "srv_dir = os.path.join(\"/srv\", \"www\", \"kundaje\", \"jesikmin\", \"gan\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(srv_dir):\n",
    "    os.makedirs(srv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 10005)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               5123072   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,254,657\n",
      "Trainable params: 5,254,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10005)             10255125  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2001, 5)           0         \n",
      "=================================================================\n",
      "Total params: 10,945,045\n",
      "Trainable params: 10,941,461\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/keras/engine/training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.754379, acc.: 57.03%] [G loss: 0.767948]\n",
      "1 [D loss: 0.505504, acc.: 67.19%] [G loss: 0.676920]\n",
      "2 [D loss: 0.505164, acc.: 62.50%] [G loss: 0.668304]\n",
      "3 [D loss: 0.577790, acc.: 66.41%] [G loss: 0.683475]\n",
      "4 [D loss: 0.565118, acc.: 60.94%] [G loss: 0.729456]\n",
      "5 [D loss: 0.545535, acc.: 67.19%] [G loss: 0.803409]\n",
      "6 [D loss: 0.599277, acc.: 61.72%] [G loss: 0.821570]\n",
      "7 [D loss: 0.523813, acc.: 67.97%] [G loss: 0.952651]\n",
      "8 [D loss: 0.444172, acc.: 74.22%] [G loss: 1.055735]\n",
      "9 [D loss: 0.435382, acc.: 75.78%] [G loss: 1.202435]\n",
      "10 [D loss: 0.388899, acc.: 78.12%] [G loss: 1.299242]\n",
      "11 [D loss: 0.431806, acc.: 78.91%] [G loss: 1.348377]\n",
      "12 [D loss: 0.331471, acc.: 83.59%] [G loss: 1.561138]\n",
      "13 [D loss: 0.324338, acc.: 85.16%] [G loss: 1.655893]\n",
      "14 [D loss: 0.359821, acc.: 85.16%] [G loss: 1.875226]\n",
      "15 [D loss: 0.317543, acc.: 86.72%] [G loss: 1.938598]\n",
      "16 [D loss: 0.259511, acc.: 91.41%] [G loss: 2.045011]\n",
      "17 [D loss: 0.192969, acc.: 92.97%] [G loss: 2.133209]\n",
      "18 [D loss: 0.211573, acc.: 95.31%] [G loss: 2.263313]\n",
      "19 [D loss: 0.214302, acc.: 93.75%] [G loss: 2.330443]\n",
      "20 [D loss: 0.197151, acc.: 94.53%] [G loss: 2.385205]\n",
      "21 [D loss: 0.259081, acc.: 86.72%] [G loss: 2.597827]\n",
      "22 [D loss: 0.153202, acc.: 98.44%] [G loss: 2.749619]\n",
      "23 [D loss: 0.208643, acc.: 94.53%] [G loss: 2.726800]\n",
      "24 [D loss: 0.159630, acc.: 96.88%] [G loss: 2.858193]\n",
      "25 [D loss: 0.191723, acc.: 94.53%] [G loss: 2.885871]\n",
      "26 [D loss: 0.214261, acc.: 90.62%] [G loss: 3.013673]\n",
      "27 [D loss: 0.131468, acc.: 96.88%] [G loss: 3.104961]\n",
      "28 [D loss: 0.227300, acc.: 90.62%] [G loss: 3.202731]\n",
      "29 [D loss: 0.227336, acc.: 92.19%] [G loss: 3.150835]\n",
      "30 [D loss: 0.150211, acc.: 95.31%] [G loss: 3.210449]\n",
      "31 [D loss: 0.144181, acc.: 97.66%] [G loss: 3.376722]\n",
      "32 [D loss: 0.211682, acc.: 89.06%] [G loss: 3.412141]\n",
      "33 [D loss: 0.134906, acc.: 96.88%] [G loss: 3.327644]\n",
      "34 [D loss: 0.122983, acc.: 98.44%] [G loss: 3.149123]\n",
      "35 [D loss: 0.131247, acc.: 96.88%] [G loss: 3.201599]\n",
      "36 [D loss: 0.103888, acc.: 97.66%] [G loss: 3.080574]\n",
      "37 [D loss: 0.129988, acc.: 97.66%] [G loss: 3.104850]\n",
      "38 [D loss: 0.157962, acc.: 93.75%] [G loss: 3.039320]\n",
      "39 [D loss: 0.160616, acc.: 97.66%] [G loss: 3.259711]\n",
      "40 [D loss: 0.214562, acc.: 92.97%] [G loss: 3.176421]\n",
      "41 [D loss: 0.178479, acc.: 92.19%] [G loss: 3.296094]\n",
      "42 [D loss: 0.158422, acc.: 94.53%] [G loss: 3.430399]\n",
      "43 [D loss: 0.141118, acc.: 93.75%] [G loss: 3.325649]\n",
      "44 [D loss: 0.125180, acc.: 96.09%] [G loss: 3.131673]\n",
      "45 [D loss: 0.152044, acc.: 92.97%] [G loss: 3.019572]\n",
      "46 [D loss: 0.199309, acc.: 94.53%] [G loss: 3.126517]\n",
      "47 [D loss: 0.161863, acc.: 94.53%] [G loss: 3.158028]\n",
      "48 [D loss: 0.157319, acc.: 96.09%] [G loss: 3.048213]\n",
      "49 [D loss: 0.163370, acc.: 95.31%] [G loss: 3.026162]\n",
      "50 [D loss: 0.128059, acc.: 99.22%] [G loss: 3.238448]\n",
      "51 [D loss: 0.142141, acc.: 95.31%] [G loss: 2.884232]\n",
      "52 [D loss: 0.171450, acc.: 92.97%] [G loss: 3.005358]\n",
      "53 [D loss: 0.172089, acc.: 94.53%] [G loss: 3.096968]\n",
      "54 [D loss: 0.166839, acc.: 95.31%] [G loss: 3.152585]\n",
      "55 [D loss: 0.222224, acc.: 94.53%] [G loss: 3.212438]\n",
      "56 [D loss: 0.129591, acc.: 97.66%] [G loss: 3.102266]\n",
      "57 [D loss: 0.141352, acc.: 96.09%] [G loss: 3.033109]\n",
      "58 [D loss: 0.215109, acc.: 92.19%] [G loss: 3.122981]\n",
      "59 [D loss: 0.244048, acc.: 89.84%] [G loss: 3.215289]\n",
      "60 [D loss: 0.248125, acc.: 89.84%] [G loss: 3.500398]\n",
      "61 [D loss: 0.206127, acc.: 89.84%] [G loss: 3.347992]\n",
      "62 [D loss: 0.240959, acc.: 92.19%] [G loss: 3.355601]\n",
      "63 [D loss: 0.224156, acc.: 90.62%] [G loss: 3.554124]\n",
      "64 [D loss: 0.393552, acc.: 84.38%] [G loss: 3.838557]\n",
      "65 [D loss: 0.265468, acc.: 88.28%] [G loss: 3.326547]\n",
      "66 [D loss: 0.357182, acc.: 85.16%] [G loss: 3.383220]\n",
      "67 [D loss: 0.483598, acc.: 80.47%] [G loss: 3.924695]\n",
      "68 [D loss: 0.541149, acc.: 78.12%] [G loss: 4.222146]\n",
      "69 [D loss: 0.698121, acc.: 64.06%] [G loss: 3.334110]\n",
      "70 [D loss: 0.457535, acc.: 76.56%] [G loss: 3.385947]\n",
      "71 [D loss: 0.352592, acc.: 83.59%] [G loss: 3.516121]\n",
      "72 [D loss: 0.577853, acc.: 72.66%] [G loss: 3.511002]\n",
      "73 [D loss: 0.460826, acc.: 81.25%] [G loss: 3.112546]\n",
      "74 [D loss: 0.574829, acc.: 72.66%] [G loss: 3.407627]\n",
      "75 [D loss: 0.456809, acc.: 81.25%] [G loss: 3.419584]\n",
      "76 [D loss: 0.458882, acc.: 80.47%] [G loss: 3.483957]\n",
      "77 [D loss: 0.477451, acc.: 79.69%] [G loss: 2.830620]\n",
      "78 [D loss: 0.381985, acc.: 80.47%] [G loss: 2.652464]\n",
      "79 [D loss: 0.400635, acc.: 79.69%] [G loss: 2.752114]\n",
      "80 [D loss: 0.413308, acc.: 80.47%] [G loss: 2.500316]\n",
      "81 [D loss: 0.563288, acc.: 71.88%] [G loss: 2.910096]\n",
      "82 [D loss: 0.420846, acc.: 74.22%] [G loss: 2.517851]\n",
      "83 [D loss: 0.451679, acc.: 73.44%] [G loss: 2.391054]\n",
      "84 [D loss: 0.382639, acc.: 80.47%] [G loss: 2.511503]\n",
      "85 [D loss: 0.512173, acc.: 68.75%] [G loss: 2.429483]\n",
      "86 [D loss: 0.448702, acc.: 75.78%] [G loss: 2.308688]\n",
      "87 [D loss: 0.497599, acc.: 71.88%] [G loss: 2.114426]\n",
      "88 [D loss: 0.448010, acc.: 77.34%] [G loss: 2.267957]\n",
      "89 [D loss: 0.482238, acc.: 78.12%] [G loss: 2.126369]\n",
      "90 [D loss: 0.412049, acc.: 75.78%] [G loss: 2.156091]\n",
      "91 [D loss: 0.591226, acc.: 67.19%] [G loss: 2.308765]\n",
      "92 [D loss: 0.603804, acc.: 66.41%] [G loss: 1.932570]\n",
      "93 [D loss: 0.536627, acc.: 75.00%] [G loss: 2.075400]\n",
      "94 [D loss: 0.608009, acc.: 67.19%] [G loss: 1.859680]\n",
      "95 [D loss: 0.584787, acc.: 63.28%] [G loss: 1.758347]\n",
      "96 [D loss: 0.481559, acc.: 79.69%] [G loss: 1.733526]\n",
      "97 [D loss: 0.476795, acc.: 74.22%] [G loss: 2.285713]\n",
      "98 [D loss: 0.439789, acc.: 75.78%] [G loss: 2.175948]\n",
      "99 [D loss: 0.379258, acc.: 81.25%] [G loss: 2.075375]\n",
      "100 [D loss: 0.565175, acc.: 67.97%] [G loss: 1.750231]\n",
      "101 [D loss: 0.392338, acc.: 83.59%] [G loss: 2.242916]\n",
      "102 [D loss: 0.506926, acc.: 72.66%] [G loss: 1.896109]\n",
      "103 [D loss: 0.462711, acc.: 75.00%] [G loss: 2.433336]\n",
      "104 [D loss: 0.486491, acc.: 74.22%] [G loss: 2.255034]\n",
      "105 [D loss: 0.681906, acc.: 60.16%] [G loss: 1.554082]\n",
      "106 [D loss: 0.487303, acc.: 75.00%] [G loss: 2.260605]\n",
      "107 [D loss: 0.572483, acc.: 67.19%] [G loss: 2.095219]\n",
      "108 [D loss: 0.809507, acc.: 47.66%] [G loss: 1.724724]\n",
      "109 [D loss: 0.485937, acc.: 72.66%] [G loss: 1.863022]\n",
      "110 [D loss: 0.645479, acc.: 59.38%] [G loss: 1.685817]\n",
      "111 [D loss: 0.550142, acc.: 65.62%] [G loss: 1.947358]\n",
      "112 [D loss: 0.571211, acc.: 66.41%] [G loss: 1.852483]\n",
      "113 [D loss: 0.426785, acc.: 77.34%] [G loss: 1.785475]\n",
      "114 [D loss: 0.483256, acc.: 76.56%] [G loss: 1.836640]\n",
      "115 [D loss: 0.492825, acc.: 75.78%] [G loss: 1.921456]\n",
      "116 [D loss: 0.459008, acc.: 74.22%] [G loss: 2.041628]\n",
      "117 [D loss: 0.488827, acc.: 71.88%] [G loss: 1.875034]\n",
      "118 [D loss: 0.453605, acc.: 75.00%] [G loss: 1.992481]\n",
      "119 [D loss: 0.502636, acc.: 71.88%] [G loss: 1.644536]\n",
      "120 [D loss: 0.460028, acc.: 71.88%] [G loss: 1.898703]\n",
      "121 [D loss: 0.524251, acc.: 73.44%] [G loss: 1.833615]\n",
      "122 [D loss: 0.520273, acc.: 73.44%] [G loss: 1.657140]\n",
      "123 [D loss: 0.510645, acc.: 73.44%] [G loss: 1.850125]\n",
      "124 [D loss: 0.567253, acc.: 65.62%] [G loss: 1.730731]\n",
      "125 [D loss: 0.498847, acc.: 70.31%] [G loss: 1.774019]\n",
      "126 [D loss: 0.566424, acc.: 68.75%] [G loss: 1.481424]\n",
      "127 [D loss: 0.530521, acc.: 68.75%] [G loss: 1.655158]\n",
      "128 [D loss: 0.600765, acc.: 60.16%] [G loss: 1.686749]\n",
      "129 [D loss: 0.499661, acc.: 71.09%] [G loss: 1.679015]\n",
      "130 [D loss: 0.665386, acc.: 55.47%] [G loss: 1.421152]\n",
      "131 [D loss: 0.419907, acc.: 85.94%] [G loss: 1.721806]\n",
      "132 [D loss: 0.471435, acc.: 78.12%] [G loss: 1.759881]\n",
      "133 [D loss: 0.570850, acc.: 65.62%] [G loss: 1.651696]\n",
      "134 [D loss: 0.541449, acc.: 66.41%] [G loss: 1.910785]\n",
      "135 [D loss: 0.704458, acc.: 50.78%] [G loss: 1.414833]\n",
      "136 [D loss: 0.576391, acc.: 66.41%] [G loss: 1.797049]\n",
      "137 [D loss: 0.532617, acc.: 67.97%] [G loss: 1.769520]\n",
      "138 [D loss: 0.632721, acc.: 61.72%] [G loss: 1.623783]\n",
      "139 [D loss: 0.535248, acc.: 67.97%] [G loss: 1.735333]\n",
      "140 [D loss: 0.629022, acc.: 59.38%] [G loss: 1.673643]\n",
      "141 [D loss: 0.589244, acc.: 62.50%] [G loss: 1.750852]\n",
      "142 [D loss: 0.567541, acc.: 62.50%] [G loss: 1.730965]\n",
      "143 [D loss: 0.581840, acc.: 63.28%] [G loss: 1.656778]\n",
      "144 [D loss: 0.659597, acc.: 57.03%] [G loss: 1.606522]\n",
      "145 [D loss: 0.597722, acc.: 62.50%] [G loss: 1.676913]\n",
      "146 [D loss: 0.701482, acc.: 56.25%] [G loss: 1.189032]\n",
      "147 [D loss: 0.549495, acc.: 64.06%] [G loss: 1.572322]\n",
      "148 [D loss: 0.598099, acc.: 60.94%] [G loss: 1.525659]\n",
      "149 [D loss: 0.510908, acc.: 69.53%] [G loss: 1.703950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.533569, acc.: 69.53%] [G loss: 1.695473]\n",
      "151 [D loss: 0.535806, acc.: 67.97%] [G loss: 1.612031]\n",
      "152 [D loss: 0.543193, acc.: 71.09%] [G loss: 1.679833]\n",
      "153 [D loss: 0.493947, acc.: 72.66%] [G loss: 1.952168]\n",
      "154 [D loss: 0.814003, acc.: 40.62%] [G loss: 1.228248]\n",
      "155 [D loss: 0.489825, acc.: 68.75%] [G loss: 1.634797]\n",
      "156 [D loss: 0.583287, acc.: 57.81%] [G loss: 1.642817]\n",
      "157 [D loss: 0.699962, acc.: 47.66%] [G loss: 1.274595]\n",
      "158 [D loss: 0.559168, acc.: 65.62%] [G loss: 1.636345]\n",
      "159 [D loss: 0.483347, acc.: 75.78%] [G loss: 1.676914]\n",
      "160 [D loss: 0.579552, acc.: 60.94%] [G loss: 1.448999]\n",
      "161 [D loss: 0.517621, acc.: 68.75%] [G loss: 1.613416]\n",
      "162 [D loss: 0.550974, acc.: 68.75%] [G loss: 1.457991]\n",
      "163 [D loss: 0.631136, acc.: 57.03%] [G loss: 1.324066]\n",
      "164 [D loss: 0.556889, acc.: 64.84%] [G loss: 1.379614]\n",
      "165 [D loss: 0.532920, acc.: 71.09%] [G loss: 1.615093]\n",
      "166 [D loss: 0.656722, acc.: 54.69%] [G loss: 1.400248]\n",
      "167 [D loss: 0.536091, acc.: 70.31%] [G loss: 1.483285]\n",
      "168 [D loss: 0.540349, acc.: 67.19%] [G loss: 1.538563]\n",
      "169 [D loss: 0.714643, acc.: 52.34%] [G loss: 1.442191]\n",
      "170 [D loss: 0.534304, acc.: 69.53%] [G loss: 1.507698]\n",
      "171 [D loss: 0.599510, acc.: 63.28%] [G loss: 1.458149]\n",
      "172 [D loss: 0.619478, acc.: 62.50%] [G loss: 1.632834]\n",
      "173 [D loss: 0.563000, acc.: 65.62%] [G loss: 1.561148]\n",
      "174 [D loss: 0.642424, acc.: 54.69%] [G loss: 1.365823]\n",
      "175 [D loss: 0.654901, acc.: 54.69%] [G loss: 1.587419]\n",
      "176 [D loss: 0.750196, acc.: 44.53%] [G loss: 1.339374]\n",
      "177 [D loss: 0.594764, acc.: 60.94%] [G loss: 1.528348]\n",
      "178 [D loss: 0.549252, acc.: 67.97%] [G loss: 1.578716]\n",
      "179 [D loss: 0.597513, acc.: 63.28%] [G loss: 1.517726]\n",
      "180 [D loss: 0.583774, acc.: 60.94%] [G loss: 1.588239]\n",
      "181 [D loss: 0.628355, acc.: 54.69%] [G loss: 1.453460]\n",
      "182 [D loss: 0.583614, acc.: 59.38%] [G loss: 1.644646]\n",
      "183 [D loss: 0.592748, acc.: 62.50%] [G loss: 1.725691]\n",
      "184 [D loss: 0.547016, acc.: 75.78%] [G loss: 1.732261]\n",
      "185 [D loss: 0.608216, acc.: 58.59%] [G loss: 1.563937]\n",
      "186 [D loss: 0.548017, acc.: 66.41%] [G loss: 1.633959]\n",
      "187 [D loss: 0.644730, acc.: 54.69%] [G loss: 1.662666]\n",
      "188 [D loss: 0.718555, acc.: 52.34%] [G loss: 1.370313]\n",
      "189 [D loss: 0.691140, acc.: 52.34%] [G loss: 1.247159]\n",
      "190 [D loss: 0.556870, acc.: 65.62%] [G loss: 1.479109]\n",
      "191 [D loss: 0.716242, acc.: 47.66%] [G loss: 1.244734]\n",
      "192 [D loss: 0.650679, acc.: 57.81%] [G loss: 1.344692]\n",
      "193 [D loss: 0.625100, acc.: 53.91%] [G loss: 1.331038]\n",
      "194 [D loss: 0.661405, acc.: 53.12%] [G loss: 1.247765]\n",
      "195 [D loss: 0.648419, acc.: 55.47%] [G loss: 1.161742]\n",
      "196 [D loss: 0.629492, acc.: 58.59%] [G loss: 1.336002]\n",
      "197 [D loss: 0.617969, acc.: 60.94%] [G loss: 1.263851]\n",
      "198 [D loss: 0.601774, acc.: 57.81%] [G loss: 1.281057]\n",
      "199 [D loss: 0.571676, acc.: 67.19%] [G loss: 1.231807]\n",
      "200 [D loss: 0.654483, acc.: 51.56%] [G loss: 1.176566]\n",
      "201 [D loss: 0.614611, acc.: 57.03%] [G loss: 1.368313]\n",
      "202 [D loss: 0.540127, acc.: 69.53%] [G loss: 1.488687]\n",
      "203 [D loss: 0.634985, acc.: 60.16%] [G loss: 1.266670]\n",
      "204 [D loss: 0.642092, acc.: 54.69%] [G loss: 1.366613]\n",
      "205 [D loss: 0.709949, acc.: 48.44%] [G loss: 1.216484]\n",
      "206 [D loss: 0.550389, acc.: 67.19%] [G loss: 1.400574]\n",
      "207 [D loss: 0.582555, acc.: 60.16%] [G loss: 1.428718]\n",
      "208 [D loss: 0.641626, acc.: 57.81%] [G loss: 1.378443]\n",
      "209 [D loss: 0.635539, acc.: 57.03%] [G loss: 1.408543]\n",
      "210 [D loss: 0.734728, acc.: 42.19%] [G loss: 1.086557]\n",
      "211 [D loss: 0.589709, acc.: 60.16%] [G loss: 1.422983]\n",
      "212 [D loss: 0.679218, acc.: 52.34%] [G loss: 1.347334]\n",
      "213 [D loss: 0.638920, acc.: 55.47%] [G loss: 1.174984]\n",
      "214 [D loss: 0.624311, acc.: 60.16%] [G loss: 1.255491]\n",
      "215 [D loss: 0.606845, acc.: 64.06%] [G loss: 1.282064]\n",
      "216 [D loss: 0.685814, acc.: 47.66%] [G loss: 1.018806]\n",
      "217 [D loss: 0.627461, acc.: 54.69%] [G loss: 1.274176]\n",
      "218 [D loss: 0.621246, acc.: 64.06%] [G loss: 1.389074]\n",
      "219 [D loss: 0.716274, acc.: 49.22%] [G loss: 1.145447]\n",
      "220 [D loss: 0.599153, acc.: 60.16%] [G loss: 1.430878]\n",
      "221 [D loss: 0.684594, acc.: 50.78%] [G loss: 1.318005]\n",
      "222 [D loss: 0.596219, acc.: 59.38%] [G loss: 1.382269]\n",
      "223 [D loss: 0.646470, acc.: 55.47%] [G loss: 1.280214]\n",
      "224 [D loss: 0.654505, acc.: 54.69%] [G loss: 1.377923]\n",
      "225 [D loss: 0.589482, acc.: 64.06%] [G loss: 1.394239]\n",
      "226 [D loss: 0.652145, acc.: 53.12%] [G loss: 1.097390]\n",
      "227 [D loss: 0.535942, acc.: 70.31%] [G loss: 1.196794]\n",
      "228 [D loss: 0.629140, acc.: 60.16%] [G loss: 1.264070]\n",
      "229 [D loss: 0.633936, acc.: 54.69%] [G loss: 1.309838]\n",
      "230 [D loss: 0.671202, acc.: 50.78%] [G loss: 1.102141]\n",
      "231 [D loss: 0.615690, acc.: 57.81%] [G loss: 1.315111]\n",
      "232 [D loss: 0.637345, acc.: 55.47%] [G loss: 1.314609]\n",
      "233 [D loss: 0.637545, acc.: 58.59%] [G loss: 1.160943]\n",
      "234 [D loss: 0.625077, acc.: 53.91%] [G loss: 1.281552]\n",
      "235 [D loss: 0.665175, acc.: 56.25%] [G loss: 1.194783]\n",
      "236 [D loss: 0.624025, acc.: 60.94%] [G loss: 1.170815]\n",
      "237 [D loss: 0.589246, acc.: 62.50%] [G loss: 1.302824]\n",
      "238 [D loss: 0.670554, acc.: 53.12%] [G loss: 1.106007]\n",
      "239 [D loss: 0.690428, acc.: 50.00%] [G loss: 1.043612]\n",
      "240 [D loss: 0.642922, acc.: 59.38%] [G loss: 1.155858]\n",
      "241 [D loss: 0.663472, acc.: 52.34%] [G loss: 1.092290]\n",
      "242 [D loss: 0.623161, acc.: 63.28%] [G loss: 1.127012]\n",
      "243 [D loss: 0.626261, acc.: 60.16%] [G loss: 1.071989]\n",
      "244 [D loss: 0.584936, acc.: 61.72%] [G loss: 1.214009]\n",
      "245 [D loss: 0.639364, acc.: 60.16%] [G loss: 1.138330]\n",
      "246 [D loss: 0.590008, acc.: 60.94%] [G loss: 1.105390]\n",
      "247 [D loss: 0.593123, acc.: 60.16%] [G loss: 1.227699]\n",
      "248 [D loss: 0.657685, acc.: 59.38%] [G loss: 1.158269]\n",
      "249 [D loss: 0.607344, acc.: 64.06%] [G loss: 1.135552]\n",
      "250 [D loss: 0.586461, acc.: 65.62%] [G loss: 1.134820]\n",
      "251 [D loss: 0.618136, acc.: 59.38%] [G loss: 1.214549]\n",
      "252 [D loss: 0.649821, acc.: 59.38%] [G loss: 1.196773]\n",
      "253 [D loss: 0.650313, acc.: 57.81%] [G loss: 1.027642]\n",
      "254 [D loss: 0.590715, acc.: 67.97%] [G loss: 1.182531]\n",
      "255 [D loss: 0.627810, acc.: 57.03%] [G loss: 1.326040]\n",
      "256 [D loss: 0.657261, acc.: 51.56%] [G loss: 1.108872]\n",
      "257 [D loss: 0.602835, acc.: 63.28%] [G loss: 1.188690]\n",
      "258 [D loss: 0.592389, acc.: 69.53%] [G loss: 1.243599]\n",
      "259 [D loss: 0.666526, acc.: 55.47%] [G loss: 1.240730]\n",
      "260 [D loss: 0.578871, acc.: 60.16%] [G loss: 1.269710]\n",
      "261 [D loss: 0.631099, acc.: 58.59%] [G loss: 1.334128]\n",
      "262 [D loss: 0.594131, acc.: 64.06%] [G loss: 1.177935]\n",
      "263 [D loss: 0.629807, acc.: 58.59%] [G loss: 1.096944]\n",
      "264 [D loss: 0.604645, acc.: 60.94%] [G loss: 1.141343]\n",
      "265 [D loss: 0.650167, acc.: 60.16%] [G loss: 1.133933]\n",
      "266 [D loss: 0.618203, acc.: 60.94%] [G loss: 1.142692]\n",
      "267 [D loss: 0.649211, acc.: 50.00%] [G loss: 1.096670]\n",
      "268 [D loss: 0.653162, acc.: 53.91%] [G loss: 1.172055]\n",
      "269 [D loss: 0.658336, acc.: 57.81%] [G loss: 1.170510]\n",
      "270 [D loss: 0.629781, acc.: 61.72%] [G loss: 1.092271]\n",
      "271 [D loss: 0.639833, acc.: 54.69%] [G loss: 1.099792]\n",
      "272 [D loss: 0.623814, acc.: 56.25%] [G loss: 1.118197]\n",
      "273 [D loss: 0.646491, acc.: 53.91%] [G loss: 1.122802]\n",
      "274 [D loss: 0.603829, acc.: 67.97%] [G loss: 1.143512]\n",
      "275 [D loss: 0.568308, acc.: 67.97%] [G loss: 1.226509]\n",
      "276 [D loss: 0.632196, acc.: 57.81%] [G loss: 1.096796]\n",
      "277 [D loss: 0.598296, acc.: 63.28%] [G loss: 1.112135]\n",
      "278 [D loss: 0.612736, acc.: 60.94%] [G loss: 1.274474]\n",
      "279 [D loss: 0.569524, acc.: 70.31%] [G loss: 1.184433]\n",
      "280 [D loss: 0.662914, acc.: 51.56%] [G loss: 1.117445]\n",
      "281 [D loss: 0.643535, acc.: 60.94%] [G loss: 1.109355]\n",
      "282 [D loss: 0.614936, acc.: 67.19%] [G loss: 1.188701]\n",
      "283 [D loss: 0.644362, acc.: 54.69%] [G loss: 1.095219]\n",
      "284 [D loss: 0.602891, acc.: 61.72%] [G loss: 1.159360]\n",
      "285 [D loss: 0.661415, acc.: 53.91%] [G loss: 1.164627]\n",
      "286 [D loss: 0.573656, acc.: 72.66%] [G loss: 1.071674]\n",
      "287 [D loss: 0.664642, acc.: 59.38%] [G loss: 0.985200]\n",
      "288 [D loss: 0.595555, acc.: 57.03%] [G loss: 1.132035]\n",
      "289 [D loss: 0.687806, acc.: 50.00%] [G loss: 1.052678]\n",
      "290 [D loss: 0.611731, acc.: 63.28%] [G loss: 0.989716]\n",
      "291 [D loss: 0.605116, acc.: 60.16%] [G loss: 1.033357]\n",
      "292 [D loss: 0.586206, acc.: 63.28%] [G loss: 1.086267]\n",
      "293 [D loss: 0.618743, acc.: 59.38%] [G loss: 1.058150]\n",
      "294 [D loss: 0.640319, acc.: 57.81%] [G loss: 1.035890]\n",
      "295 [D loss: 0.583206, acc.: 67.19%] [G loss: 1.071002]\n",
      "296 [D loss: 0.582745, acc.: 67.19%] [G loss: 1.060633]\n",
      "297 [D loss: 0.651570, acc.: 57.03%] [G loss: 1.033527]\n",
      "298 [D loss: 0.593381, acc.: 64.84%] [G loss: 1.115595]\n",
      "299 [D loss: 0.613073, acc.: 59.38%] [G loss: 1.166933]\n"
     ]
    }
   ],
   "source": [
    "print \"Fitting the model...\"\n",
    "X_train, y_train = atac_seq_day0[:7500], histone_mark_day0[:7500]\n",
    "X_val, y_val = atac_seq_day0[7500:10000], histone_mark_day0[7500:10000]\n",
    "X_test, y_test = atac_seq_day0[10000:11000], histone_mark_day0[10000:11000]\n",
    "\n",
    "gan = GAN()\n",
    "gan.train(num_epochs, batch_size, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
