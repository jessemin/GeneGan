{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeneGan Jupyter Barebone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview: Barebone Jupyter notebook version of vanilla-GAN.\n",
    "#### Note: Need to activate genomelake environment before running this code. Simply type 'genomelake' in terminal on kali under 'jesikmin'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "# custom file path package\n",
    "from data import Data_Directories\n",
    "# custom utility package\n",
    "from utils.compute_util import *\n",
    "# package for genomic data\n",
    "from pybedtools import Interval, BedTool\n",
    "from genomelake.extractors import ArrayExtractor, BigwigExtractor\n",
    "# package for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# package for correlation\n",
    "from scipy.stats.stats import pearsonr,spearmanr\n",
    "# tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "window_size = 2001\n",
    "sample_num = 10000\n",
    "day = 'day0'\n",
    "frag = '140'\n",
    "histone = 'H3K27ac'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "does_load_model = True\n",
    "model_path = \"\"\n",
    "if does_load_model:\n",
    "    model_path = \"models/cnn_2000_new/best_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Logging directories\n",
    "# model_dir = os.path.join(\"models\", \"gan_fixed_nonsmooth_nonwide\")\n",
    "# srv_dir = os.path.join(\"/srv\", \"www\", \"kundaje\", \"jesikmin\", \"experiments\", \"gan_fixed_nonsmooth_nonwide\")\n",
    "# if not os.path.exists(model_dir):\n",
    "#     os.makedirs(model_dir)\n",
    "# if not os.path.exists(srv_dir):\n",
    "#     os.makedirs(srv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train/val/test intervals\n",
    "DATA_DIR = '/srv/scratch/jesikmin'\n",
    "train_dir, val_dir, test_dir = os.path.join(DATA_DIR, 'train_interval'),\\\n",
    "                               os.path.join(DATA_DIR, 'val_interval'),\\\n",
    "                               os.path.join(DATA_DIR, 'test_interval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/scratch/jesikmin/train_interval /srv/scratch/jesikmin/val_interval /srv/scratch/jesikmin/test_interval\n"
     ]
    }
   ],
   "source": [
    "print train_dir, val_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Train Intervals: 106855\n",
      "# of Val Intervals: 35618\n",
      "# of Test Intervals: 37068\n"
     ]
    }
   ],
   "source": [
    "# Get train/val/test intervals\n",
    "train_intervals = list(BedTool(train_dir))\n",
    "val_intervals = list(BedTool(val_dir))\n",
    "test_intervals = list(BedTool(test_dir))\n",
    "print '# of Train Intervals: {}'.format(len(train_intervals))\n",
    "print '# of Val Intervals: {}'.format(len(val_intervals))\n",
    "print '# of Test Intervals: {}'.format(len(test_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day6', 'day3', 'day0']\n",
      "['100', '140']\n",
      "['H3K27me3', 'H3K4me1', 'H3K27ac']\n"
     ]
    }
   ],
   "source": [
    "# Get input/output data directories\n",
    "data = Data_Directories()\n",
    "print data.intervals.keys()\n",
    "print data.input_atac[day].keys()\n",
    "print data.output_histone[day].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract input candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, 140bp\n"
     ]
    }
   ],
   "source": [
    "# Create an ArrayExtractor for ATAC-seq of a given day and specified fragment length\n",
    "input_candidates = ArrayExtractor(data.input_atac[day][frag])\n",
    "print 'Finished extracting bigwig for {}, {}bp'.format(day, frag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract output candiates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, H3K27ac\n"
     ]
    }
   ],
   "source": [
    "# Create a BigWigExtractor for histone mark of a given day\n",
    "output_candidates = BigwigExtractor(data.output_histone[day][histone])\n",
    "print 'Finished extracting bigwig for {}, {}'.format(day, histone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished normalizing train intervals!\n",
      "Finished normalizing val intervals!\n",
      "Finished normalizing test intervals!\n"
     ]
    }
   ],
   "source": [
    "# Normalize train intervals\n",
    "normalized_train_intervals = [normalize_interval(interval, window_size) for interval in train_intervals if normalize_interval(interval, window_size)]\n",
    "print 'Finished normalizing train intervals!'\n",
    "# Normalize val intervals\n",
    "normalized_val_intervals = [normalize_interval(interval, window_size) for interval in val_intervals if normalize_interval(interval, window_size)]\n",
    "print 'Finished normalizing val intervals!'\n",
    "# Normalize test intervals\n",
    "normalized_test_intervals = [normalize_interval(interval, window_size) for interval in test_intervals if normalize_interval(interval, window_size)]\n",
    "print 'Finished normalizing test intervals!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of original intervals\n",
      "[(123412027, [123411855, 123412989]), (123411941, [123411855, 123412989]), (131908564, [131908487, 131910071])]\n",
      "Examples of normalized intervals with window size of 2001\n",
      "[[123411027, 123413028], [123410941, 123412942], [131907564, 131909565]]\n"
     ]
    }
   ],
   "source": [
    "# Assertions of normalization step\n",
    "assert (len(train_intervals)==len(normalized_train_intervals))\n",
    "assert (len(val_intervals)==len(normalized_val_intervals))\n",
    "assert (len(test_intervals)==len(normalized_test_intervals))\n",
    "# Examples of normalized intervals\n",
    "print \"Examples of original train intervals\"\n",
    "print [(int(_interval.start)+int(_interval[-1]), [int(_interval.start), int(_interval.end)])\n",
    "       for _interval in train_intervals[:3]]\n",
    "print \"Examples of normalized train intervals with window size of {}\".format(window_size)\n",
    "print [([int(_interval.start), int(_interval.end)])\n",
    "       for _interval in  normalized_train_intervals[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching X_train\n",
      "Finished fetching X_val\n",
      "Finished fetching X_test\n",
      "(106852, 2001, 5) (35618, 2001, 5) (37068, 2001, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train = input_candidates(normalized_train_intervals)\n",
    "print \"Finished fetching X_train\"\n",
    "X_val = input_candidates(normalized_val_intervals)\n",
    "print \"Finished fetching X_val\"\n",
    "X_test = input_candidates(normalized_test_intervals)\n",
    "print \"Finished fetching X_test\"\n",
    "print X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of ATAC-seq signal (input): (2001, 5)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of ATAC-seq signal (input): {}\".format(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching y_train\n",
      "Finished fetching y_val\n",
      "Finished fetching y_test\n",
      "(106852, 2001) (35618, 2001) (37068, 2001)\n"
     ]
    }
   ],
   "source": [
    "# Replace nan values with zeros\n",
    "y_train = np.nan_to_num(output_candidates(normalized_train_intervals))\n",
    "print \"Finished fetching y_train\"\n",
    "y_val = np.nan_to_num(output_candidates(normalized_val_intervals))\n",
    "print \"Finished fetching y_val\"\n",
    "y_test = np.nan_to_num(output_candidates(normalized_test_intervals))\n",
    "print \"Finished fetching y_test\"\n",
    "print y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106852, 2001, 1) (35618, 2001, 1) (37068, 2001, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.expand_dims(y_train, axis=2)\n",
    "y_val = np.expand_dims(y_val, axis=2)\n",
    "y_test = np.expand_dims(y_test, axis=2)\n",
    "print y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of histone mark signal (output): (2001, 1)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of histone mark signal (output): {}\".format(y_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import keras\n",
    "from keras.layers import AveragePooling1D, Input, Dense, Conv1D, Dropout, BatchNormalization, Activation, ZeroPadding1D, Reshape, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "HYPERPARAMETERS\n",
    "'''\n",
    "# GAN Discriminator\n",
    "smooth_rate = 0.1\n",
    "d_train_freq = 16\n",
    "# Dropout Rate\n",
    "dropout_rate = 0.5\n",
    "# First conv layer\n",
    "hidden_filters_1 = 32\n",
    "hidden_kernel_size_1 = window_size\n",
    "# Second conv layer\n",
    "output_filters = 1\n",
    "output_kernel_size = 32\n",
    "# Training\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "evaluation_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for writing the scores into bigwig file\n",
    "from itertools import izip\n",
    "from itertools import groupby\n",
    "import subprocess\n",
    "\n",
    "def interval_key(interval):\n",
    "    return (interval.chrom, interval.start, interval.stop)\n",
    "\n",
    "def merged_scores(scores, intervals, merge_type):\n",
    "    # A generator that returns merged intervals/scores\n",
    "    # Scores should have shape: #examples x #categories x #interval_size\n",
    "    # Second dimension can be omitted for a 1D signal\n",
    "    signal_dims = scores.ndim - 1\n",
    "    assert signal_dims in {1, 2}\n",
    "\n",
    "    # Only support max for now\n",
    "    assert merge_type == 'max'\n",
    "    score_first_dim = 1 if signal_dims == 1 else scores.shape[1]\n",
    "\n",
    "    dtype = scores.dtype\n",
    "\n",
    "    sort_idx, sorted_intervals = \\\n",
    "        zip(*sorted(enumerate(intervals),\n",
    "                    key=lambda item: interval_key(item[1])))\n",
    "    sorted_intervals = BedTool(sorted_intervals)\n",
    "\n",
    "    # Require at least 1bp overlap\n",
    "    # Explicitly convert to list otherwise it will keep opening a file when\n",
    "    # retrieving an index resulting in an error (too many open files)\n",
    "    interval_clust = list(sorted_intervals.cluster(d=-1))\n",
    "    for _, group in groupby(izip(sort_idx, interval_clust),\n",
    "                            key=lambda item: item[1].fields[-1]):\n",
    "        idx_interval_pairs = list(group)\n",
    "        group_idx, group_intervals = zip(*idx_interval_pairs)\n",
    "\n",
    "        if len(idx_interval_pairs) == 1:\n",
    "            yield group_intervals[0], scores[group_idx[0], ...]\n",
    "        else:\n",
    "            group_chrom = group_intervals[0].chrom\n",
    "            group_start = min(interval.start for interval in group_intervals)\n",
    "            group_stop = max(interval.stop for interval in group_intervals)\n",
    "\n",
    "            # This part needs to change to support more merge_types (e.g. mean)\n",
    "            group_score = np.full((score_first_dim, group_stop - group_start),\n",
    "                                  -np.inf, dtype)\n",
    "            for idx, interval in idx_interval_pairs:\n",
    "                slice_start = interval.start - group_start\n",
    "                slice_stop = slice_start + (interval.stop - interval.start)\n",
    "                group_score[..., slice_start:slice_stop] = \\\n",
    "                    np.maximum(group_score[..., slice_start:slice_stop],\n",
    "                               scores[idx, ...])\n",
    "            if signal_dims == 1:\n",
    "                group_score = group_score.squeeze(axis=0)\n",
    "            yield Interval(group_chrom, group_start, group_stop), group_score\n",
    "            \n",
    "def interval_score_pairs(intervals, scores, merge_type):\n",
    "    return (izip(intervals, scores) if merge_type is None\n",
    "            else merged_scores(scores, intervals, merge_type))\n",
    "\n",
    "def _write_1D_deeplift_track(scores, intervals, file_prefix, merge_type='max',\n",
    "                             CHROM_SIZES='/mnt/data/annotations/by_release/hg19.GRCh37/hg19.chrom.sizes'):\n",
    "    assert scores.ndim == 2\n",
    "\n",
    "    bedgraph = file_prefix + '.bedGraph'\n",
    "    bigwig = file_prefix + '.bw'\n",
    "\n",
    "    print 'Writing 1D track of shape: {}'.format(scores.shape)\n",
    "    print 'Writing to file: {}'.format(bigwig)\n",
    "\n",
    "    with open(bedgraph, 'w') as fp:\n",
    "        for interval, score in interval_score_pairs(intervals, scores,\n",
    "                                                    merge_type):\n",
    "            chrom = interval.chrom\n",
    "            start = interval.start\n",
    "            for score_idx, val in enumerate(score):\n",
    "                fp.write('%s\\t%d\\t%d\\t%g\\n' % (chrom,\n",
    "                                               start + score_idx,\n",
    "                                               start + score_idx + 1,\n",
    "                                               val))\n",
    "    print 'Wrote bedgraph.'\n",
    "\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            ['wigToBigWig', bedgraph, CHROM_SIZES, bigwig],\n",
    "            stderr=subprocess.STDOUT)\n",
    "        print 'wigToBigWig output: {}'.format(output)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print 'wigToBigWig terminated with exit code {}'.format(\n",
    "            e.returncode)\n",
    "        print 'output was:\\n' + e.output\n",
    "\n",
    "    print 'Wrote bigwig.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our GAN Model (based on vanilla gan)\n",
    "class GAN():\n",
    "    def __init__(self, window_size, sample_num,\n",
    "                 X_train, y_train,\n",
    "                 X_val, y_val,\n",
    "                 X_test, y_test,\n",
    "                 model_dir, srv_dir):\n",
    "        \n",
    "        # Set train/val/test\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        \n",
    "        self.model_dir = model_dir\n",
    "        self.srv_dir = srv_dir\n",
    "        \n",
    "        # Basic parameters\n",
    "        # 1) Window size of input/ouput signal\n",
    "        self.window_size = window_size\n",
    "        # 2) Number of channels\n",
    "        self.channels = 5\n",
    "        # 3) Input and Output shape\n",
    "        self.input_shape = (self.window_size, self.channels,)\n",
    "        self.output_shape = (self.window_size, 1,)\n",
    "        # 4) Total number of samples\n",
    "        self.sample_num = sample_num\n",
    "        \n",
    "        # Adam optimizer for generator\n",
    "        optimizer = Adam(0.002, beta_1=0.5)\n",
    "        # Adam optimizer for discriminator\n",
    "        doptimizer = Adam(0.0005, beta_1=0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', \n",
    "                                   optimizer=doptimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy',\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=self.input_shape)\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity \n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy',\n",
    "                              optimizer=optimizer)\n",
    "        print \"Combined Model\"\n",
    "        print self.combined.summary()\n",
    "\n",
    "    def build_generator(self):\n",
    "        # Generator\n",
    "        # 1) 32 * window_size Conv1D layers with RELU and Dropout\n",
    "\n",
    "        noise_shape = self.input_shape\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv1D(hidden_filters_1,\n",
    "                         hidden_kernel_size_1,\n",
    "                         padding=\"same\",\n",
    "                         strides=1,\n",
    "                         input_shape=noise_shape,\n",
    "                         activation='relu',\n",
    "                         name='gen_conv1d_1'))\n",
    "        model.add(Dropout(dropout_rate,\n",
    "                  name='gen_dropout_1'))\n",
    "\n",
    "        # 2) 1 * 16 Conv1D layers with Linear\n",
    "        # NOTE: All same padding\n",
    "        model.add(Conv1D(output_filters,\n",
    "                         output_kernel_size,\n",
    "                         padding='same',\n",
    "                         strides=1,\n",
    "                         activation='linear',\n",
    "                         name='gen_conv1d_output'))\n",
    "        \n",
    "        print \"Generator\"\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "        \n",
    "        # load weights for generator if specified\n",
    "        if does_load_model:\n",
    "            print \"-\"*50\n",
    "            print model.get_weights()\n",
    "            model.load_weights(model_path, by_name=True)\n",
    "            print \"-\"*50\n",
    "            print model.get_weights()\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        # Discriminator\n",
    "        # 1) 16 * 200 Conv1D with LeakyRelu, Dropout, and BatchNorm\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv1D(hidden_filters_1,\n",
    "                         200,\n",
    "                         padding=\"valid\",\n",
    "                         strides=1,\n",
    "                         input_shape=self.output_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        # 2) Average Pooling, Flatten, Dense, and LeakyRelu\n",
    "        model.add(AveragePooling1D(25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(int(window_size/16)))\n",
    "        model.add(LeakyReLU(alpha=0.2)) \n",
    "        \n",
    "        # 3) Final output with sigmoid\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        print \"Discriminator\"\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.output_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size):\n",
    "        \n",
    "        max_pearson = -1.0\n",
    "        \n",
    "        # size of the half of the batch\n",
    "        half_batch = int(batch_size / 2)\n",
    "            \n",
    "        d_loss_real, d_loss_fake, g_loss = [1, 0], [1, 0], [1, 0]\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # list for storing losses/accuracies for both discriminator and generator\n",
    "            d_losses, d_accuracies, g_losses = [], [], []\n",
    "            \n",
    "            # sufficient number of minibatches for each epoch\n",
    "            for _minibatch_idx in range(128):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random half batch of images\n",
    "                dis_idx = np.random.randint(0, y_train.shape[0], half_batch)\n",
    "                # Get the real images\n",
    "                imgs = y_train[dis_idx]\n",
    "\n",
    "                # Generate a half batch of new images\n",
    "                dis_noise = self.X_train[dis_idx]\n",
    "                gen_imgs = self.generator.predict(dis_noise)\n",
    "                \n",
    "                # Train the discriminator with label smoothing\n",
    "                if smooth_rate > 0.0:\n",
    "                    smoothed_idx = np.random.choice(half_batch, int(half_batch*smooth_rate), replace=False)\n",
    "                smoothed_labels = np.ones((half_batch, 1))\n",
    "                if smooth_rate > 0.0:\n",
    "                    smoothed_labels[smoothed_idx] = 0\n",
    "                \n",
    "                # Train the discriminator for every d_train_freq minibatch\n",
    "                if _minibatch_idx % d_train_freq == 0:\n",
    "                    # 1) Concatenate the real images and generated images\n",
    "                    all_imgs = np.concatenate([imgs, gen_imgs])\n",
    "                    # 2) Concatenate the real labels and labels for generated images\n",
    "                    all_labels = np.concatenate([smoothed_labels, np.zeros((half_batch, 1))])\n",
    "                    # 3) Shuffle two lists in same order\n",
    "                    def unison_shuffled_copies(a, b):\n",
    "                        assert len(a) == len(b)\n",
    "                        p = np.random.permutation(len(a))\n",
    "                        return a[p], b[p]\n",
    "                    all_imgs, all_labels = unison_shuffled_copies(all_imgs, all_labels)\n",
    "                    # 4) Train the discriminator for current batch\n",
    "                    d_loss = self.discriminator.train_on_batch(all_imgs, all_labels)\n",
    "                                    \n",
    "                    # ---------------------\n",
    "                    # Store loss and accuracy\n",
    "                    # ---------------------\n",
    "                    d_losses.append(d_loss[0])\n",
    "                    d_accuracies.append(d_loss[1])\n",
    "                \n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "                \n",
    "                # 1) Randomly choose the batch number of indices\n",
    "                gen_idx = np.random.randint(0, y_train.shape[0], batch_size)\n",
    "                # 2) Get the randomly chosen inputs (noises in traditional gan)\n",
    "                gen_noise = self.X_train[gen_idx]\n",
    "                # 3) Create the labels for noises\n",
    "                # The generator wants the discriminator to label the generated samples as valid (ones)\n",
    "                valid_y = np.array([1] * batch_size)\n",
    "\n",
    "                # 4) Train the generator\n",
    "                # Discriminator is no longer trained at this point\n",
    "                # Generator do its best to fake discriminator\n",
    "                g_loss = self.combined.train_on_batch(gen_noise, valid_y)\n",
    "\n",
    "                g_losses.append(g_loss)\n",
    "            \n",
    "            # ---------------------\n",
    "            # Convert each histories into numpy arrays to get means\n",
    "            # ---------------------\n",
    "            d_losses = np.array(d_losses)\n",
    "            d_accuracies = np.array(d_accuracies)\n",
    "            g_losses = np.array(g_losses)\n",
    "            \n",
    "            # ---------------------\n",
    "            # Get generator's prediction and compute overall pearson on train set\n",
    "            # ---------------------\n",
    "            predictions = self.generator.predict(self.X_train).flatten()\n",
    "            avg_pearson = pearsonr(predictions, self.y_train.flatten())\n",
    "            print \"Pearson R on Train set: {}\".format(avg_pearson)\n",
    "            \n",
    "            # ---------------------\n",
    "            # Get generator's prediction and compute overall pearson on validation set\n",
    "            # ---------------------\n",
    "            val_predictions = self.generator.predict(self.X_val).flatten()\n",
    "            avg_val_pearson = pearsonr(val_predictions, self.y_val.flatten())\n",
    "            print \"Pearson R on Val set: {}\".format(avg_val_pearson)\n",
    "            \n",
    "            # if current pearson on validation set is greatest so far, update the max pearson,\n",
    "            # and \n",
    "            if max_pearson < avg_val_pearson:\n",
    "                print \"Perason on val improved from {} to {}\".format(max_pearson, avg_val_pearson)\n",
    "                _write_1D_deeplift_track(predictions.reshape(self.X_train.shape[0], self.window_size),\n",
    "                                         normalized_train_intervals, os.path.join(self.srv_dir, 'train'))\n",
    "                _write_1D_deeplift_track(val_predictions.reshape(self.X_val.shape[0], self.window_size),\n",
    "                                         normalized_val_intervals, os.path.join(self.srv_dir, 'val'))\n",
    "                f = open(os.path.join(self.srv_dir, 'meta.txt'), 'wb')\n",
    "                f.write(str(epoch) + \" \" + str(avg_pearson) + \"  \" + str(avg_val_pearson) + \"\\n\")\n",
    "                max_pearson = avg_val_pearson\n",
    "                \n",
    "                # ---------------------\n",
    "                # Get generator's prediction and compute overall pearson on test set\n",
    "                # ---------------------\n",
    "                test_predictions = self.generator.predict(self.X_test).flatten()\n",
    "                avg_test_pearson = pearsonr(test_predictions, self.y_test.flatten())\n",
    "                print \"Pearson R on Test set: {}\".format(avg_test_pearson)\n",
    "                f.write(\"Test Pearson: \" + str(avg_test_pearson))\n",
    "                f.close()\n",
    "                _write_1D_deeplift_track(val_predictions.reshape(self.X_test.shape[0], self.window_size),\n",
    "                                         normalized_test_intervals, os.path.join(self.srv_dir, 'test'))\n",
    "                \n",
    "                self.generator.save(os.path.join(self.model_dir, 'best_generator.h5'))\n",
    "                self.discriminator.save(os.path.join(self.model_dir, 'best_discriminator.h5'))\n",
    "                \n",
    "            \n",
    "            # Print the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_losses.mean(), 100.0*d_accuracies.mean(), g_losses.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for computing Pearson R in Keras\n",
    "def pearson(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(np.multiply(xm,ym))\n",
    "    r_den = K.sqrt(np.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model...\n",
      "Discriminator\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 1802, 32)          6432      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1802, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1802, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1802, 32)          128       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_3 (Average (None, 72, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 125)               288125    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 126       \n",
      "=================================================================\n",
      "Total params: 294,811\n",
      "Trainable params: 294,747\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Generator\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gen_conv1d_1 (Conv1D)        (None, 2001, 32)          320192    \n",
      "_________________________________________________________________\n",
      "gen_dropout_1 (Dropout)      (None, 2001, 32)          0         \n",
      "_________________________________________________________________\n",
      "gen_conv1d_output (Conv1D)   (None, 2001, 1)           1025      \n",
      "=================================================================\n",
      "Total params: 321,217\n",
      "Trainable params: 321,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "--------------------------------------------------\n",
      "[array([[[  5.71743492e-03,   9.56593081e-04,  -8.06894340e-03, ...,\n",
      "          -5.59036154e-03,   3.67791392e-03,   1.98710896e-03],\n",
      "        [ -6.41410612e-03,   8.45854916e-03,   2.61614751e-03, ...,\n",
      "          -2.73425225e-03,  -8.27156100e-03,   8.51721503e-03],\n",
      "        [  5.02969045e-03,   1.46491732e-03,  -6.38322532e-03, ...,\n",
      "           6.21237140e-03,   3.47575545e-03,  -1.74537254e-03],\n",
      "        [  6.52319193e-03,   4.18597180e-03,   1.98998488e-03, ...,\n",
      "          -7.73960818e-03,   1.28455926e-03,   2.30395235e-04],\n",
      "        [  4.21434827e-03,  -7.76548823e-03,  -1.98264886e-03, ...,\n",
      "          -8.30762740e-03,   6.63560629e-03,  -5.92431985e-04]],\n",
      "\n",
      "       [[ -1.05919223e-03,  -4.19865455e-03,  -4.54559363e-03, ...,\n",
      "           1.10943522e-03,  -7.42505444e-03,   6.63892739e-03],\n",
      "        [ -8.49580485e-03,  -7.55824149e-03,   2.07061507e-03, ...,\n",
      "           1.11208297e-04,  -6.02299813e-03,   3.78516689e-03],\n",
      "        [ -6.22149929e-03,  -4.71510272e-03,   7.17380457e-03, ...,\n",
      "          -9.73509625e-04,   7.18093105e-03,  -1.38378702e-03],\n",
      "        [ -4.56275558e-03,   8.43676366e-03,   1.31344143e-03, ...,\n",
      "           5.32802660e-03,  -5.73340384e-03,  -4.37583821e-03],\n",
      "        [  6.33668248e-03,  -3.73596465e-03,   6.59257174e-04, ...,\n",
      "           7.83808157e-03,   5.04095666e-03,  -3.19237914e-03]],\n",
      "\n",
      "       [[  3.05458158e-03,   6.42184354e-03,   1.95699837e-03, ...,\n",
      "           1.08605623e-03,  -7.70503143e-03,  -7.92105496e-03],\n",
      "        [  5.66491485e-03,  -1.34636601e-03,   8.39977339e-03, ...,\n",
      "           6.27334975e-03,   4.23950329e-03,  -5.54396771e-04],\n",
      "        [  1.69955194e-04,  -1.21625280e-03,   1.99156906e-03, ...,\n",
      "          -8.81260354e-03,   3.46223637e-03,  -6.01086300e-03],\n",
      "        [  2.22541764e-03,   8.46902840e-03,   8.69435072e-03, ...,\n",
      "           3.17570847e-03,   3.63627542e-03,   5.86527400e-04],\n",
      "        [  7.76644982e-03,   5.68755902e-04,   6.61974773e-03, ...,\n",
      "          -3.55340447e-03,   6.90941140e-03,   2.45731883e-03]],\n",
      "\n",
      "       ..., \n",
      "       [[ -1.76502438e-03,  -4.73782327e-03,   1.91605743e-03, ...,\n",
      "           6.80290349e-03,   2.43989751e-04,   3.02332081e-03],\n",
      "        [  3.77455726e-04,   5.50591294e-03,  -3.75351729e-03, ...,\n",
      "          -1.65524287e-03,   4.34723031e-03,  -1.61604024e-04],\n",
      "        [  4.06032987e-03,  -7.69759202e-03,   5.55647537e-03, ...,\n",
      "           3.99059150e-03,  -6.23570988e-03,   1.66818686e-03],\n",
      "        [  6.72296993e-03,  -6.72754645e-03,  -6.31837454e-03, ...,\n",
      "          -4.28076973e-03,  -5.91628626e-03,   8.35955516e-03],\n",
      "        [  9.51396301e-04,  -1.47300772e-04,   5.55671565e-03, ...,\n",
      "          -3.44487466e-03,  -4.54668654e-03,  -3.82768735e-03]],\n",
      "\n",
      "       [[ -3.97237018e-05,  -4.93009342e-03,   1.00993924e-03, ...,\n",
      "           3.33553273e-03,   4.99782246e-03,  -3.08575900e-03],\n",
      "        [ -6.41249120e-04,  -4.38391231e-04,   3.35861836e-03, ...,\n",
      "          -5.09128906e-04,   5.88539429e-03,  -7.15909293e-03],\n",
      "        [ -7.83748180e-03,  -4.68949694e-03,  -3.87644488e-03, ...,\n",
      "          -4.04520426e-03,   4.77991626e-03,  -2.31242180e-03],\n",
      "        [ -1.59396604e-03,   1.51330326e-03,   6.35440089e-03, ...,\n",
      "          -8.78952630e-03,  -1.12663582e-03,  -3.31393443e-04],\n",
      "        [  1.34578627e-03,   8.86018947e-03,  -3.75841931e-03, ...,\n",
      "           7.60290585e-03,   2.44545843e-03,   3.50063574e-03]],\n",
      "\n",
      "       [[  4.18517180e-03,   7.12089799e-03,  -2.51595583e-03, ...,\n",
      "           6.91332482e-03,   6.10571634e-03,  -5.46010630e-03],\n",
      "        [ -5.68812992e-03,   8.93539004e-03,  -6.37346413e-03, ...,\n",
      "          -3.96786444e-03,   2.50964612e-03,   4.23053373e-03],\n",
      "        [  4.27288190e-03,  -8.60020518e-05,   5.13207167e-04, ...,\n",
      "          -4.26227320e-03,  -6.91712741e-03,   3.98116745e-03],\n",
      "        [ -2.52294866e-03,  -1.68001559e-03,   6.98772632e-03, ...,\n",
      "          -8.39541666e-04,   1.99611206e-03,  -6.86486252e-03],\n",
      "        [ -5.19366376e-03,   1.06970128e-03,  -8.83790664e-04, ...,\n",
      "           3.64583917e-04,  -6.60406286e-03,  -5.85867092e-03]]], dtype=float32), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.], dtype=float32), array([[[-0.04387287],\n",
      "        [-0.02108591],\n",
      "        [-0.05854336],\n",
      "        ..., \n",
      "        [ 0.05645536],\n",
      "        [ 0.03170165],\n",
      "        [ 0.01677519]],\n",
      "\n",
      "       [[-0.04539415],\n",
      "        [-0.00354879],\n",
      "        [-0.06676098],\n",
      "        ..., \n",
      "        [-0.0653723 ],\n",
      "        [ 0.05962422],\n",
      "        [-0.05454972]],\n",
      "\n",
      "       [[-0.02871957],\n",
      "        [-0.05223498],\n",
      "        [-0.07128893],\n",
      "        ..., \n",
      "        [-0.05934446],\n",
      "        [-0.07370491],\n",
      "        [ 0.04689299]],\n",
      "\n",
      "       ..., \n",
      "       [[-0.02819985],\n",
      "        [-0.03097657],\n",
      "        [ 0.06578179],\n",
      "        ..., \n",
      "        [-0.06316917],\n",
      "        [ 0.02664746],\n",
      "        [ 0.06669091]],\n",
      "\n",
      "       [[-0.03997843],\n",
      "        [-0.02269524],\n",
      "        [ 0.05157562],\n",
      "        ..., \n",
      "        [-0.01745988],\n",
      "        [ 0.06146225],\n",
      "        [ 0.01747354]],\n",
      "\n",
      "       [[-0.06297489],\n",
      "        [ 0.05400984],\n",
      "        [ 0.01547217],\n",
      "        ..., \n",
      "        [-0.00042614],\n",
      "        [-0.02267005],\n",
      "        [ 0.04024865]]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "--------------------------------------------------\n",
      "[array([[[  7.80140422e-03,  -5.68425097e-03,  -3.30807595e-03, ...,\n",
      "          -7.56293419e-04,  -1.31612420e-02,  -2.07759487e-03],\n",
      "        [  1.55153091e-03,  -5.90131897e-03,  -8.07849865e-04, ...,\n",
      "          -3.63257388e-03,  -1.52964210e-02,  -7.98088778e-03],\n",
      "        [  1.30543960e-02,  -3.24833614e-04,   2.58893333e-03, ...,\n",
      "          -2.96852505e-03,  -6.00321405e-03,  -1.24348365e-02],\n",
      "        [  2.10901350e-02,   1.82037652e-02,  -1.11784590e-02, ...,\n",
      "          -9.39051062e-03,   1.84451248e-02,  -3.43841664e-03],\n",
      "        [  1.49808973e-02,   4.44400869e-02,  -9.00911167e-03, ...,\n",
      "          -4.34400374e-03,   1.32570816e-02,  -1.17945680e-02]],\n",
      "\n",
      "       [[  5.72956109e-04,   2.14463123e-03,   1.36920344e-03, ...,\n",
      "          -1.21469181e-02,  -9.58680082e-03,  -5.90424845e-03],\n",
      "        [  1.10957166e-02,  -1.22155575e-02,   2.18214220e-04, ...,\n",
      "          -2.30260077e-03,  -1.00341607e-02,  -9.50034242e-03],\n",
      "        [  1.84579063e-02,   6.65391376e-03,   2.18446483e-03, ...,\n",
      "          -1.02900229e-02,  -1.34119643e-02,   6.44335640e-04],\n",
      "        [  1.53687913e-02,   1.85305923e-02,  -1.26206409e-02, ...,\n",
      "          -1.78432628e-03,  -2.30341102e-03,  -7.53228611e-04],\n",
      "        [  2.77396571e-02,   3.17627527e-02,   2.43991171e-03, ...,\n",
      "          -2.83298409e-03,   1.79263279e-02,  -2.21170089e-03]],\n",
      "\n",
      "       [[ -1.15996068e-02,   7.71989673e-03,  -4.97705489e-03, ...,\n",
      "          -8.30142479e-03,  -5.68771444e-04,  -5.52918704e-04],\n",
      "        [ -9.05818946e-04,  -1.53015349e-02,  -7.87034910e-03, ...,\n",
      "          -3.61373858e-03,  -1.01112863e-02,  -2.89903395e-03],\n",
      "        [  3.72285466e-03,   7.33145047e-03,   3.06088803e-03, ...,\n",
      "          -2.57315137e-03,   1.96554785e-04,  -9.57690366e-03],\n",
      "        [  3.30123752e-02,   2.43040100e-02,   1.89050927e-03, ...,\n",
      "          -1.21337250e-02,   1.33168781e-02,  -8.22338369e-03],\n",
      "        [  2.77121030e-02,   4.20399606e-02,  -9.14123747e-03, ...,\n",
      "          -8.13327730e-03,   1.25936307e-02,  -3.27681744e-04]],\n",
      "\n",
      "       ..., \n",
      "       [[  2.52813171e-03,   1.49052292e-02,  -1.14620943e-02, ...,\n",
      "          -7.91135430e-03,   6.42095972e-03,  -1.29134785e-02],\n",
      "        [  6.07001130e-03,  -1.75142717e-02,   1.71150500e-03, ...,\n",
      "          -8.11370276e-03,   3.96935828e-03,  -8.04954115e-03],\n",
      "        [  2.02145427e-02,   4.81210789e-03,  -8.65291618e-03, ...,\n",
      "          -8.75960384e-03,   2.61951052e-03,  -1.40340989e-02],\n",
      "        [  1.74272712e-02,   4.75441944e-03,  -7.14888144e-03, ...,\n",
      "          -1.11232009e-02,   2.71056276e-02,  -8.82878527e-03],\n",
      "        [  2.30731200e-02,  -1.94286145e-02,  -1.36876069e-02, ...,\n",
      "          -5.10188192e-03,   4.62934300e-02,   1.48754800e-03]],\n",
      "\n",
      "       [[  1.15652708e-03,   1.28918956e-03,  -5.89920394e-03, ...,\n",
      "          -1.24090500e-02,  -1.49232813e-03,  -3.69054731e-04],\n",
      "        [  2.45330879e-03,  -1.51712878e-03,  -8.27793777e-03, ...,\n",
      "          -7.35456450e-03,   2.89417244e-03,  -1.20497646e-03],\n",
      "        [  2.36488339e-02,  -3.99246020e-03,  -2.91186222e-03, ...,\n",
      "          -1.15710720e-02,   6.51354296e-03,  -4.76245163e-03],\n",
      "        [  1.19483853e-02,   1.18980824e-03,   3.79775697e-03, ...,\n",
      "          -1.26554659e-02,   2.16579251e-02,  -3.90488567e-04],\n",
      "        [  2.51432844e-02,   6.37300406e-03,  -7.71799637e-03, ...,\n",
      "          -1.26553811e-02,   3.96520905e-02,  -2.47501279e-03]],\n",
      "\n",
      "       [[ -6.36896258e-03,   1.76108233e-03,  -9.77804884e-03, ...,\n",
      "          -8.06820113e-03,  -2.27928674e-03,  -4.62262332e-03],\n",
      "        [  4.64546774e-03,  -1.91012677e-02,  -2.50044861e-03, ...,\n",
      "          -4.42389341e-04,  -2.89203854e-05,   3.48092895e-03],\n",
      "        [  2.12463122e-02,  -2.13750801e-03,  -6.00008108e-03, ...,\n",
      "          -1.29441675e-02,   8.00874829e-03,  -7.06379628e-03],\n",
      "        [  5.63262310e-03,  -6.16362644e-03,  -9.24547017e-03, ...,\n",
      "          -1.36477100e-02,   2.24031713e-02,  -3.18711367e-03],\n",
      "        [  4.70744399e-03,   3.80863925e-03,  -2.67129252e-03, ...,\n",
      "          -9.57153272e-03,   4.56856079e-02,   5.03705931e-04]]], dtype=float32), array([ 0.1232998 ,  0.11833536, -0.00556394,  0.11763768, -0.00555487,\n",
      "        0.12409153, -0.00556222, -0.00554229,  0.13523737,  0.14813882,\n",
      "        0.14922561,  0.15491167, -0.00556701, -0.00556154, -0.00556405,\n",
      "       -0.00556378, -0.00556604, -0.00556638, -0.00125248,  0.16916552,\n",
      "       -0.00556108, -0.00555979,  0.16129296, -0.00556417, -0.00555449,\n",
      "        0.15497892, -0.00553238,  0.17176498,  0.13744788, -0.00555774,\n",
      "        0.10444979, -0.00555485], dtype=float32), array([[[ 0.06940675],\n",
      "        [-0.04223299],\n",
      "        [-0.00280784],\n",
      "        ..., \n",
      "        [-0.02957064],\n",
      "        [ 0.00632906],\n",
      "        [-0.0132335 ]],\n",
      "\n",
      "       [[-0.02617549],\n",
      "        [ 0.00687119],\n",
      "        [ 0.026844  ],\n",
      "        ..., \n",
      "        [ 0.07589156],\n",
      "        [ 0.06681927],\n",
      "        [-0.00660096]],\n",
      "\n",
      "       [[ 0.02288536],\n",
      "        [ 0.00645277],\n",
      "        [-0.01302421],\n",
      "        ..., \n",
      "        [-0.0031528 ],\n",
      "        [ 0.042993  ],\n",
      "        [-0.01468077]],\n",
      "\n",
      "       ..., \n",
      "       [[ 0.01628296],\n",
      "        [ 0.0565566 ],\n",
      "        [-0.04043021],\n",
      "        ..., \n",
      "        [-0.06462874],\n",
      "        [-0.01620904],\n",
      "        [-0.01467474]],\n",
      "\n",
      "       [[ 0.0896868 ],\n",
      "        [ 0.06600266],\n",
      "        [-0.04947671],\n",
      "        ..., \n",
      "        [ 0.01787736],\n",
      "        [-0.00886448],\n",
      "        [-0.05908247]],\n",
      "\n",
      "       [[-0.00976067],\n",
      "        [ 0.01483573],\n",
      "        [-0.00786494],\n",
      "        ..., \n",
      "        [-0.0497314 ],\n",
      "        [ 0.0342944 ],\n",
      "        [ 0.00733144]]], dtype=float32), array([ 0.18373644], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 2001, 5)           0         \n",
      "_________________________________________________________________\n",
      "model_8 (Model)              (None, 2001, 1)           321217    \n",
      "_________________________________________________________________\n",
      "model_7 (Model)              (None, 1)                 294811    \n",
      "=================================================================\n",
      "Total params: 616,028\n",
      "Trainable params: 321,217\n",
      "Non-trainable params: 294,811\n",
      "_________________________________________________________________\n",
      "None\n",
      "Pearson R on Train set: (-0.13246292, 0.0)\n",
      "Pearson R on Val set: (-0.17326175, 0.0)\n",
      "Perason on val improved from -1.0 to (-0.17326175, 0.0)\n",
      "Writing 1D track of shape: (8000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "0 [D loss: 1.073546, acc.: 45.70%] [G loss: 0.489385]\n",
      "Pearson R on Train set: (0.18059379, 0.0)\n",
      "Pearson R on Val set: (0.20969525, 0.0)\n",
      "Perason on val improved from (-0.17326175, 0.0) to (0.20969525, 0.0)\n",
      "Writing 1D track of shape: (8000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "1 [D loss: 0.666393, acc.: 67.38%] [G loss: 0.635949]\n",
      "Pearson R on Train set: (0.29482883, 0.0)\n",
      "Pearson R on Val set: (0.32463741, 0.0)\n",
      "Perason on val improved from (0.20969525, 0.0) to (0.32463741, 0.0)\n",
      "Writing 1D track of shape: (8000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "2 [D loss: 0.652640, acc.: 66.02%] [G loss: 0.913835]\n",
      "Pearson R on Train set: (0.2696051, 0.0)\n",
      "Pearson R on Val set: (0.28685153, 0.0)\n",
      "3 [D loss: 0.594217, acc.: 71.09%] [G loss: 0.791863]\n",
      "Pearson R on Train set: (0.20926091, 0.0)\n",
      "Pearson R on Val set: (0.24031085, 0.0)\n",
      "4 [D loss: 0.574919, acc.: 68.07%] [G loss: 0.892464]\n",
      "Pearson R on Train set: (0.25426212, 0.0)\n",
      "Pearson R on Val set: (0.25995305, 0.0)\n",
      "5 [D loss: 0.343534, acc.: 84.96%] [G loss: 1.086198]\n",
      "Pearson R on Train set: (0.2685807, 0.0)\n",
      "Pearson R on Val set: (0.27746317, 0.0)\n",
      "6 [D loss: 0.522674, acc.: 78.32%] [G loss: 0.888452]\n",
      "Pearson R on Train set: (0.12326457, 0.0)\n",
      "Pearson R on Val set: (0.14120783, 0.0)\n",
      "7 [D loss: 0.535346, acc.: 75.29%] [G loss: 0.856771]\n",
      "Pearson R on Train set: (0.28256029, 0.0)\n",
      "Pearson R on Val set: (0.31067926, 0.0)\n",
      "8 [D loss: 0.404040, acc.: 82.62%] [G loss: 1.063160]\n",
      "Pearson R on Train set: (0.31944281, 0.0)\n",
      "Pearson R on Val set: (0.34767255, 0.0)\n",
      "Perason on val improved from (0.32463741, 0.0) to (0.34767255, 0.0)\n",
      "Writing 1D track of shape: (8000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "9 [D loss: 0.365514, acc.: 85.74%] [G loss: 0.942775]\n",
      "Pearson R on Train set: (0.31052482, 0.0)\n",
      "Pearson R on Val set: (0.33094329, 0.0)\n",
      "10 [D loss: 0.390472, acc.: 84.08%] [G loss: 0.973658]\n",
      "Pearson R on Train set: (0.18792203, 0.0)\n",
      "Pearson R on Val set: (0.22199072, 0.0)\n",
      "11 [D loss: 0.429419, acc.: 82.23%] [G loss: 1.087558]\n",
      "Pearson R on Train set: (0.17113782, 0.0)\n",
      "Pearson R on Val set: (0.16416918, 0.0)\n",
      "12 [D loss: 0.315165, acc.: 87.30%] [G loss: 1.507126]\n",
      "Pearson R on Train set: (0.18412663, 0.0)\n",
      "Pearson R on Val set: (0.19272992, 0.0)\n",
      "13 [D loss: 0.637701, acc.: 70.80%] [G loss: 1.444290]\n",
      "Pearson R on Train set: (0.15119343, 0.0)\n",
      "Pearson R on Val set: (0.14917159, 0.0)\n",
      "14 [D loss: 0.405733, acc.: 81.93%] [G loss: 1.107543]\n",
      "Pearson R on Train set: (0.09798795, 0.0)\n",
      "Pearson R on Val set: (0.069689877, 0.0)\n",
      "15 [D loss: 0.528641, acc.: 74.61%] [G loss: 1.109919]\n",
      "Pearson R on Train set: (0.11242714, 0.0)\n",
      "Pearson R on Val set: (0.092638351, 0.0)\n",
      "16 [D loss: 0.539445, acc.: 73.05%] [G loss: 1.044150]\n",
      "Pearson R on Train set: (0.13315664, 0.0)\n",
      "Pearson R on Val set: (0.12162649, 0.0)\n",
      "17 [D loss: 0.479480, acc.: 76.27%] [G loss: 1.067693]\n",
      "Pearson R on Train set: (0.12385237, 0.0)\n",
      "Pearson R on Val set: (0.094388746, 0.0)\n",
      "18 [D loss: 0.494064, acc.: 76.86%] [G loss: 1.149447]\n",
      "Pearson R on Train set: (0.12518209, 0.0)\n",
      "Pearson R on Val set: (0.13194478, 0.0)\n",
      "19 [D loss: 0.447491, acc.: 84.57%] [G loss: 1.168251]\n",
      "Pearson R on Train set: (0.15222609, 0.0)\n",
      "Pearson R on Val set: (0.16932584, 0.0)\n",
      "20 [D loss: 0.449285, acc.: 83.98%] [G loss: 0.937439]\n",
      "Pearson R on Train set: (0.021581963, 0.0)\n",
      "Pearson R on Val set: (0.048248731, 0.0)\n",
      "21 [D loss: 0.462498, acc.: 82.52%] [G loss: 0.971107]\n",
      "Pearson R on Train set: (0.17581132, 0.0)\n",
      "Pearson R on Val set: (0.17476489, 0.0)\n",
      "22 [D loss: 0.443900, acc.: 84.77%] [G loss: 0.987159]\n",
      "Pearson R on Train set: (0.089144193, 0.0)\n",
      "Pearson R on Val set: (0.10615612, 0.0)\n",
      "23 [D loss: 0.350035, acc.: 87.89%] [G loss: 0.933528]\n",
      "Pearson R on Train set: (0.12693858, 0.0)\n",
      "Pearson R on Val set: (0.13527609, 0.0)\n",
      "24 [D loss: 0.387150, acc.: 85.84%] [G loss: 1.087443]\n",
      "Pearson R on Train set: (0.16220322, 0.0)\n",
      "Pearson R on Val set: (0.18764244, 0.0)\n",
      "25 [D loss: 0.363599, acc.: 86.72%] [G loss: 0.874253]\n",
      "Pearson R on Train set: (0.22459511, 0.0)\n",
      "Pearson R on Val set: (0.23460719, 0.0)\n",
      "26 [D loss: 0.353093, acc.: 86.72%] [G loss: 0.888837]\n",
      "Pearson R on Train set: (0.25364587, 0.0)\n",
      "Pearson R on Val set: (0.26691043, 0.0)\n",
      "27 [D loss: 0.307642, acc.: 90.04%] [G loss: 0.943498]\n",
      "Pearson R on Train set: (0.25924867, 0.0)\n",
      "Pearson R on Val set: (0.25225729, 0.0)\n",
      "28 [D loss: 0.326431, acc.: 88.87%] [G loss: 0.871222]\n",
      "Pearson R on Train set: (0.26225832, 0.0)\n",
      "Pearson R on Val set: (0.2639848, 0.0)\n",
      "29 [D loss: 0.292005, acc.: 89.55%] [G loss: 1.019695]\n",
      "Pearson R on Train set: (0.28700641, 0.0)\n",
      "Pearson R on Val set: (0.2901133, 0.0)\n",
      "30 [D loss: 0.236067, acc.: 92.29%] [G loss: 0.969137]\n",
      "Pearson R on Train set: (0.25753275, 0.0)\n",
      "Pearson R on Val set: (0.25449598, 0.0)\n",
      "31 [D loss: 0.254891, acc.: 91.50%] [G loss: 0.920956]\n",
      "Pearson R on Train set: (0.28030404, 0.0)\n",
      "Pearson R on Val set: (0.27215046, 0.0)\n",
      "32 [D loss: 0.261501, acc.: 91.70%] [G loss: 0.955426]\n",
      "Pearson R on Train set: (0.30759695, 0.0)\n",
      "Pearson R on Val set: (0.29259679, 0.0)\n",
      "33 [D loss: 0.234547, acc.: 92.38%] [G loss: 0.965728]\n",
      "Pearson R on Train set: (0.25742522, 0.0)\n",
      "Pearson R on Val set: (0.26677334, 0.0)\n",
      "34 [D loss: 0.247615, acc.: 92.87%] [G loss: 0.951267]\n",
      "Pearson R on Train set: (0.26915327, 0.0)\n",
      "Pearson R on Val set: (0.27348664, 0.0)\n",
      "35 [D loss: 0.227587, acc.: 93.07%] [G loss: 0.983179]\n",
      "Pearson R on Train set: (0.32588091, 0.0)\n",
      "Pearson R on Val set: (0.32595855, 0.0)\n",
      "36 [D loss: 0.212699, acc.: 93.65%] [G loss: 0.987417]\n",
      "Pearson R on Train set: (0.27813616, 0.0)\n",
      "Pearson R on Val set: (0.29589954, 0.0)\n",
      "37 [D loss: 0.260687, acc.: 91.60%] [G loss: 1.030553]\n",
      "Pearson R on Train set: (0.26324067, 0.0)\n",
      "Pearson R on Val set: (0.27644968, 0.0)\n",
      "38 [D loss: 0.228469, acc.: 93.07%] [G loss: 1.000761]\n",
      "Pearson R on Train set: (0.28517208, 0.0)\n",
      "Pearson R on Val set: (0.28929216, 0.0)\n",
      "39 [D loss: 0.266002, acc.: 91.02%] [G loss: 1.162757]\n",
      "Pearson R on Train set: (0.28064361, 0.0)\n",
      "Pearson R on Val set: (0.30686012, 0.0)\n",
      "40 [D loss: 0.275952, acc.: 90.33%] [G loss: 1.012741]\n",
      "Pearson R on Train set: (0.29857361, 0.0)\n",
      "Pearson R on Val set: (0.30075124, 0.0)\n",
      "41 [D loss: 0.208516, acc.: 93.46%] [G loss: 1.027559]\n",
      "Pearson R on Train set: (0.29459405, 0.0)\n",
      "Pearson R on Val set: (0.30802906, 0.0)\n",
      "42 [D loss: 0.201923, acc.: 92.68%] [G loss: 1.108843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson R on Train set: (0.1396088, 0.0)\n",
      "Pearson R on Val set: (0.17076859, 0.0)\n",
      "43 [D loss: 0.202737, acc.: 95.21%] [G loss: 0.999713]\n",
      "Pearson R on Train set: (0.195499, 0.0)\n",
      "Pearson R on Val set: (0.21353495, 0.0)\n",
      "44 [D loss: 0.271274, acc.: 90.92%] [G loss: 1.282908]\n",
      "Pearson R on Train set: (0.29523858, 0.0)\n",
      "Pearson R on Val set: (0.30124798, 0.0)\n",
      "45 [D loss: 0.214921, acc.: 92.38%] [G loss: 1.194173]\n",
      "Pearson R on Train set: (0.26748991, 0.0)\n",
      "Pearson R on Val set: (0.27928147, 0.0)\n",
      "46 [D loss: 0.156534, acc.: 95.31%] [G loss: 1.083248]\n",
      "Pearson R on Train set: (0.14695144, 0.0)\n",
      "Pearson R on Val set: (0.18316913, 0.0)\n",
      "47 [D loss: 0.207688, acc.: 93.65%] [G loss: 1.100943]\n",
      "Pearson R on Train set: (0.27692011, 0.0)\n",
      "Pearson R on Val set: (0.28581348, 0.0)\n",
      "48 [D loss: 0.174338, acc.: 94.24%] [G loss: 0.990468]\n",
      "Pearson R on Train set: (0.19383903, 0.0)\n",
      "Pearson R on Val set: (0.21937162, 0.0)\n",
      "49 [D loss: 0.145149, acc.: 96.00%] [G loss: 1.052385]\n",
      "Pearson R on Train set: (0.28886703, 0.0)\n",
      "Pearson R on Val set: (0.30937949, 0.0)\n",
      "50 [D loss: 0.173226, acc.: 93.75%] [G loss: 1.037182]\n",
      "Pearson R on Train set: (0.27738926, 0.0)\n",
      "Pearson R on Val set: (0.30972672, 0.0)\n",
      "51 [D loss: 0.186828, acc.: 94.04%] [G loss: 1.133827]\n",
      "Pearson R on Train set: (0.28165001, 0.0)\n",
      "Pearson R on Val set: (0.29278976, 0.0)\n",
      "52 [D loss: 0.224346, acc.: 92.29%] [G loss: 1.132725]\n",
      "Pearson R on Train set: (0.25260231, 0.0)\n",
      "Pearson R on Val set: (0.26521963, 0.0)\n",
      "53 [D loss: 0.147966, acc.: 95.02%] [G loss: 1.019050]\n",
      "Pearson R on Train set: (0.25120446, 0.0)\n",
      "Pearson R on Val set: (0.27442035, 0.0)\n",
      "54 [D loss: 0.177790, acc.: 94.34%] [G loss: 1.039519]\n",
      "Pearson R on Train set: (0.3166295, 0.0)\n",
      "Pearson R on Val set: (0.33154035, 0.0)\n",
      "55 [D loss: 0.156866, acc.: 94.82%] [G loss: 1.079328]\n",
      "Pearson R on Train set: (0.30767924, 0.0)\n",
      "Pearson R on Val set: (0.31757393, 0.0)\n",
      "56 [D loss: 0.194072, acc.: 93.26%] [G loss: 1.092588]\n",
      "Pearson R on Train set: (0.27043661, 0.0)\n",
      "Pearson R on Val set: (0.2890088, 0.0)\n",
      "57 [D loss: 0.185279, acc.: 94.43%] [G loss: 1.082576]\n",
      "Pearson R on Train set: (0.27636337, 0.0)\n",
      "Pearson R on Val set: (0.29223233, 0.0)\n",
      "58 [D loss: 0.164777, acc.: 95.31%] [G loss: 1.141088]\n",
      "Pearson R on Train set: (0.32962933, 0.0)\n",
      "Pearson R on Val set: (0.35599601, 0.0)\n",
      "Perason on val improved from (0.34767255, 0.0) to (0.35599601, 0.0)\n",
      "Writing 1D track of shape: (8000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "59 [D loss: 0.127116, acc.: 96.00%] [G loss: 1.110312]\n",
      "Pearson R on Train set: (0.30696622, 0.0)\n",
      "Pearson R on Val set: (0.32191139, 0.0)\n",
      "60 [D loss: 0.193309, acc.: 93.75%] [G loss: 1.135617]\n",
      "Pearson R on Train set: (0.33762997, 0.0)\n",
      "Pearson R on Val set: (0.36000621, 0.0)\n",
      "Perason on val improved from (0.35599601, 0.0) to (0.36000621, 0.0)\n",
      "Writing 1D track of shape: (8000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2000, 2001)\n",
      "Writing to file: /srv/www/kundaje/jesikmin/gan_fixed_nonsmooth_nonwide/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "61 [D loss: 0.139371, acc.: 96.39%] [G loss: 1.109655]\n",
      "Pearson R on Train set: (0.14982004, 0.0)\n",
      "Pearson R on Val set: (0.19515757, 0.0)\n",
      "62 [D loss: 0.116567, acc.: 96.78%] [G loss: 1.136009]\n",
      "Pearson R on Train set: (0.30809873, 0.0)\n",
      "Pearson R on Val set: (0.32229215, 0.0)\n",
      "63 [D loss: 0.149551, acc.: 96.29%] [G loss: 1.099213]\n",
      "Pearson R on Train set: (0.26130712, 0.0)\n",
      "Pearson R on Val set: (0.2777963, 0.0)\n",
      "64 [D loss: 0.157149, acc.: 96.00%] [G loss: 1.106062]\n",
      "Pearson R on Train set: (0.23948635, 0.0)\n",
      "Pearson R on Val set: (0.25973099, 0.0)\n",
      "65 [D loss: 0.169491, acc.: 95.12%] [G loss: 1.111316]\n",
      "Pearson R on Train set: (0.23753458, 0.0)\n",
      "Pearson R on Val set: (0.26964149, 0.0)\n",
      "66 [D loss: 0.118306, acc.: 96.97%] [G loss: 1.055246]\n",
      "Pearson R on Train set: (0.29671898, 0.0)\n",
      "Pearson R on Val set: (0.30555537, 0.0)\n",
      "67 [D loss: 0.155894, acc.: 95.90%] [G loss: 1.099228]\n",
      "Pearson R on Train set: (0.20050485, 0.0)\n",
      "Pearson R on Val set: (0.23541462, 0.0)\n",
      "68 [D loss: 0.175079, acc.: 95.12%] [G loss: 1.151530]\n",
      "Pearson R on Train set: (0.28016186, 0.0)\n",
      "Pearson R on Val set: (0.29570958, 0.0)\n",
      "69 [D loss: 0.137714, acc.: 96.00%] [G loss: 1.084875]\n",
      "Pearson R on Train set: (0.30992332, 0.0)\n",
      "Pearson R on Val set: (0.33991668, 0.0)\n",
      "70 [D loss: 0.119215, acc.: 96.78%] [G loss: 1.110004]\n",
      "Pearson R on Train set: (0.26389548, 0.0)\n",
      "Pearson R on Val set: (0.27648097, 0.0)\n",
      "71 [D loss: 0.103092, acc.: 97.17%] [G loss: 1.143160]\n",
      "Pearson R on Train set: (0.21590914, 0.0)\n",
      "Pearson R on Val set: (0.23906156, 0.0)\n",
      "72 [D loss: 0.132577, acc.: 96.19%] [G loss: 1.040529]\n",
      "Pearson R on Train set: (0.25185364, 0.0)\n",
      "Pearson R on Val set: (0.26687869, 0.0)\n",
      "73 [D loss: 0.150879, acc.: 95.90%] [G loss: 0.988158]\n",
      "Pearson R on Train set: (0.22325143, 0.0)\n",
      "Pearson R on Val set: (0.24321638, 0.0)\n",
      "74 [D loss: 0.104488, acc.: 97.46%] [G loss: 1.079936]\n",
      "Pearson R on Train set: (0.17346652, 0.0)\n",
      "Pearson R on Val set: (0.18813555, 0.0)\n",
      "75 [D loss: 0.523083, acc.: 89.26%] [G loss: 1.708830]\n",
      "Pearson R on Train set: (0.31160906, 0.0)\n",
      "Pearson R on Val set: (0.31246883, 0.0)\n",
      "76 [D loss: 0.108678, acc.: 96.09%] [G loss: 1.253570]\n",
      "Pearson R on Train set: (0.28332287, 0.0)\n",
      "Pearson R on Val set: (0.29143494, 0.0)\n",
      "77 [D loss: 0.080214, acc.: 97.66%] [G loss: 1.128243]\n",
      "Pearson R on Train set: (0.29754442, 0.0)\n",
      "Pearson R on Val set: (0.30928707, 0.0)\n",
      "78 [D loss: 0.083124, acc.: 97.75%] [G loss: 1.081773]\n",
      "Pearson R on Train set: (0.12150986, 0.0)\n",
      "Pearson R on Val set: (0.15278816, 0.0)\n",
      "79 [D loss: 0.082333, acc.: 97.46%] [G loss: 1.138979]\n",
      "Pearson R on Train set: (0.30127561, 0.0)\n",
      "Pearson R on Val set: (0.32638305, 0.0)\n",
      "80 [D loss: 0.139263, acc.: 95.90%] [G loss: 1.135287]\n",
      "Pearson R on Train set: (0.27243328, 0.0)\n",
      "Pearson R on Val set: (0.30161658, 0.0)\n",
      "81 [D loss: 0.095544, acc.: 96.78%] [G loss: 1.140036]\n",
      "Pearson R on Train set: (0.28046331, 0.0)\n",
      "Pearson R on Val set: (0.3039459, 0.0)\n",
      "82 [D loss: 0.111186, acc.: 96.97%] [G loss: 1.124223]\n",
      "Pearson R on Train set: (0.22522065, 0.0)\n",
      "Pearson R on Val set: (0.24244533, 0.0)\n",
      "83 [D loss: 0.083533, acc.: 97.56%] [G loss: 1.212438]\n",
      "Pearson R on Train set: (0.083506383, 0.0)\n",
      "Pearson R on Val set: (0.10769788, 0.0)\n",
      "84 [D loss: 0.189089, acc.: 94.34%] [G loss: 1.235702]\n",
      "Pearson R on Train set: (0.21905187, 0.0)\n",
      "Pearson R on Val set: (0.23904447, 0.0)\n",
      "85 [D loss: 0.119086, acc.: 96.88%] [G loss: 0.811766]\n",
      "Pearson R on Train set: (0.10044495, 0.0)\n",
      "Pearson R on Val set: (0.14158727, 0.0)\n",
      "86 [D loss: 0.129693, acc.: 95.41%] [G loss: 1.542682]\n",
      "Pearson R on Train set: (0.26201591, 0.0)\n",
      "Pearson R on Val set: (0.27758661, 0.0)\n",
      "87 [D loss: 0.099126, acc.: 97.07%] [G loss: 1.402322]\n",
      "Pearson R on Train set: (0.30197921, 0.0)\n",
      "Pearson R on Val set: (0.32483584, 0.0)\n",
      "88 [D loss: 0.118247, acc.: 97.27%] [G loss: 1.116928]\n",
      "Pearson R on Train set: (0.30993214, 0.0)\n",
      "Pearson R on Val set: (0.32580251, 0.0)\n",
      "89 [D loss: 0.089829, acc.: 97.75%] [G loss: 1.063450]\n",
      "Pearson R on Train set: (0.3129949, 0.0)\n",
      "Pearson R on Val set: (0.32457146, 0.0)\n",
      "90 [D loss: 0.108473, acc.: 96.97%] [G loss: 1.050778]\n",
      "Pearson R on Train set: (0.28405413, 0.0)\n",
      "Pearson R on Val set: (0.31319946, 0.0)\n",
      "91 [D loss: 0.122452, acc.: 96.39%] [G loss: 1.005119]\n",
      "Pearson R on Train set: (0.32388851, 0.0)\n",
      "Pearson R on Val set: (0.33530247, 0.0)\n",
      "92 [D loss: 0.113131, acc.: 96.68%] [G loss: 1.142045]\n",
      "Pearson R on Train set: (0.25728106, 0.0)\n",
      "Pearson R on Val set: (0.28809536, 0.0)\n",
      "93 [D loss: 0.128231, acc.: 96.58%] [G loss: 1.092202]\n",
      "Pearson R on Train set: (0.20867118, 0.0)\n",
      "Pearson R on Val set: (0.22296932, 0.0)\n",
      "94 [D loss: 0.123323, acc.: 96.68%] [G loss: 1.029950]\n",
      "Pearson R on Train set: (0.25119638, 0.0)\n",
      "Pearson R on Val set: (0.28161561, 0.0)\n",
      "95 [D loss: 0.127828, acc.: 96.39%] [G loss: 1.097762]\n",
      "Pearson R on Train set: (0.18296927, 0.0)\n",
      "Pearson R on Val set: (0.22406884, 0.0)\n",
      "96 [D loss: 0.108844, acc.: 97.17%] [G loss: 1.236925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson R on Train set: (0.28099322, 0.0)\n",
      "Pearson R on Val set: (0.29857999, 0.0)\n",
      "97 [D loss: 0.136533, acc.: 95.02%] [G loss: 1.206554]\n",
      "Pearson R on Train set: (0.23145241, 0.0)\n",
      "Pearson R on Val set: (0.26925838, 0.0)\n",
      "98 [D loss: 0.103111, acc.: 97.17%] [G loss: 1.116656]\n",
      "Pearson R on Train set: (0.28716421, 0.0)\n",
      "Pearson R on Val set: (0.31599602, 0.0)\n",
      "99 [D loss: 0.127253, acc.: 97.36%] [G loss: 1.145701]\n",
      "Pearson R on Train set: (0.18772438, 0.0)\n",
      "Pearson R on Val set: (0.22872466, 0.0)\n",
      "100 [D loss: 0.133544, acc.: 95.31%] [G loss: 1.171169]\n",
      "Pearson R on Train set: (0.29617894, 0.0)\n",
      "Pearson R on Val set: (0.31164742, 0.0)\n",
      "101 [D loss: 0.107348, acc.: 96.88%] [G loss: 1.143002]\n",
      "Pearson R on Train set: (0.28545457, 0.0)\n",
      "Pearson R on Val set: (0.29324779, 0.0)\n",
      "102 [D loss: 0.107114, acc.: 96.78%] [G loss: 1.124478]\n",
      "Pearson R on Train set: (0.28678155, 0.0)\n",
      "Pearson R on Val set: (0.30647355, 0.0)\n",
      "103 [D loss: 0.102826, acc.: 97.27%] [G loss: 1.154449]\n",
      "Pearson R on Train set: (0.24593537, 0.0)\n",
      "Pearson R on Val set: (0.26359352, 0.0)\n",
      "104 [D loss: 0.091730, acc.: 97.95%] [G loss: 1.149881]\n",
      "Pearson R on Train set: (0.16806234, 0.0)\n",
      "Pearson R on Val set: (0.2009754, 0.0)\n",
      "105 [D loss: 0.116700, acc.: 95.90%] [G loss: 1.128558]\n",
      "Pearson R on Train set: (0.31132191, 0.0)\n",
      "Pearson R on Val set: (0.32662722, 0.0)\n",
      "106 [D loss: 0.101677, acc.: 97.46%] [G loss: 1.095199]\n",
      "Pearson R on Train set: (0.33133879, 0.0)\n",
      "Pearson R on Val set: (0.35012329, 0.0)\n",
      "107 [D loss: 0.088815, acc.: 97.46%] [G loss: 1.193819]\n",
      "Pearson R on Train set: (0.28163728, 0.0)\n",
      "Pearson R on Val set: (0.29170358, 0.0)\n",
      "108 [D loss: 0.108744, acc.: 96.58%] [G loss: 1.056120]\n",
      "Pearson R on Train set: (0.24954186, 0.0)\n",
      "Pearson R on Val set: (0.28527951, 0.0)\n",
      "109 [D loss: 0.145062, acc.: 95.41%] [G loss: 1.087582]\n",
      "Pearson R on Train set: (0.30573079, 0.0)\n",
      "Pearson R on Val set: (0.31971836, 0.0)\n",
      "110 [D loss: 0.101617, acc.: 96.68%] [G loss: 1.123460]\n",
      "Pearson R on Train set: (0.25403738, 0.0)\n",
      "Pearson R on Val set: (0.28816277, 0.0)\n",
      "111 [D loss: 0.075637, acc.: 97.66%] [G loss: 1.062468]\n",
      "Pearson R on Train set: (0.24007605, 0.0)\n",
      "Pearson R on Val set: (0.25163138, 0.0)\n",
      "112 [D loss: 0.159706, acc.: 93.85%] [G loss: 1.579723]\n",
      "Pearson R on Train set: (0.20003641, 0.0)\n",
      "Pearson R on Val set: (0.2151823, 0.0)\n",
      "113 [D loss: 0.313198, acc.: 92.09%] [G loss: 2.236642]\n",
      "Pearson R on Train set: (0.30616403, 0.0)\n",
      "Pearson R on Val set: (0.32678118, 0.0)\n",
      "114 [D loss: 0.138979, acc.: 96.00%] [G loss: 1.645508]\n",
      "Pearson R on Train set: (0.30835006, 0.0)\n",
      "Pearson R on Val set: (0.32447374, 0.0)\n",
      "115 [D loss: 0.063122, acc.: 97.85%] [G loss: 1.450239]\n",
      "Pearson R on Train set: (0.29209021, 0.0)\n",
      "Pearson R on Val set: (0.31259808, 0.0)\n",
      "116 [D loss: 0.057424, acc.: 98.34%] [G loss: 1.267948]\n",
      "Pearson R on Train set: (0.31899035, 0.0)\n",
      "Pearson R on Val set: (0.3344416, 0.0)\n",
      "117 [D loss: 0.062705, acc.: 97.75%] [G loss: 1.259454]\n",
      "Pearson R on Train set: (0.28451994, 0.0)\n",
      "Pearson R on Val set: (0.3132754, 0.0)\n",
      "118 [D loss: 0.076975, acc.: 97.75%] [G loss: 1.265430]\n",
      "Pearson R on Train set: (0.32366788, 0.0)\n",
      "Pearson R on Val set: (0.33852381, 0.0)\n",
      "119 [D loss: 0.124316, acc.: 96.09%] [G loss: 1.231243]\n",
      "Pearson R on Train set: (0.28179163, 0.0)\n",
      "Pearson R on Val set: (0.29400897, 0.0)\n",
      "120 [D loss: 0.081194, acc.: 97.66%] [G loss: 1.220663]\n",
      "Pearson R on Train set: (0.25403228, 0.0)\n",
      "Pearson R on Val set: (0.25470048, 0.0)\n",
      "121 [D loss: 0.074896, acc.: 98.05%] [G loss: 1.340290]\n",
      "Pearson R on Train set: (0.26600936, 0.0)\n",
      "Pearson R on Val set: (0.28680602, 0.0)\n",
      "122 [D loss: 0.098768, acc.: 96.78%] [G loss: 1.221987]\n",
      "Pearson R on Train set: (0.2082421, 0.0)\n",
      "Pearson R on Val set: (0.22016843, 0.0)\n",
      "123 [D loss: 0.044865, acc.: 98.73%] [G loss: 1.275475]\n",
      "Pearson R on Train set: (0.31727865, 0.0)\n",
      "Pearson R on Val set: (0.32710606, 0.0)\n",
      "124 [D loss: 0.062500, acc.: 98.54%] [G loss: 1.239297]\n",
      "Pearson R on Train set: (0.31935018, 0.0)\n",
      "Pearson R on Val set: (0.34062523, 0.0)\n",
      "125 [D loss: 0.090625, acc.: 97.17%] [G loss: 1.053474]\n",
      "Pearson R on Train set: (0.2479502, 0.0)\n",
      "Pearson R on Val set: (0.26605195, 0.0)\n",
      "126 [D loss: 0.093949, acc.: 96.88%] [G loss: 1.200313]\n",
      "Pearson R on Train set: (0.30928686, 0.0)\n",
      "Pearson R on Val set: (0.32637265, 0.0)\n",
      "127 [D loss: 0.109278, acc.: 96.19%] [G loss: 1.151259]\n",
      "Pearson R on Train set: (0.27308643, 0.0)\n",
      "Pearson R on Val set: (0.28890556, 0.0)\n",
      "128 [D loss: 0.089121, acc.: 97.36%] [G loss: 1.292552]\n",
      "Pearson R on Train set: (0.30413675, 0.0)\n",
      "Pearson R on Val set: (0.31641221, 0.0)\n",
      "129 [D loss: 0.106955, acc.: 96.09%] [G loss: 1.232265]\n",
      "Pearson R on Train set: (0.3127113, 0.0)\n",
      "Pearson R on Val set: (0.33642554, 0.0)\n",
      "130 [D loss: 0.065182, acc.: 98.44%] [G loss: 1.158154]\n",
      "Pearson R on Train set: (0.28596067, 0.0)\n",
      "Pearson R on Val set: (0.31244668, 0.0)\n",
      "131 [D loss: 0.061425, acc.: 98.34%] [G loss: 1.248835]\n",
      "Pearson R on Train set: (0.29270992, 0.0)\n",
      "Pearson R on Val set: (0.31132084, 0.0)\n",
      "132 [D loss: 0.100199, acc.: 97.66%] [G loss: 1.169302]\n",
      "Pearson R on Train set: (0.14109537, 0.0)\n",
      "Pearson R on Val set: (0.17472649, 0.0)\n",
      "133 [D loss: 0.051945, acc.: 98.63%] [G loss: 1.168455]\n",
      "Pearson R on Train set: (0.3196134, 0.0)\n",
      "Pearson R on Val set: (0.3319715, 0.0)\n",
      "134 [D loss: 0.049714, acc.: 98.83%] [G loss: 1.232913]\n",
      "Pearson R on Train set: (0.27584091, 0.0)\n",
      "Pearson R on Val set: (0.29688931, 0.0)\n",
      "135 [D loss: 0.082948, acc.: 97.85%] [G loss: 1.236989]\n",
      "Pearson R on Train set: (0.27878588, 0.0)\n",
      "Pearson R on Val set: (0.30054837, 0.0)\n",
      "136 [D loss: 0.093777, acc.: 96.97%] [G loss: 1.281182]\n",
      "Pearson R on Train set: (0.20653693, 0.0)\n",
      "Pearson R on Val set: (0.25825399, 0.0)\n",
      "137 [D loss: 0.085514, acc.: 97.85%] [G loss: 1.148121]\n",
      "Pearson R on Train set: (0.28911743, 0.0)\n",
      "Pearson R on Val set: (0.30772668, 0.0)\n",
      "138 [D loss: 0.128536, acc.: 95.90%] [G loss: 1.257003]\n",
      "Pearson R on Train set: (0.28091422, 0.0)\n",
      "Pearson R on Val set: (0.30631638, 0.0)\n",
      "139 [D loss: 0.050297, acc.: 98.54%] [G loss: 1.213553]\n",
      "Pearson R on Train set: (0.24193573, 0.0)\n",
      "Pearson R on Val set: (0.25045779, 0.0)\n",
      "140 [D loss: 0.089081, acc.: 97.46%] [G loss: 1.375283]\n",
      "Pearson R on Train set: (0.30311328, 0.0)\n",
      "Pearson R on Val set: (0.30822375, 0.0)\n",
      "141 [D loss: 0.037223, acc.: 98.83%] [G loss: 1.455585]\n",
      "Pearson R on Train set: (0.32639635, 0.0)\n",
      "Pearson R on Val set: (0.34140664, 0.0)\n",
      "142 [D loss: 0.065290, acc.: 97.95%] [G loss: 1.184201]\n",
      "Pearson R on Train set: (0.31967857, 0.0)\n",
      "Pearson R on Val set: (0.34492651, 0.0)\n",
      "143 [D loss: 0.086496, acc.: 97.17%] [G loss: 1.126818]\n",
      "Pearson R on Train set: (0.29383165, 0.0)\n",
      "Pearson R on Val set: (0.31098783, 0.0)\n",
      "144 [D loss: 0.147839, acc.: 95.70%] [G loss: 1.162830]\n",
      "Pearson R on Train set: (0.32744217, 0.0)\n",
      "Pearson R on Val set: (0.34631661, 0.0)\n",
      "145 [D loss: 0.082168, acc.: 96.97%] [G loss: 1.411985]\n",
      "Pearson R on Train set: (0.33173856, 0.0)\n",
      "Pearson R on Val set: (0.35142761, 0.0)\n",
      "146 [D loss: 0.077569, acc.: 98.34%] [G loss: 1.323413]\n",
      "Pearson R on Train set: (0.30768442, 0.0)\n",
      "Pearson R on Val set: (0.34071326, 0.0)\n",
      "147 [D loss: 0.069214, acc.: 98.14%] [G loss: 1.185359]\n",
      "Pearson R on Train set: (0.18774433, 0.0)\n",
      "Pearson R on Val set: (0.20241505, 0.0)\n",
      "148 [D loss: 0.079815, acc.: 97.56%] [G loss: 1.253601]\n",
      "Pearson R on Train set: (0.2254265, 0.0)\n",
      "Pearson R on Val set: (0.24334604, 0.0)\n",
      "149 [D loss: 0.083919, acc.: 97.27%] [G loss: 1.425313]\n",
      "Pearson R on Train set: (0.30059892, 0.0)\n",
      "Pearson R on Val set: (0.30999631, 0.0)\n",
      "150 [D loss: 0.054450, acc.: 98.44%] [G loss: 1.420610]\n",
      "Pearson R on Train set: (0.26065674, 0.0)\n",
      "Pearson R on Val set: (0.29437369, 0.0)\n",
      "151 [D loss: 0.086298, acc.: 97.36%] [G loss: 1.263176]\n",
      "Pearson R on Train set: (0.26523161, 0.0)\n",
      "Pearson R on Val set: (0.26912659, 0.0)\n",
      "152 [D loss: 0.075424, acc.: 97.85%] [G loss: 1.257255]\n",
      "Pearson R on Train set: (0.26491964, 0.0)\n",
      "Pearson R on Val set: (0.29062873, 0.0)\n",
      "153 [D loss: 0.124297, acc.: 95.80%] [G loss: 1.237597]\n",
      "Pearson R on Train set: (0.31944627, 0.0)\n",
      "Pearson R on Val set: (0.3234497, 0.0)\n",
      "154 [D loss: 0.107829, acc.: 97.27%] [G loss: 1.301547]\n",
      "Pearson R on Train set: (0.27271733, 0.0)\n",
      "Pearson R on Val set: (0.29286799, 0.0)\n",
      "155 [D loss: 0.078516, acc.: 96.97%] [G loss: 1.336665]\n",
      "Pearson R on Train set: (0.33789963, 0.0)\n",
      "Pearson R on Val set: (0.34981686, 0.0)\n",
      "156 [D loss: 0.059907, acc.: 98.24%] [G loss: 1.275104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson R on Train set: (0.31954449, 0.0)\n",
      "Pearson R on Val set: (0.32387361, 0.0)\n",
      "157 [D loss: 0.053244, acc.: 98.63%] [G loss: 1.107920]\n",
      "Pearson R on Train set: (0.29301196, 0.0)\n",
      "Pearson R on Val set: (0.31703591, 0.0)\n",
      "158 [D loss: 0.085173, acc.: 97.95%] [G loss: 1.115534]\n",
      "Pearson R on Train set: (0.31845382, 0.0)\n",
      "Pearson R on Val set: (0.32678843, 0.0)\n",
      "159 [D loss: 0.066304, acc.: 97.95%] [G loss: 1.156746]\n",
      "Pearson R on Train set: (0.15471406, 0.0)\n",
      "Pearson R on Val set: (0.17156518, 0.0)\n",
      "160 [D loss: 0.141930, acc.: 96.29%] [G loss: 1.485949]\n",
      "Pearson R on Train set: (0.23136602, 0.0)\n",
      "Pearson R on Val set: (0.25710094, 0.0)\n",
      "161 [D loss: 0.042237, acc.: 98.83%] [G loss: 1.561953]\n",
      "Pearson R on Train set: (0.25757244, 0.0)\n",
      "Pearson R on Val set: (0.269086, 0.0)\n",
      "162 [D loss: 0.054789, acc.: 98.34%] [G loss: 1.511140]\n",
      "Pearson R on Train set: (0.25952366, 0.0)\n",
      "Pearson R on Val set: (0.27359396, 0.0)\n",
      "163 [D loss: 0.019356, acc.: 99.41%] [G loss: 1.469830]\n",
      "Pearson R on Train set: (0.25164753, 0.0)\n",
      "Pearson R on Val set: (0.26532161, 0.0)\n",
      "164 [D loss: 0.054395, acc.: 98.54%] [G loss: 1.387679]\n",
      "Pearson R on Train set: (0.30237603, 0.0)\n",
      "Pearson R on Val set: (0.31719327, 0.0)\n",
      "165 [D loss: 0.077261, acc.: 96.88%] [G loss: 1.301544]\n",
      "Pearson R on Train set: (0.26555884, 0.0)\n",
      "Pearson R on Val set: (0.27667183, 0.0)\n",
      "166 [D loss: 0.042784, acc.: 99.22%] [G loss: 1.356302]\n",
      "Pearson R on Train set: (0.23630026, 0.0)\n",
      "Pearson R on Val set: (0.23053737, 0.0)\n",
      "167 [D loss: 0.053270, acc.: 97.95%] [G loss: 1.459064]\n",
      "Pearson R on Train set: (0.25024292, 0.0)\n",
      "Pearson R on Val set: (0.24637283, 0.0)\n",
      "168 [D loss: 0.274968, acc.: 94.14%] [G loss: 1.513059]\n",
      "Pearson R on Train set: (0.26652318, 0.0)\n",
      "Pearson R on Val set: (0.27570793, 0.0)\n",
      "169 [D loss: 0.045413, acc.: 99.02%] [G loss: 1.047883]\n",
      "Pearson R on Train set: (0.32068929, 0.0)\n",
      "Pearson R on Val set: (0.32919165, 0.0)\n",
      "170 [D loss: 0.049266, acc.: 97.85%] [G loss: 1.244419]\n",
      "Pearson R on Train set: (0.29200894, 0.0)\n",
      "Pearson R on Val set: (0.3123171, 0.0)\n",
      "171 [D loss: 0.045389, acc.: 98.63%] [G loss: 1.349334]\n",
      "Pearson R on Train set: (0.2656157, 0.0)\n",
      "Pearson R on Val set: (0.28920493, 0.0)\n",
      "172 [D loss: 0.029035, acc.: 99.41%] [G loss: 1.588116]\n",
      "Pearson R on Train set: (0.29783827, 0.0)\n",
      "Pearson R on Val set: (0.31828129, 0.0)\n",
      "173 [D loss: 0.027263, acc.: 99.32%] [G loss: 1.743811]\n",
      "Pearson R on Train set: (0.20680174, 0.0)\n",
      "Pearson R on Val set: (0.20065475, 0.0)\n",
      "174 [D loss: 0.048877, acc.: 98.63%] [G loss: 2.070181]\n",
      "Pearson R on Train set: (0.098870091, 0.0)\n",
      "Pearson R on Val set: (0.098856725, 0.0)\n",
      "175 [D loss: 0.007466, acc.: 99.80%] [G loss: 1.969336]\n",
      "Pearson R on Train set: (0.00040501382, 0.10513345808023779)\n",
      "Pearson R on Val set: (-0.01127014, 1.4492869869422734e-112)\n",
      "176 [D loss: 0.073735, acc.: 99.32%] [G loss: 1.272767]\n",
      "Pearson R on Train set: (0.0012403192, 6.9584908310533674e-07)\n",
      "Pearson R on Val set: (-0.006326837, 1.0238829621055885e-36)\n",
      "177 [D loss: 0.000995, acc.: 100.00%] [G loss: 0.957383]\n",
      "Pearson R on Train set: (0.0038826144, 2.0297186868000732e-54)\n",
      "Pearson R on Val set: (-7.402592e-06, 0.98818465285032764)\n",
      "178 [D loss: 0.004178, acc.: 100.00%] [G loss: 0.892842]\n",
      "Pearson R on Train set: (-0.0027983193, 4.2609849940936522e-29)\n",
      "Pearson R on Val set: (-0.010429224, 1.1321980976446183e-96)\n",
      "179 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.939216]\n",
      "Pearson R on Train set: (-0.00074119779, 0.003021594920062784)\n",
      "Pearson R on Val set: (-0.0058462555, 1.3434589236071771e-31)\n",
      "180 [D loss: 0.000675, acc.: 100.00%] [G loss: 0.860998]\n",
      "Pearson R on Train set: (-0.0022000221, 1.340425175871186e-18)\n",
      "Pearson R on Val set: (-0.0094844885, 2.7904309442075541e-80)\n",
      "181 [D loss: 0.008748, acc.: 100.00%] [G loss: 0.883464]\n",
      "Pearson R on Train set: (-0.0018678326, 7.8265372262197325e-14)\n",
      "Pearson R on Val set: (-0.0084333094, 7.3379591616955842e-64)\n",
      "182 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.863305]\n",
      "Pearson R on Train set: (-0.0040712352, 1.1799522021214861e-59)\n",
      "Pearson R on Val set: (-0.011268279, 1.5765028634313839e-112)\n",
      "183 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.905631]\n",
      "Pearson R on Train set: (-0.0032592779, 7.2074034348571805e-39)\n",
      "Pearson R on Val set: (-0.010921932, 7.7786141046921541e-106)\n",
      "184 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.879725]\n",
      "Pearson R on Train set: (-0.002248911, 2.3006970216745731e-19)\n",
      "Pearson R on Val set: (-0.0079792934, 2.3198889977320206e-57)\n",
      "185 [D loss: 0.018366, acc.: 100.00%] [G loss: 0.906429]\n",
      "Pearson R on Train set: (-0.0036786729, 4.9100825956841048e-49)\n",
      "Pearson R on Val set: (-0.011284864, 7.4467138812251202e-113)\n",
      "186 [D loss: 0.002403, acc.: 100.00%] [G loss: 1.027379]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e34b04ccfe23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-f40059eb266a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, X_train, y_train)\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# Discriminator is no longer trained at this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;31m# Generator do its best to fake discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mg_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print \"Training the model...\"\n",
    "gan = GAN(window_size, sample_num,\n",
    "          X_train, y_train,\n",
    "          X_val, y_val,\n",
    "          X_test, y_test,\n",
    "          model_dir, srv_dir)\n",
    "gan.train(num_epochs, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
