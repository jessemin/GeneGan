{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "#### Convert histone mark signals and use deep CNN for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Need to activate genomelake environment before this code. Simply type 'genomelake' in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import random\n",
    "# custom file path package\n",
    "from data import Data_Directories\n",
    "# custom utility package\n",
    "from utils.compute_util import *\n",
    "# package for genomic data\n",
    "from pybedtools import Interval, BedTool\n",
    "from genomelake.extractors import ArrayExtractor, BigwigExtractor\n",
    "# package for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats.stats import pearsonr,spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 2001\n",
    "process_all = False\n",
    "sample_num = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day6', 'day3', 'day0']\n",
      "['100', '140']\n",
      "['H3K27me3', 'H3K4me1', 'H3K27ac']\n"
     ]
    }
   ],
   "source": [
    "# retrieve data\n",
    "data = Data_Directories()\n",
    "print data.intervals.keys()\n",
    "print data.input_atac['day0'].keys()\n",
    "print data.output_histone['day0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Intervals Extracted for day0: 267226\n",
      "# of Intervals Extracted for day3: 233846\n"
     ]
    }
   ],
   "source": [
    "# get intervals for day0 data\n",
    "day0_intervals = list(BedTool(data.intervals['day0']))\n",
    "print '# of Intervals Extracted for day0: {}'.format(len(day0_intervals))\n",
    "\n",
    "# get intervals for day3 data\n",
    "day3_intervals = list(BedTool(data.intervals['day3']))\n",
    "print '# of Intervals Extracted for day3: {}'.format(len(day3_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, 140bp\n",
      "Finished extracting bigwig for day3, 140bp\n"
     ]
    }
   ],
   "source": [
    "# create an ArrayExtractor for ATAC-seq for day0 with 140 base pairs\n",
    "bw_140bp_day0 = ArrayExtractor(data.input_atac['day0']['140'])\n",
    "print 'Finished extracting bigwig for day0, 140bp'\n",
    "\n",
    "# create an ArrayExtractor for ATAC-seq for day3 with 140 base pairs\n",
    "bw_140bp_day3 = ArrayExtractor(data.input_atac['day3']['140'])\n",
    "print 'Finished extracting bigwig for day3, 140bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, 140bp\n",
      "Finished extracting bigwig for day3, 140bp\n"
     ]
    }
   ],
   "source": [
    "# create a BigWigExtractor for histone makr 'H3K27ac' for day0\n",
    "bw_histone_mark_day0 = BigwigExtractor(data.output_histone['day0']['H3K27ac'])\n",
    "print 'Finished extracting bigwig for day0, 140bp'\n",
    "\n",
    "# create a BigWigExtractor for histone makr 'H3K27ac' for day3\n",
    "bw_histone_mark_day3 = BigwigExtractor(data.output_histone['day3']['H3K27ac'])\n",
    "print 'Finished extracting bigwig for day3, 140bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished normalizing day0 intervals!\n",
      "Finished normalizing day3 intervals!\n"
     ]
    }
   ],
   "source": [
    "# normalize day0 intervals\n",
    "normalized_day0_intervals = [normalize_interval(interval, window_size) for interval in day0_intervals if normalize_interval(interval, window_size)]\n",
    "print 'Finished normalizing day0 intervals!'\n",
    "\n",
    "# normalize day3 intervals\n",
    "normalized_day3_intervals = [normalize_interval(interval, window_size) for interval in day3_intervals if normalize_interval(interval, window_size)]\n",
    "print 'Finished normalizing day3 intervals!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of original intervals\n",
      "[(123412027, [123411855, 123412989]), (123411941, [123411855, 123412989]), (131908564, [131908487, 131910071])]\n",
      "Examples of normalized intervals with window size of 2001\n",
      "[[123411027, 123413028], [123410941, 123412942], [131907564, 131909565]]\n"
     ]
    }
   ],
   "source": [
    "assert (len(day0_intervals)==len(normalized_day0_intervals))\n",
    "assert (len(day3_intervals)==len(normalized_day3_intervals))\n",
    "print \"Examples of original intervals\"\n",
    "print [(int(_interval.start)+int(_interval[-1]), [int(_interval.start), int(_interval.end)])\n",
    "       for _interval in day0_intervals[:3]]\n",
    "print \"Examples of normalized intervals with window size of {}\".format(window_size)\n",
    "print [([int(_interval.start), int(_interval.end)])\n",
    "       for _interval in  normalized_day0_intervals[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 2001, 5) (233846, 2001, 5)\n"
     ]
    }
   ],
   "source": [
    "atac_seq_day0 = bw_140bp_day0(normalized_day0_intervals)\n",
    "atac_seq_day3 = bw_140bp_day3(normalized_day3_intervals)\n",
    "print atac_seq_day0.shape, atac_seq_day3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning day0: 267226\n",
      "After pruning day0: 267226\n",
      "Before pruning day3: 233846\n",
      "After pruning day3: 233846\n"
     ]
    }
   ],
   "source": [
    "#TODO: put this into utils if possible\n",
    "def prune_invalid_intervals(intervals, bigwig_file):\n",
    "    for _interval in intervals[:]:\n",
    "        try:\n",
    "            bigwig_file([_interval])\n",
    "        except:\n",
    "            intervals.remove(_interval)\n",
    "            pass\n",
    "        \n",
    "print \"Before pruning day0: {}\".format(len(normalized_day0_intervals))\n",
    "prune_invalid_intervals(normalized_day0_intervals, bw_140bp_day0)\n",
    "print \"After pruning day0: {}\".format(len(normalized_day0_intervals))\n",
    "\n",
    "print \"Before pruning day3: {}\".format(len(normalized_day3_intervals))\n",
    "prune_invalid_intervals(normalized_day3_intervals, bw_140bp_day3)\n",
    "print \"After pruning day3: {}\".format(len(normalized_day3_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of ATAC-seq signal: (1, 2001, 5)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of ATAC-seq signal: {}\".format(bw_140bp_day0(normalized_day0_intervals[:1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of histone mark signal: (1, 2001)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of histone mark signal: {}\".format(bw_histone_mark_day0(normalized_day0_intervals[:1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 2001)\n",
      "(233846, 2001)\n"
     ]
    }
   ],
   "source": [
    "# replace nan values with zeros and convert it to p-values\n",
    "histone_mark_day0 = np.nan_to_num(bw_histone_mark_day0(normalized_day0_intervals))\n",
    "print histone_mark_day0.shape\n",
    "\n",
    "histone_mark_day3 = np.nan_to_num(bw_histone_mark_day3(normalized_day3_intervals))\n",
    "print histone_mark_day3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 2001, 1) (233846, 2001, 1)\n"
     ]
    }
   ],
   "source": [
    "histone_mark_day0 = np.expand_dims(histone_mark_day0, axis=2)\n",
    "histone_mark_day3 = np.expand_dims(histone_mark_day3, axis=2)\n",
    "print histone_mark_day0.shape, histone_mark_day3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example histone mark signal\n",
      "\tRaw value: [ 0.01014  0.01014  0.01014  0.02435  0.02435]\n"
     ]
    }
   ],
   "source": [
    "print \"Example histone mark signal\"\n",
    "print \"\\tRaw value: {}\".format(bw_histone_mark_day0(normalized_day0_intervals[:1])[0][:5].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, Dropout\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "# parameters for first conv layer\n",
    "hidden_filters_1 = 20\n",
    "hidden_kernel_size_1 = 30\n",
    "# parameters for second conv layer\n",
    "hidden_filters_2 = 10\n",
    "hidden_kernel_size_2 = 20\n",
    "# parameters for the output layer\n",
    "output_filters = 1\n",
    "output_kernel_size = 5\n",
    "# parameters for training\n",
    "batch_size = 64\n",
    "num_epochs = 1000\n",
    "evaluation_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Baseline CNN (based on KERAS functional API)\n",
    "'''\n",
    "# \n",
    "inputs = Input(shape=(window_size, 5, ))\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(inputs)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_1,\n",
    "    kernel_size=hidden_kernel_size_1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_2,\n",
    "    kernel_size=hidden_kernel_size_2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_2,\n",
    "    kernel_size=hidden_kernel_size_2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_2,\n",
    "    kernel_size=hidden_kernel_size_2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_2,\n",
    "    kernel_size=hidden_kernel_size_2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_2,\n",
    "    kernel_size=hidden_kernel_size_2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Conv1D(\n",
    "    filters=hidden_filters_2,\n",
    "    kernel_size=hidden_kernel_size_2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    strides=1)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "outputs = Conv1D(\n",
    "    filters=output_filters,\n",
    "    kernel_size=output_kernel_size,\n",
    "    padding='same',\n",
    "    activation='linear',\n",
    "    strides=1\n",
    "    )(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearson(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(np.multiply(xm,ym))\n",
    "    r_den = K.sqrt(np.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearson_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(np.multiply(xm,ym))\n",
    "    r_den = K.sqrt(np.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return 1 - K.square(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling a model with adam optimizer\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2001, 5)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2001, 20)          3020      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 2001, 20)          12020     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2001, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 2001, 10)          4010      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2001, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 2001, 10)          2010      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2001, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 2001, 10)          2010      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2001, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 2001, 10)          2010      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2001, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 2001, 10)          2010      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 2001, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 2001, 10)          2010      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 2001, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 2001, 1)           51        \n",
      "=================================================================\n",
      "Total params: 137,331\n",
      "Trainable params: 137,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# setting optimizer\n",
    "# adam = optimizers.Adam(clipnorm=1.)\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "print \"Compiling a model with adam optimizer\"\n",
    "model.compile(loss=pearson_loss,\n",
    "              optimizer=adam,\n",
    "              metrics=[pearson])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/\", write_grads=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=5,\n",
    "                              min_lr=0.001)\n",
    "\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_freq = evaluation_freq\n",
    "output_dir = \"plots\"\n",
    "model_dir = \"models\"\n",
    "pickle_dir = \"pickles\"\n",
    "import pickle\n",
    "# callback function for plotting loss graph for every 500 epochs\n",
    "class Plot_Train_Loss_Callback(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.pearsons = []\n",
    "        self.lrs = []\n",
    "        self.val_losses = []\n",
    "        self.val_pearsons = []\n",
    "        self.epochs = 0\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        #'loss': 'pearson': 'lr': 'val_loss': 'val_pearson'\n",
    "        self.losses.append(logs['loss'])\n",
    "        self.pearsons.append(logs['pearson'])\n",
    "        self.lrs.append(logs['lr'])\n",
    "        self.val_losses.append(logs['val_loss'])\n",
    "        self.val_pearsons.append(logs['val_pearson'])\n",
    "        self.epochs += 1\n",
    "        if self.epochs % save_freq == 0:\n",
    "            self.model.save(os.path.join(model_dir, 'model_'+str(self.epochs)+\".h5\"))\n",
    "            pickle.dump(self.losses, open(os.path.join(pickle_dir, 'loss_'+str(self.epochs)+\".p\"), 'wb'))\n",
    "            pickle.dump(self.pearsons, open(os.path.join(pickle_dir, 'pearson_'+str(self.epochs)+\".p\"), 'wb'))\n",
    "            pickle.dump(self.lrs, open(os.path.join(pickle_dir, 'lr_'+str(self.epochs)+\".p\"), 'wb'))\n",
    "            pickle.dump(self.val_losses, open(os.path.join(pickle_dir, 'val_loss_'+str(self.epochs)+\".p\"), 'wb'))\n",
    "            pickle.dump(self.val_pearsons, open(os.path.join(pickle_dir, 'val_pearson_'+str(self.epochs)+\".p\"), 'wb'))\n",
    "            # summarize history for loss\n",
    "#             plt.plot(range(self.epochs), self.losses)\n",
    "#             plt.title('model loss')\n",
    "#             plt.ylabel('loss')\n",
    "#             plt.xlabel('epoch')\n",
    "#             plt.savefig(os.path.join(output_dir, 'model_train_'+str(self.epochs)+'.png'))\n",
    "#             plt.close()\n",
    "    \n",
    "class Compute_Pearson_Callback(Callback):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.p_train_history = []\n",
    "        self.epochs = 0\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.epochs += 1\n",
    "        x_train, y_train = self.x_train, self.y_train\n",
    "        y_pred_train = self.model.predict(x_train)\n",
    "        p_train_list = []\n",
    "        for y_t, y_pt in zip(y_train, y_pred_train):\n",
    "            p_train_list.append(pearsonr(y_t.squeeze(), y_pt.squeeze()))\n",
    "        p_train = np.mean(p_train_list)\n",
    "        print \"Train Pearson Corr: {}\".format(p_train)\n",
    "        self.p_train_history.append(p_train)\n",
    "        if self.epochs % save_freq == 0:\n",
    "            # record pearson correlation for train\n",
    "            plt.plot(range(self.epochs), self.p_train_history)\n",
    "            plt.title('Pearson Correlation - Train')\n",
    "            plt.ylabel('pearson correlation')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.savefig(os.path.join(output_dir, 'Pearson', 'train_'+str(self.epochs)+'.png'))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4,7\n",
      "Fitting the model...\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "7500/7500 [==============================] - 15s 2ms/step - loss: 0.8805 - pearson: 0.1195 - val_loss: 0.8681 - val_pearson: 0.1319\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8447 - pearson: 0.1553 - val_loss: 0.8530 - val_pearson: 0.1470\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8336 - pearson: 0.1664 - val_loss: 0.8515 - val_pearson: 0.1485\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8283 - pearson: 0.1717 - val_loss: 0.8590 - val_pearson: 0.1410\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8274 - pearson: 0.1726 - val_loss: 0.8496 - val_pearson: 0.1504\n",
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8235 - pearson: 0.1765 - val_loss: 0.8469 - val_pearson: 0.1531\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8207 - pearson: 0.1793 - val_loss: 0.8502 - val_pearson: 0.1498\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8181 - pearson: 0.1819 - val_loss: 0.8496 - val_pearson: 0.1504\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8190 - pearson: 0.1810 - val_loss: 0.8489 - val_pearson: 0.1511\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8143 - pearson: 0.1857 - val_loss: 0.8539 - val_pearson: 0.1461\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8123 - pearson: 0.1877 - val_loss: 0.8518 - val_pearson: 0.1482\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8097 - pearson: 0.1903 - val_loss: 0.8445 - val_pearson: 0.1555\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8115 - pearson: 0.1885 - val_loss: 0.8480 - val_pearson: 0.1520\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8097 - pearson: 0.1903 - val_loss: 0.8444 - val_pearson: 0.1556\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8055 - pearson: 0.1945 - val_loss: 0.8463 - val_pearson: 0.1537\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8087 - pearson: 0.1913 - val_loss: 0.8498 - val_pearson: 0.1502\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8023 - pearson: 0.1977 - val_loss: 0.8483 - val_pearson: 0.1517\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.8019 - pearson: 0.1981 - val_loss: 0.8420 - val_pearson: 0.1580\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7994 - pearson: 0.2006 - val_loss: 0.8401 - val_pearson: 0.1599\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7912 - pearson: 0.2088 - val_loss: 0.8454 - val_pearson: 0.1546\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7931 - pearson: 0.2069 - val_loss: 0.8499 - val_pearson: 0.1501\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7917 - pearson: 0.2083 - val_loss: 0.8413 - val_pearson: 0.1587\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7882 - pearson: 0.2118 - val_loss: 0.8545 - val_pearson: 0.1455\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7899 - pearson: 0.2101 - val_loss: 0.8404 - val_pearson: 0.1596\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7847 - pearson: 0.2153 - val_loss: 0.8517 - val_pearson: 0.1483\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7784 - pearson: 0.2216 - val_loss: 0.8534 - val_pearson: 0.1466\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7808 - pearson: 0.2192 - val_loss: 0.8448 - val_pearson: 0.1552\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7726 - pearson: 0.2274 - val_loss: 0.8425 - val_pearson: 0.1575\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7796 - pearson: 0.2204 - val_loss: 0.8424 - val_pearson: 0.1576\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7716 - pearson: 0.2284 - val_loss: 0.8426 - val_pearson: 0.1574\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7704 - pearson: 0.2296 - val_loss: 0.8460 - val_pearson: 0.1540\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7673 - pearson: 0.2327 - val_loss: 0.8424 - val_pearson: 0.1576\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7614 - pearson: 0.2386 - val_loss: 0.8475 - val_pearson: 0.1525\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7711 - pearson: 0.2289 - val_loss: 0.8464 - val_pearson: 0.1536\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7502 - pearson: 0.2498 - val_loss: 0.8452 - val_pearson: 0.1548\n",
      "Epoch 36/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7620 - pearson: 0.2380 - val_loss: 0.8484 - val_pearson: 0.1516\n",
      "Epoch 37/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7600 - pearson: 0.2400 - val_loss: 0.8516 - val_pearson: 0.1484\n",
      "Epoch 38/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7563 - pearson: 0.2437 - val_loss: 0.8389 - val_pearson: 0.1611\n",
      "Epoch 39/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7516 - pearson: 0.2484 - val_loss: 0.8445 - val_pearson: 0.1555\n",
      "Epoch 40/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7483 - pearson: 0.2517 - val_loss: 0.8485 - val_pearson: 0.1515\n",
      "Epoch 41/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7596 - pearson: 0.2404 - val_loss: 0.8434 - val_pearson: 0.1566\n",
      "Epoch 42/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7456 - pearson: 0.2544 - val_loss: 0.8416 - val_pearson: 0.1584\n",
      "Epoch 43/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7430 - pearson: 0.2570 - val_loss: 0.8423 - val_pearson: 0.1577\n",
      "Epoch 44/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7467 - pearson: 0.2533 - val_loss: 0.8437 - val_pearson: 0.1563\n",
      "Epoch 45/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7415 - pearson: 0.2585 - val_loss: 0.8472 - val_pearson: 0.1528\n",
      "Epoch 46/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7413 - pearson: 0.2587 - val_loss: 0.8481 - val_pearson: 0.1519\n",
      "Epoch 47/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7489 - pearson: 0.2511 - val_loss: 0.8476 - val_pearson: 0.1524\n",
      "Epoch 48/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7418 - pearson: 0.2582 - val_loss: 0.8537 - val_pearson: 0.1463\n",
      "Epoch 49/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7346 - pearson: 0.2654 - val_loss: 0.8496 - val_pearson: 0.1504\n",
      "Epoch 50/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7337 - pearson: 0.2663 - val_loss: 0.8471 - val_pearson: 0.1529\n",
      "1000/1000 [==============================] - 1s 729us/step\n",
      "[('loss', 0.73720287895202641), ('pearson', 0.26279712969064711)]\n",
      "Test spearman: -0.30936922827\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7367 - pearson: 0.2633 - val_loss: 0.8513 - val_pearson: 0.1487\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7266 - pearson: 0.2734 - val_loss: 0.8460 - val_pearson: 0.1540\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7261 - pearson: 0.2739 - val_loss: 0.8504 - val_pearson: 0.1496\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7229 - pearson: 0.2771 - val_loss: 0.8470 - val_pearson: 0.1530\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7363 - pearson: 0.2637 - val_loss: 0.8495 - val_pearson: 0.1505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7337 - pearson: 0.2663 - val_loss: 0.8399 - val_pearson: 0.1601\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7253 - pearson: 0.2747 - val_loss: 0.8398 - val_pearson: 0.1602\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7292 - pearson: 0.2708 - val_loss: 0.8443 - val_pearson: 0.1557\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7262 - pearson: 0.2738 - val_loss: 0.8446 - val_pearson: 0.1554\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7257 - pearson: 0.2743 - val_loss: 0.8465 - val_pearson: 0.1535\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7148 - pearson: 0.2852 - val_loss: 0.8508 - val_pearson: 0.1492\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7241 - pearson: 0.2759 - val_loss: 0.8498 - val_pearson: 0.1502\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7184 - pearson: 0.2816 - val_loss: 0.8557 - val_pearson: 0.1443\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7201 - pearson: 0.2799 - val_loss: 0.8481 - val_pearson: 0.1519\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7112 - pearson: 0.2888 - val_loss: 0.8518 - val_pearson: 0.1482\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7228 - pearson: 0.2772 - val_loss: 0.8484 - val_pearson: 0.1516\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7100 - pearson: 0.2900 - val_loss: 0.8508 - val_pearson: 0.1492\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7119 - pearson: 0.2881 - val_loss: 0.8476 - val_pearson: 0.1524\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7211 - pearson: 0.2789 - val_loss: 0.8499 - val_pearson: 0.1501\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7162 - pearson: 0.2838 - val_loss: 0.8538 - val_pearson: 0.1462\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7208 - pearson: 0.2792 - val_loss: 0.8442 - val_pearson: 0.1558\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7146 - pearson: 0.2854 - val_loss: 0.8404 - val_pearson: 0.1596\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7186 - pearson: 0.2814 - val_loss: 0.8423 - val_pearson: 0.1577\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.6999 - pearson: 0.3001 - val_loss: 0.8455 - val_pearson: 0.1545\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7170 - pearson: 0.2830 - val_loss: 0.8360 - val_pearson: 0.1640\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7129 - pearson: 0.2871 - val_loss: 0.8581 - val_pearson: 0.1419\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.7152 - pearson: 0.2848 - val_loss: 0.8509 - val_pearson: 0.1491\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.9879 - pearson: 0.0121 - val_loss: 1.0000 - val_pearson: 0.0000e+00\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 1.0000 - pearson: 0.0000e+00 - val_loss: 1.0000 - val_pearson: 0.0000e+00\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 1.0000 - pearson: 0.0000e+00 - val_loss: 1.0000 - val_pearson: 0.0000e+00\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 1.0000 - pearson: 0.0000e+00 - val_loss: 1.0000 - val_pearson: 0.0000e+00\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 1.0000 - pearson: 0.0000e+00 - val_loss: 1.0000 - val_pearson: 0.0000e+00\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 1.0000 - pearson: 0.0000e+00 - val_loss: 1.0000 - val_pearson: 0.0000e+00\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 1.0000 - pearson: 0.0000e+00 - val_loss: 1.0000 - val_pearson: 0.0000e+00\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 1.0000 - pearson: 0.0000e+00 - val_loss: 1.0000 - val_pearson: 0.0000e+00\n",
      "Epoch 36/50\n",
      "5184/7500 [===================>..........] - ETA: 3s - loss: 1.0000 - pearson: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f57a0333bb8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m               callbacks=[tensorboard,\n\u001b[1;32m     14\u001b[0m                          \u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                          Plot_Train_Loss_Callback()])\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/jesikmin/anaconda2/envs/genomelake_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=4,7\n",
    "print \"Fitting the model...\"\n",
    "test_spearmans = []\n",
    "test_losses = []\n",
    "test_pearsons = []\n",
    "for i in range(num_epochs/evaluation_freq):\n",
    "    model.fit(atac_seq_day0[:10000],\n",
    "              histone_mark_day0[:10000],\n",
    "              batch_size=batch_size,\n",
    "              epochs=evaluation_freq,\n",
    "              validation_split=0.25,   \n",
    "              #shuffle=True,\n",
    "              callbacks=[tensorboard,\n",
    "                         reduce_lr,\n",
    "                         Plot_Train_Loss_Callback()])\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(atac_seq_day0[10000:11000], histone_mark_day0[10000:11000])\n",
    "    print zip(model.metrics_names, scores)\n",
    "    test_losses.append(scores[0])\n",
    "    test_pearsons.append(scores[1])\n",
    "    test_prediction = model.predict(atac_seq_day0[10000:11000])\n",
    "    test_spearman_sum = 0.0\n",
    "    for idx, _pred in enumerate(test_prediction):\n",
    "        rho, _ = spearmanr(histone_mark_day0[10000+idx], _pred)\n",
    "        test_spearman_sum += rho\n",
    "    print \"Test spearman: {}\".format(test_spearman_sum * 1.0 / 1000)\n",
    "    test_spearmans.append(test_spearman_sum * 1.0 / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'Test Spearman Coefficient')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEKCAYAAAArYJMgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiKiUUk0ERHRqP8P4tc8vUv7xOsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07d0668150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(test_spearmans)), test_spearmans, color='blue')\n",
    "plt.xlabel('Evaluation Epoch')\n",
    "plt.ylabel('Test Spearman Coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'Test Losses')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUivDQpLUyrCQJLX6/4TjyIozDb7YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07651e63d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(test_losses)), test_losses, color='blue')\n",
    "plt.xlabel('Evaluation Epoch')\n",
    "plt.ylabel('Test Losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'Test Pearsons')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0747413b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(test_pearsons)), test_pearsons, color='blue')\n",
    "plt.xlabel('Evaluation Epoch')\n",
    "plt.ylabel('Test Pearsons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import izip\n",
    "import subprocess\n",
    "def interval_score_pairs(intervals, scores, merge_type):\n",
    "    return (izip(intervals, scores) if merge_type is None\n",
    "            else merged_scores(scores, intervals, merge_type))\n",
    "def _write_1D_deeplift_track(scores, intervals, file_prefix, merge_type=None,\n",
    "                             CHROM_SIZES='/mnt/data/annotations/by_release/hg19.GRCh37/hg19.chrom.sizes'):\n",
    "    assert scores.ndim == 2\n",
    "\n",
    "    bedgraph = file_prefix + '.bedGraph'\n",
    "    bigwig = file_prefix + '.bw'\n",
    "\n",
    "    print 'Writing 1D track of shape: {}'.format(scores.shape)\n",
    "    print 'Writing to file: {}'.format(bigwig)\n",
    "\n",
    "    with open(bedgraph, 'w') as fp:\n",
    "        for interval, score in interval_score_pairs(intervals, scores,\n",
    "                                                    merge_type):\n",
    "            chrom = interval.chrom\n",
    "            start = interval.start\n",
    "            for score_idx, val in enumerate(score):\n",
    "                fp.write('%s\\t%d\\t%d\\t%g\\n' % (chrom,\n",
    "                                               start + score_idx,\n",
    "                                               start + score_idx + 1,\n",
    "                                               val))\n",
    "    print 'Wrote bedgraph.'\n",
    "\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            ['wigToBigWig', bedgraph, CHROM_SIZES, bigwig],\n",
    "            stderr=subprocess.STDOUT)\n",
    "        print 'wigToBigWig output: {}'.format(output)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print 'wigToBigWig terminated with exit code {}'.format(\n",
    "            e.returncode)\n",
    "        print 'output was:\\n' + e.output\n",
    "\n",
    "    print 'Wrote bigwig.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2001)\n",
      "Writing 1D track of shape: (1000, 2001)\n",
      "Writing to file: bigwig/test.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig terminated with exit code 255\n",
      "output was:\n",
      "Overlap between chr1 15177248 15177249 and chr1 15177248 15177249.\n",
      "Please remove overlaps and try again\n",
      "\n",
      "Wrote bigwig.\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(atac_seq_day0[10000:11000]).reshape(1000, 2001)\n",
    "print prediction.shape\n",
    "_write_1D_deeplift_track(prediction, normalized_day0_intervals[10000:11000], \"bigwig/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2001)\n",
      "Writing 1D track of shape: (10000, 2001)\n",
      "Writing to file: bigwig/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig terminated with exit code 255\n",
      "output was:\n",
      "Overlap between chr1 1284150 1284151 and chr1 1284150 1284151.\n",
      "Please remove overlaps and try again\n",
      "\n",
      "Wrote bigwig.\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(atac_seq_day0[0:10000]).reshape(10000, 2001)\n",
    "print prediction.shape\n",
    "_write_1D_deeplift_track(prediction, normalized_day0_intervals[0:10000], \"bigwig/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
