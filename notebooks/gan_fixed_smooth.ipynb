{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Need to activate genomelake environment before this code. Simply type 'genomelake' in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4,5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=4,5\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import random\n",
    "# custom file path package\n",
    "from data import Data_Directories\n",
    "# custom utility package\n",
    "from utils.compute_util import *\n",
    "# package for genomic data\n",
    "from pybedtools import Interval, BedTool\n",
    "from genomelake.extractors import ArrayExtractor, BigwigExtractor\n",
    "# package for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats.stats import pearsonr,spearmanr\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 2001\n",
    "process_all = False\n",
    "sample_num = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day6', 'day3', 'day0']\n",
      "['100', '140']\n",
      "['H3K27me3', 'H3K4me1', 'H3K27ac']\n"
     ]
    }
   ],
   "source": [
    "# retrieve data\n",
    "data = Data_Directories()\n",
    "print data.intervals.keys()\n",
    "print data.input_atac['day0'].keys()\n",
    "print data.output_histone['day0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Intervals Extracted for day0: 267226\n"
     ]
    }
   ],
   "source": [
    "# get intervals for day0 data\n",
    "day0_intervals = list(BedTool(data.intervals['day0']))\n",
    "print '# of Intervals Extracted for day0: {}'.format(len(day0_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, 140bp\n"
     ]
    }
   ],
   "source": [
    "# create an ArrayExtractor for ATAC-seq for day0 with 140 base pairs\n",
    "bw_140bp_day0 = ArrayExtractor(data.input_atac['day0']['140'])\n",
    "print 'Finished extracting bigwig for day0, 140bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting bigwig for day0, 140bp\n"
     ]
    }
   ],
   "source": [
    "# create a BigWigExtractor for histone makr 'H3K27ac' for day0\n",
    "bw_histone_mark_day0 = BigwigExtractor(data.output_histone['day0']['H3K27ac'])\n",
    "print 'Finished extracting bigwig for day0, 140bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished normalizing day0 intervals!\n"
     ]
    }
   ],
   "source": [
    "# normalize day0 intervals\n",
    "normalized_day0_intervals = [normalize_interval(interval, window_size) for interval in day0_intervals if normalize_interval(interval, window_size)]\n",
    "print 'Finished normalizing day0 intervals!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of original intervals\n",
      "[(123412027, [123411855, 123412989]), (123411941, [123411855, 123412989]), (131908564, [131908487, 131910071])]\n",
      "Examples of normalized intervals with window size of 2001\n",
      "[[123411027, 123413028], [123410941, 123412942], [131907564, 131909565]]\n"
     ]
    }
   ],
   "source": [
    "assert (len(day0_intervals)==len(normalized_day0_intervals))\n",
    "print \"Examples of original intervals\"\n",
    "print [(int(_interval.start)+int(_interval[-1]), [int(_interval.start), int(_interval.end)])\n",
    "       for _interval in day0_intervals[:3]]\n",
    "print \"Examples of normalized intervals with window size of {}\".format(window_size)\n",
    "print [([int(_interval.start), int(_interval.end)])\n",
    "       for _interval in  normalized_day0_intervals[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 2001, 5)\n"
     ]
    }
   ],
   "source": [
    "atac_seq_day0 = bw_140bp_day0(normalized_day0_intervals)\n",
    "print atac_seq_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning day0: 267226\n",
      "After pruning day0: 267226\n"
     ]
    }
   ],
   "source": [
    "#TODO: put this into utils if possible\n",
    "def prune_invalid_intervals(intervals, bigwig_file):\n",
    "    for _interval in intervals[:]:\n",
    "        try:\n",
    "            bigwig_file([_interval])\n",
    "        except:\n",
    "            intervals.remove(_interval)\n",
    "            pass\n",
    "        \n",
    "print \"Before pruning day0: {}\".format(len(normalized_day0_intervals))\n",
    "prune_invalid_intervals(normalized_day0_intervals, bw_140bp_day0)\n",
    "print \"After pruning day0: {}\".format(len(normalized_day0_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of ATAC-seq signal: (1, 2001, 5)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of ATAC-seq signal: {}\".format(bw_140bp_day0(normalized_day0_intervals[:1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of histone mark signal: (1, 2001)\n"
     ]
    }
   ],
   "source": [
    "print \"Dimension of histone mark signal: {}\".format(bw_histone_mark_day0(normalized_day0_intervals[:1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 2001)\n"
     ]
    }
   ],
   "source": [
    "# replace nan values with zeros and convert it to p-values\n",
    "histone_mark_day0 = np.nan_to_num(bw_histone_mark_day0(normalized_day0_intervals))\n",
    "print histone_mark_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267226, 2001, 1)\n"
     ]
    }
   ],
   "source": [
    "histone_mark_day0 = np.expand_dims(histone_mark_day0, axis=2)\n",
    "print histone_mark_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example histone mark signal\n",
      "\tRaw value: [ 0.01014  0.01014  0.01014  0.02435  0.02435]\n"
     ]
    }
   ],
   "source": [
    "print \"Example histone mark signal\"\n",
    "print \"\\tRaw value: {}\".format(bw_histone_mark_day0(normalized_day0_intervals[:1])[0][:5].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, Dropout, BatchNormalization, Activation, ZeroPadding1D, Reshape, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth_rate = 0.2\n",
    "dropout_rate = 0.5\n",
    "# parameters for first conv layer\n",
    "hidden_filters_1 = 32\n",
    "hidden_kernel_size_1 = window_size\n",
    "# parameters for second conv layer\n",
    "output_filters = 1\n",
    "output_kernel_size = 16\n",
    "# parameters for training\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "evaluation_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for writing the scores into bigwig file\n",
    "from itertools import izip\n",
    "from itertools import groupby\n",
    "import subprocess\n",
    "\n",
    "def interval_key(interval):\n",
    "    return (interval.chrom, interval.start, interval.stop)\n",
    "\n",
    "def merged_scores(scores, intervals, merge_type):\n",
    "    # A generator that returns merged intervals/scores\n",
    "    # Scores should have shape: #examples x #categories x #interval_size\n",
    "    # Second dimension can be omitted for a 1D signal\n",
    "    signal_dims = scores.ndim - 1\n",
    "    assert signal_dims in {1, 2}\n",
    "\n",
    "    # Only support max for now\n",
    "    assert merge_type == 'max'\n",
    "    score_first_dim = 1 if signal_dims == 1 else scores.shape[1]\n",
    "\n",
    "    dtype = scores.dtype\n",
    "\n",
    "    sort_idx, sorted_intervals = \\\n",
    "        zip(*sorted(enumerate(intervals),\n",
    "                    key=lambda item: interval_key(item[1])))\n",
    "    sorted_intervals = BedTool(sorted_intervals)\n",
    "\n",
    "    # Require at least 1bp overlap\n",
    "    # Explicitly convert to list otherwise it will keep opening a file when\n",
    "    # retrieving an index resulting in an error (too many open files)\n",
    "    interval_clust = list(sorted_intervals.cluster(d=-1))\n",
    "    for _, group in groupby(izip(sort_idx, interval_clust),\n",
    "                            key=lambda item: item[1].fields[-1]):\n",
    "        idx_interval_pairs = list(group)\n",
    "        group_idx, group_intervals = zip(*idx_interval_pairs)\n",
    "\n",
    "        if len(idx_interval_pairs) == 1:\n",
    "            yield group_intervals[0], scores[group_idx[0], ...]\n",
    "        else:\n",
    "            group_chrom = group_intervals[0].chrom\n",
    "            group_start = min(interval.start for interval in group_intervals)\n",
    "            group_stop = max(interval.stop for interval in group_intervals)\n",
    "\n",
    "            # This part needs to change to support more merge_types (e.g. mean)\n",
    "            group_score = np.full((score_first_dim, group_stop - group_start),\n",
    "                                  -np.inf, dtype)\n",
    "            for idx, interval in idx_interval_pairs:\n",
    "                slice_start = interval.start - group_start\n",
    "                slice_stop = slice_start + (interval.stop - interval.start)\n",
    "                group_score[..., slice_start:slice_stop] = \\\n",
    "                    np.maximum(group_score[..., slice_start:slice_stop],\n",
    "                               scores[idx, ...])\n",
    "            if signal_dims == 1:\n",
    "                group_score = group_score.squeeze(axis=0)\n",
    "            yield Interval(group_chrom, group_start, group_stop), group_score\n",
    "            \n",
    "def interval_score_pairs(intervals, scores, merge_type):\n",
    "    return (izip(intervals, scores) if merge_type is None\n",
    "            else merged_scores(scores, intervals, merge_type))\n",
    "\n",
    "def _write_1D_deeplift_track(scores, intervals, file_prefix, merge_type='max',\n",
    "                             CHROM_SIZES='/mnt/data/annotations/by_release/hg19.GRCh37/hg19.chrom.sizes'):\n",
    "    assert scores.ndim == 2\n",
    "\n",
    "    bedgraph = file_prefix + '.bedGraph'\n",
    "    bigwig = file_prefix + '.bw'\n",
    "\n",
    "    print 'Writing 1D track of shape: {}'.format(scores.shape)\n",
    "    print 'Writing to file: {}'.format(bigwig)\n",
    "\n",
    "    with open(bedgraph, 'w') as fp:\n",
    "        for interval, score in interval_score_pairs(intervals, scores,\n",
    "                                                    merge_type):\n",
    "            chrom = interval.chrom\n",
    "            start = interval.start\n",
    "            for score_idx, val in enumerate(score):\n",
    "                fp.write('%s\\t%d\\t%d\\t%g\\n' % (chrom,\n",
    "                                               start + score_idx,\n",
    "                                               start + score_idx + 1,\n",
    "                                               val))\n",
    "    print 'Wrote bedgraph.'\n",
    "\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            ['wigToBigWig', bedgraph, CHROM_SIZES, bigwig],\n",
    "            stderr=subprocess.STDOUT)\n",
    "        print 'wigToBigWig output: {}'.format(output)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print 'wigToBigWig terminated with exit code {}'.format(\n",
    "            e.returncode)\n",
    "        print 'output was:\\n' + e.output\n",
    "\n",
    "    print 'Wrote bigwig.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = os.path.join(\"models\", \"gan_fixed_smooth\")\n",
    "log_dir = os.path.join(\"logs\", \"gan_fixed_smooth\")\n",
    "#srv_dir = os.path.join(\"/srv\", \"www\", \"kundaje\", \"jesikmin\", \"gan_fixed_smooth\")\n",
    "srv_dir = os.path.join(\"/users\", \"jesikmin\", \"gan_fixed_smooth\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(srv_dir):\n",
    "    os.makedirs(srv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.channels = 5\n",
    "        self.input_shape = (self.window_size, self.channels,)\n",
    "        self.output_shape = (self.window_size, 1,)\n",
    "\n",
    "        optimizer = Adam(0.002, clipnorm=1., beta_1=0.5)\n",
    "        doptimizer = Adam(0.001, clipnorm=1., beta_1=0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', \n",
    "                                   optimizer=doptimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy',\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=self.input_shape)\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity \n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy',\n",
    "                              optimizer=optimizer)\n",
    "        print \"Combined Model\"\n",
    "        print self.combined.summary()\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = self.input_shape\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv1D(hidden_filters_1,\n",
    "                         hidden_kernel_size_1,\n",
    "                         padding=\"same\",\n",
    "                         strides=1,\n",
    "                         input_shape=noise_shape,\n",
    "                         activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model.add(Conv1D(output_filters,\n",
    "                         output_kernel_size,\n",
    "                         padding='same',\n",
    "                         strides=1))\n",
    "        model.add(Activation('linear'))\n",
    "        \n",
    "        print \"Generator\"\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv1D(hidden_filters_1,\n",
    "                         hidden_kernel_size_1,\n",
    "                         padding=\"same\",\n",
    "                         strides=1,\n",
    "                         input_shape=self.output_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(int(window_size/16)))\n",
    "        model.add(LeakyReLU(alpha=0.2)) \n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        print \"Discriminator\"\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.output_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size, X_train, y_train):\n",
    "        \n",
    "        max_pearson = -1.0\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "            \n",
    "        d_loss_real, d_loss_fake, g_loss = [1, 0], [1, 0], [1, 0]\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # list for storing losses/accuracies for both discriminator and generator\n",
    "            d_losses, d_accuracies, g_losses = [], [], []\n",
    "            \n",
    "            # sufficient number of minibatches for each epoch\n",
    "            for _minibatch_idx in range(128):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random half batch of images\n",
    "                dis_idx = np.random.randint(0, y_train.shape[0], half_batch)\n",
    "                imgs = y_train[dis_idx]\n",
    "                dis_noise = X_train[dis_idx]\n",
    "\n",
    "                # Generate a half batch of new images\n",
    "                gen_imgs = self.generator.predict(dis_noise)\n",
    "                \n",
    "                # Train the discriminator with label smoothing\n",
    "                smoothed_idx = np.random.choice(half_batch, int(half_batch*smooth_rate), replace=False)\n",
    "                smoothed_labels = np.ones((half_batch, 1))\n",
    "                smoothed_labels[smoothed_idx] = 0\n",
    "                \n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, smoothed_labels)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "                    \n",
    "                # take the average of each loss and accuracy\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "                \n",
    "                gen_idx = np.random.randint(0, y_train.shape[0], batch_size)\n",
    "                gen_noise = X_train[gen_idx]\n",
    "                #noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "                # The generator wants the discriminator to label the generated samples\n",
    "                # as valid (ones)\n",
    "                valid_y = np.array([1] * batch_size)\n",
    "\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch(gen_noise, valid_y)\n",
    "                \n",
    "                d_losses.append(d_loss[0])\n",
    "                d_accuracies.append(d_loss[1])\n",
    "                \n",
    "                g_losses.append(g_loss)\n",
    "                \n",
    "            # convert each histories into numpy arrays to get means\n",
    "            d_losses = np.array(d_losses)\n",
    "            d_accuracies = np.array(d_accuracies)\n",
    "            g_losses = np.array(g_losses)\n",
    "            \n",
    "            predictions = self.generator.predict(X_train)\n",
    "            pearsons = []\n",
    "            for pred_idx in range(len(predictions)):\n",
    "                prediction = predictions[pred_idx]\n",
    "                pearsons.append(pearsonr(prediction, y_train[pred_idx]))\n",
    "            avg_pearson = np.array(pearsons).mean()\n",
    "            print \"Pearson R on Train set: {}\".format(avg_pearson)\n",
    "            \n",
    "            val_predictions = self.generator.predict(X_val)\n",
    "            val_pearsons = []\n",
    "            for val_pred_idx in range(len(val_predictions)):\n",
    "                prediction = val_predictions[val_pred_idx]\n",
    "                val_pearsons.append(pearsonr(prediction, y_val[val_pred_idx]))\n",
    "            avg_val_pearson = np.array(val_pearsons).mean()\n",
    "            print \"Pearson R on Val set: {}\".format(avg_val_pearson)  \n",
    "            \n",
    "            if max_pearson < avg_val_pearson:\n",
    "                print \"Perason on val improved from {} to {}\".format(max_pearson, avg_val_pearson)\n",
    "                _write_1D_deeplift_track(predictions.reshape(7500, 2001),\n",
    "                                         normalized_day0_intervals[:7500], os.path.join(srv_dir, 'train'))\n",
    "                _write_1D_deeplift_track(val_predictions.reshape(2500, 2001),\n",
    "                                         normalized_day0_intervals[7500:10000], os.path.join(srv_dir, 'val'))\n",
    "                f = open(os.path.join(srv_dir, 'meta.txt'), 'wb')\n",
    "                f.write(str(epoch) + \" \" + str(avg_pearson) + \"  \" + str(avg_val_pearson))\n",
    "                f.close()\n",
    "                max_pearson = avg_val_pearson\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_losses.mean(), 100.0*d_accuracies.mean(), g_losses.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearson(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(np.multiply(xm,ym))\n",
    "    r_den = K.sqrt(np.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model...\n",
      "Discriminator\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 2001, 32)          64064     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 2001, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2001, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2001, 32)          128       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64032)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 125)               8004125   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 126       \n",
      "=================================================================\n",
      "Total params: 8,068,443\n",
      "Trainable params: 8,068,379\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Generator\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_11 (Conv1D)           (None, 2001, 32)          320192    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2001, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 2001, 1)           513       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2001, 1)           0         \n",
      "=================================================================\n",
      "Total params: 320,705\n",
      "Trainable params: 320,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Combined Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 2001, 5)           0         \n",
      "_________________________________________________________________\n",
      "model_11 (Model)             (None, 2001, 1)           320705    \n",
      "_________________________________________________________________\n",
      "model_10 (Model)             (None, 1)                 8068443   \n",
      "=================================================================\n",
      "Total params: 8,389,148\n",
      "Trainable params: 320,705\n",
      "Non-trainable params: 8,068,443\n",
      "_________________________________________________________________\n",
      "None\n",
      "Pearson R on Train set: 0.116693519056\n",
      "Pearson R on Val set: 0.0930212140083\n",
      "Perason on val improved from -1.0 to 0.0930212140083\n",
      "Writing 1D track of shape: (7500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "0 [D loss: 3.519821, acc.: 57.50%] [G loss: 11.413115]\n",
      "Pearson R on Train set: 0.124305970967\n",
      "Pearson R on Val set: 0.119803868234\n",
      "Perason on val improved from 0.0930212140083 to 0.119803868234\n",
      "Writing 1D track of shape: (7500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "1 [D loss: 2.092872, acc.: 43.07%] [G loss: 2.908550]\n",
      "Pearson R on Train set: 0.0871613174677\n",
      "Pearson R on Val set: 0.0767417624593\n",
      "2 [D loss: 1.540009, acc.: 45.12%] [G loss: 2.215652]\n",
      "Pearson R on Train set: 0.0632233172655\n",
      "Pearson R on Val set: 0.0627873316407\n",
      "3 [D loss: 1.125397, acc.: 51.82%] [G loss: 2.002124]\n",
      "Pearson R on Train set: 0.133572906256\n",
      "Pearson R on Val set: 0.0953860208392\n",
      "4 [D loss: 0.919547, acc.: 52.33%] [G loss: 1.480855]\n",
      "Pearson R on Train set: 0.0729415044188\n",
      "Pearson R on Val set: 0.0581379011273\n",
      "5 [D loss: 0.789942, acc.: 55.57%] [G loss: 1.291147]\n",
      "Pearson R on Train set: 0.106778249145\n",
      "Pearson R on Val set: 0.0898750722408\n",
      "6 [D loss: 0.756259, acc.: 55.43%] [G loss: 1.264496]\n",
      "Pearson R on Train set: 0.155106246471\n",
      "Pearson R on Val set: 0.113025195897\n",
      "7 [D loss: 0.738379, acc.: 54.51%] [G loss: 1.083459]\n",
      "Pearson R on Train set: 0.131794556975\n",
      "Pearson R on Val set: 0.106569625437\n",
      "8 [D loss: 0.744897, acc.: 58.50%] [G loss: 1.345880]\n",
      "Pearson R on Train set: 0.143875777721\n",
      "Pearson R on Val set: 0.121701098979\n",
      "Perason on val improved from 0.119803868234 to 0.121701098979\n",
      "Writing 1D track of shape: (7500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "9 [D loss: 0.762439, acc.: 57.91%] [G loss: 1.328990]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "10 [D loss: 0.724010, acc.: 58.18%] [G loss: 1.225257]\n",
      "Pearson R on Train set: 0.090651191771\n",
      "Pearson R on Val set: 0.0766432508826\n",
      "11 [D loss: 0.724164, acc.: 56.80%] [G loss: 1.217854]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "12 [D loss: 0.741243, acc.: 56.02%] [G loss: 1.208024]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "13 [D loss: 0.731904, acc.: 56.65%] [G loss: 1.214446]\n",
      "Pearson R on Train set: 0.114503920078\n",
      "Pearson R on Val set: 0.107665322721\n",
      "14 [D loss: 0.728385, acc.: 55.85%] [G loss: 1.187098]\n",
      "Pearson R on Train set: 0.142273962498\n",
      "Pearson R on Val set: 0.11063901335\n",
      "15 [D loss: 0.731334, acc.: 57.60%] [G loss: 1.228578]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "16 [D loss: 0.742906, acc.: 55.33%] [G loss: 1.200762]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "17 [D loss: 0.762363, acc.: 53.31%] [G loss: 1.175534]\n",
      "Pearson R on Train set: 0.128853693604\n",
      "Pearson R on Val set: 0.106354348361\n",
      "18 [D loss: 0.729965, acc.: 56.81%] [G loss: 1.177539]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "19 [D loss: 0.741583, acc.: 55.76%] [G loss: 1.186468]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "20 [D loss: 0.764970, acc.: 56.56%] [G loss: 1.437916]\n",
      "Pearson R on Train set: 0.123078815639\n",
      "Pearson R on Val set: 0.102945998311\n",
      "21 [D loss: 0.734358, acc.: 58.44%] [G loss: 1.355942]\n",
      "Pearson R on Train set: 0.118154883385\n",
      "Pearson R on Val set: 0.102596923709\n",
      "22 [D loss: 0.744561, acc.: 58.07%] [G loss: 1.305979]\n",
      "Pearson R on Train set: 0.115369483829\n",
      "Pearson R on Val set: 0.0962663367391\n",
      "23 [D loss: 0.733117, acc.: 55.31%] [G loss: 1.241476]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "24 [D loss: 0.750394, acc.: 56.56%] [G loss: 1.297076]\n",
      "Pearson R on Train set: 0.131677389145\n",
      "Pearson R on Val set: 0.11299277097\n",
      "25 [D loss: 0.743859, acc.: 55.84%] [G loss: 1.272570]\n",
      "Pearson R on Train set: 0.127031072974\n",
      "Pearson R on Val set: 0.106166988611\n",
      "26 [D loss: 0.764522, acc.: 56.63%] [G loss: 1.402038]\n",
      "Pearson R on Train set: 0.132406204939\n",
      "Pearson R on Val set: 0.111955404282\n",
      "27 [D loss: 0.755080, acc.: 56.73%] [G loss: 1.407967]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "28 [D loss: 0.771079, acc.: 56.54%] [G loss: 1.454049]\n",
      "Pearson R on Train set: 0.146716028452\n",
      "Pearson R on Val set: 0.12167109549\n",
      "29 [D loss: 0.736875, acc.: 58.17%] [G loss: 1.330996]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "30 [D loss: 0.730126, acc.: 57.57%] [G loss: 1.232120]\n",
      "Pearson R on Train set: 0.137232154608\n",
      "Pearson R on Val set: 0.116569042206\n",
      "31 [D loss: 0.748046, acc.: 56.51%] [G loss: 1.225209]\n",
      "Pearson R on Train set: 0.148179456592\n",
      "Pearson R on Val set: 0.112852454185\n",
      "32 [D loss: 0.764197, acc.: 56.54%] [G loss: 1.417576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson R on Train set: 0.133082717657\n",
      "Pearson R on Val set: 0.107813596725\n",
      "33 [D loss: 0.727406, acc.: 58.68%] [G loss: 1.318534]\n",
      "Pearson R on Train set: 0.124199591577\n",
      "Pearson R on Val set: 0.0969040989876\n",
      "34 [D loss: 0.769065, acc.: 56.06%] [G loss: 1.378312]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "35 [D loss: 0.732092, acc.: 57.49%] [G loss: 1.291424]\n",
      "Pearson R on Train set: 0.134671196342\n",
      "Pearson R on Val set: 0.108608461916\n",
      "36 [D loss: 0.733383, acc.: 57.53%] [G loss: 1.317672]\n",
      "Pearson R on Train set: 0.123841263354\n",
      "Pearson R on Val set: 0.106467984617\n",
      "37 [D loss: 0.735703, acc.: 57.19%] [G loss: 1.239470]\n",
      "Pearson R on Train set: 0.130410447717\n",
      "Pearson R on Val set: 0.108418785036\n",
      "38 [D loss: 0.748588, acc.: 58.25%] [G loss: 1.448818]\n",
      "Pearson R on Train set: 0.135032072663\n",
      "Pearson R on Val set: 0.114254005253\n",
      "39 [D loss: 0.719590, acc.: 57.66%] [G loss: 1.258385]\n",
      "Pearson R on Train set: 0.126804172993\n",
      "Pearson R on Val set: 0.101315125823\n",
      "40 [D loss: 0.757193, acc.: 58.65%] [G loss: 1.473347]\n",
      "Pearson R on Train set: 0.126570180058\n",
      "Pearson R on Val set: 0.105113625526\n",
      "41 [D loss: 0.747582, acc.: 57.82%] [G loss: 1.417940]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "42 [D loss: 0.732356, acc.: 58.14%] [G loss: 1.319210]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "43 [D loss: 0.735921, acc.: 57.00%] [G loss: 1.226617]\n",
      "Pearson R on Train set: 0.101299710572\n",
      "Pearson R on Val set: 0.0892296507955\n",
      "44 [D loss: 0.772320, acc.: 55.04%] [G loss: 1.419216]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "45 [D loss: 0.764064, acc.: 57.73%] [G loss: 1.391574]\n",
      "Pearson R on Train set: 0.136478289962\n",
      "Pearson R on Val set: 0.11632283777\n",
      "46 [D loss: 0.732422, acc.: 59.00%] [G loss: 1.345547]\n",
      "Pearson R on Train set: 0.126438677311\n",
      "Pearson R on Val set: 0.0958528667688\n",
      "47 [D loss: 0.727686, acc.: 58.32%] [G loss: 1.316257]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "48 [D loss: 0.735721, acc.: 58.78%] [G loss: 1.383439]\n",
      "Pearson R on Train set: 0.12746912241\n",
      "Pearson R on Val set: 0.105890795588\n",
      "49 [D loss: 0.764041, acc.: 59.16%] [G loss: 1.484910]\n",
      "Pearson R on Train set: 0.12109003216\n",
      "Pearson R on Val set: 0.0918032377958\n",
      "50 [D loss: 0.754629, acc.: 56.78%] [G loss: 1.344216]\n",
      "Pearson R on Train set: 0.138688832521\n",
      "Pearson R on Val set: 0.109420701861\n",
      "51 [D loss: 0.749396, acc.: 56.67%] [G loss: 1.326035]\n",
      "Pearson R on Train set: 0.124909631908\n",
      "Pearson R on Val set: 0.103452220559\n",
      "52 [D loss: 0.730739, acc.: 58.81%] [G loss: 1.362583]\n",
      "Pearson R on Train set: 0.139654323459\n",
      "Pearson R on Val set: 0.120060704648\n",
      "53 [D loss: 0.731697, acc.: 56.83%] [G loss: 1.324345]\n",
      "Pearson R on Train set: 0.122838966548\n",
      "Pearson R on Val set: 0.10063483566\n",
      "54 [D loss: 0.734882, acc.: 57.42%] [G loss: 1.318226]\n",
      "Pearson R on Train set: 0.132497385144\n",
      "Pearson R on Val set: 0.116636134684\n",
      "55 [D loss: 0.730708, acc.: 59.08%] [G loss: 1.478472]\n",
      "Pearson R on Train set: 0.114533029497\n",
      "Pearson R on Val set: 0.0917071253061\n",
      "56 [D loss: 0.723303, acc.: 57.76%] [G loss: 1.288389]\n",
      "Pearson R on Train set: 0.0962642729282\n",
      "Pearson R on Val set: 0.08639254421\n",
      "57 [D loss: 0.742052, acc.: 58.43%] [G loss: 1.427074]\n",
      "Pearson R on Train set: 0.134423360229\n",
      "Pearson R on Val set: 0.125569805503\n",
      "Perason on val improved from 0.121701098979 to 0.125569805503\n",
      "Writing 1D track of shape: (7500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "58 [D loss: 0.727560, acc.: 57.80%] [G loss: 1.429907]\n",
      "Pearson R on Train set: 0.135914117098\n",
      "Pearson R on Val set: 0.111273497343\n",
      "59 [D loss: 0.742249, acc.: 59.07%] [G loss: 1.467664]\n",
      "Pearson R on Train set: 0.118410348892\n",
      "Pearson R on Val set: 0.0897542014718\n",
      "60 [D loss: 0.724132, acc.: 58.95%] [G loss: 1.424682]\n",
      "Pearson R on Train set: 0.121989272535\n",
      "Pearson R on Val set: 0.0916741862893\n",
      "61 [D loss: 0.710085, acc.: 58.03%] [G loss: 1.267329]\n",
      "Pearson R on Train set: 0.122987352312\n",
      "Pearson R on Val set: 0.101234801114\n",
      "62 [D loss: 0.723738, acc.: 59.09%] [G loss: 1.469991]\n",
      "Pearson R on Train set: 0.118705585599\n",
      "Pearson R on Val set: 0.0927748158574\n",
      "63 [D loss: 0.716911, acc.: 58.91%] [G loss: 1.415173]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "64 [D loss: 0.697754, acc.: 58.80%] [G loss: 1.213917]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "65 [D loss: 0.707974, acc.: 59.12%] [G loss: 1.450652]\n",
      "Pearson R on Train set: 0.134488329291\n",
      "Pearson R on Val set: 0.0958237349987\n",
      "66 [D loss: 0.713544, acc.: 58.34%] [G loss: 1.377477]\n",
      "Pearson R on Train set: 0.129996210337\n",
      "Pearson R on Val set: 0.094527579844\n",
      "67 [D loss: 0.713629, acc.: 60.00%] [G loss: 1.501544]\n",
      "Pearson R on Train set: 0.134296387434\n",
      "Pearson R on Val set: 0.104364819825\n",
      "68 [D loss: 0.702842, acc.: 59.81%] [G loss: 1.414330]\n",
      "Pearson R on Train set: 0.135751724243\n",
      "Pearson R on Val set: 0.111291930079\n",
      "69 [D loss: 0.684393, acc.: 61.57%] [G loss: 1.350462]\n",
      "Pearson R on Train set: 0.130430266261\n",
      "Pearson R on Val set: 0.0955895185471\n",
      "70 [D loss: 0.690489, acc.: 61.22%] [G loss: 1.338669]\n",
      "Pearson R on Train set: 0.115306913853\n",
      "Pearson R on Val set: 0.0961316898465\n",
      "71 [D loss: 0.691140, acc.: 61.10%] [G loss: 1.365284]\n",
      "Pearson R on Train set: 0.15053717792\n",
      "Pearson R on Val set: 0.111790768802\n",
      "72 [D loss: 0.696283, acc.: 61.50%] [G loss: 1.479129]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "73 [D loss: 0.722133, acc.: 59.18%] [G loss: 1.382331]\n",
      "Pearson R on Train set: 0.148202762008\n",
      "Pearson R on Val set: 0.131137534976\n",
      "Perason on val improved from 0.125569805503 to 0.131137534976\n",
      "Writing 1D track of shape: (7500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/train.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "Writing 1D track of shape: (2500, 2001)\n",
      "Writing to file: /users/jesikmin/gan_fixed_smooth/val.bw\n",
      "Wrote bedgraph.\n",
      "wigToBigWig output: \n",
      "Wrote bigwig.\n",
      "74 [D loss: 0.719001, acc.: 58.72%] [G loss: 1.317132]\n",
      "Pearson R on Train set: 0.123559251428\n",
      "Pearson R on Val set: 0.106549084187\n",
      "75 [D loss: 0.714471, acc.: 59.20%] [G loss: 1.297321]\n",
      "Pearson R on Train set: 0.145224571228\n",
      "Pearson R on Val set: 0.114285729825\n",
      "76 [D loss: 0.734714, acc.: 59.45%] [G loss: 1.425626]\n",
      "Pearson R on Train set: 0.150017902255\n",
      "Pearson R on Val set: 0.111681126058\n",
      "77 [D loss: 0.712132, acc.: 59.08%] [G loss: 1.286903]\n",
      "Pearson R on Train set: 0.150131478906\n",
      "Pearson R on Val set: 0.11097048223\n",
      "78 [D loss: 0.724576, acc.: 59.33%] [G loss: 1.387379]\n",
      "Pearson R on Train set: 0.138619184494\n",
      "Pearson R on Val set: 0.108905471861\n",
      "79 [D loss: 0.713983, acc.: 59.19%] [G loss: 1.297490]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "80 [D loss: 0.724398, acc.: 59.28%] [G loss: 1.401105]\n",
      "Pearson R on Train set: 0.149222135544\n",
      "Pearson R on Val set: 0.127705514431\n",
      "81 [D loss: 0.744408, acc.: 59.36%] [G loss: 1.490654]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "82 [D loss: 0.755460, acc.: 59.26%] [G loss: 1.519262]\n",
      "Pearson R on Train set: 0.14249625802\n",
      "Pearson R on Val set: 0.130500257015\n",
      "83 [D loss: 0.710744, acc.: 58.92%] [G loss: 1.285232]\n",
      "Pearson R on Train set: 0.060755494982\n",
      "Pearson R on Val set: 0.0609431900084\n",
      "84 [D loss: 0.770526, acc.: 59.47%] [G loss: 1.623024]\n",
      "Pearson R on Train set: 0.127740621567\n",
      "Pearson R on Val set: 0.111509107053\n",
      "85 [D loss: 0.759831, acc.: 59.63%] [G loss: 1.607578]\n",
      "Pearson R on Train set: 0.125198677182\n",
      "Pearson R on Val set: 0.111980728805\n",
      "86 [D loss: 0.713163, acc.: 59.11%] [G loss: 1.297620]\n",
      "Pearson R on Train set: 0.139184176922\n",
      "Pearson R on Val set: 0.123932152987\n",
      "87 [D loss: 0.732360, acc.: 59.19%] [G loss: 1.447068]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "88 [D loss: 0.726085, acc.: 59.14%] [G loss: 1.437009]\n",
      "Pearson R on Train set: 0.136867180467\n",
      "Pearson R on Val set: 0.110033422709\n",
      "89 [D loss: 0.728249, acc.: 59.23%] [G loss: 1.421285]\n",
      "Pearson R on Train set: 0.148304820061\n",
      "Pearson R on Val set: 0.130929037929\n",
      "90 [D loss: 0.712463, acc.: 59.24%] [G loss: 1.314596]\n",
      "Pearson R on Train set: 0.138301819563\n",
      "Pearson R on Val set: 0.119629636407\n",
      "91 [D loss: 0.721134, acc.: 59.30%] [G loss: 1.338846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson R on Train set: 0.157678321004\n",
      "Pearson R on Val set: 0.128035455942\n",
      "92 [D loss: 0.701468, acc.: 58.79%] [G loss: 1.222862]\n",
      "Pearson R on Train set: 0.1076502949\n",
      "Pearson R on Val set: 0.100827455521\n",
      "93 [D loss: 0.715579, acc.: 58.80%] [G loss: 1.306207]\n",
      "Pearson R on Train set: 0.132234588265\n",
      "Pearson R on Val set: 0.117939330637\n",
      "94 [D loss: 0.709589, acc.: 58.79%] [G loss: 1.304731]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "95 [D loss: 0.714160, acc.: 59.00%] [G loss: 1.311453]\n",
      "Pearson R on Train set: 0.147565498948\n",
      "Pearson R on Val set: 0.130258366466\n",
      "96 [D loss: 0.720755, acc.: 59.15%] [G loss: 1.335623]\n",
      "Pearson R on Train set: nan\n",
      "Pearson R on Val set: nan\n",
      "97 [D loss: 0.732234, acc.: 59.33%] [G loss: 1.423001]\n",
      "Pearson R on Train set: 0.137741878629\n",
      "Pearson R on Val set: 0.109219409525\n",
      "98 [D loss: 0.707186, acc.: 59.06%] [G loss: 1.274501]\n",
      "Pearson R on Train set: 0.137016445398\n",
      "Pearson R on Val set: 0.129614681005\n",
      "99 [D loss: 0.729717, acc.: 59.48%] [G loss: 1.410974]\n"
     ]
    }
   ],
   "source": [
    "print \"Fitting the model...\"\n",
    "X_train, y_train = atac_seq_day0[:7500], histone_mark_day0[:7500]\n",
    "X_val, y_val = atac_seq_day0[7500:10000], histone_mark_day0[7500:10000]\n",
    "X_test, y_test = atac_seq_day0[10000:11000], histone_mark_day0[10000:11000]\n",
    "\n",
    "gan = GAN()\n",
    "gan.train(num_epochs, batch_size, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
